{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fernsrea/flow_matching/blob/main/MNIST_Discreet_FM_model_Inpainting_DeterministicMask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeFku-8ZtT9I",
        "outputId": "e64e1608-f545-4abf-dc0e-79c2e47d73c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jRnYLF-sE8H",
        "outputId": "77b8a983-5eca-479d-fc44-c0dc9059024b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'flow_matching'...\n",
            "remote: Enumerating objects: 485, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 485 (delta 184), reused 73 (delta 73), pack-reused 220 (from 2)\u001b[K\n",
            "Receiving objects: 100% (485/485), 3.46 MiB | 12.21 MiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n",
            "/content/flow_matching\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Fernsrea/flow_matching.git\n",
        "%cd flow_matching"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd examples/image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1R7FXmqsYCR",
        "outputId": "8d22de8d-f9f0-4e55-a5b3-23ca07a5333e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/flow_matching/examples/image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzyJxPEasbeA",
        "outputId": "ca23ec2d-c6af-44c8-8bbc-05179fea1ffc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting submitit (from -r requirements.txt (line 1))\n",
            "  Downloading submitit-1.5.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.21.0+cu124)\n",
            "Collecting torchdiffeq (from -r requirements.txt (line 4))\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting flow_matching (from -r requirements.txt (line 5))\n",
            "  Downloading flow_matching-1.0.10-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting torchmetrics[image] (from -r requirements.txt (line 2))\n",
            "  Downloading torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: scipy>1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (1.15.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 3)) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[image]->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity<=0.4.0->torchmetrics[image]->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.0.2)\n",
            "Downloading submitit-1.5.3-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Downloading flow_matching-1.0.10-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading torchmetrics-1.7.2-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: submitit, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, torchdiffeq, torch-fidelity, flow_matching\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flow_matching-1.0.10 lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 submitit-1.5.3 torch-fidelity-0.3.0 torchdiffeq-0.2.5 torchmetrics-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSbE2uw71gz3",
        "outputId": "57bd4a0f-5b7a-477d-a7d8-d8de48584abf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.4.0->torchdiffeq) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.2)\n",
            "Downloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flow_matching"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4SivJfO2boL",
        "outputId": "8738271e-d971-4ef0-ac03-b345334e3984"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flow_matching\n",
            "  Downloading flow_matching-1.0.10-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flow_matching) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flow_matching) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.11/dist-packages (from flow_matching) (0.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flow_matching) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq->flow_matching) (1.15.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flow_matching) (3.0.2)\n",
            "Downloading flow_matching-1.0.10-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flow_matching\n",
            "Successfully installed flow_matching-1.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we7T4aktbEUT",
        "outputId": "8593d3ca-6758-40ac-e3da-f0c7d5643ae5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/masked_mnist/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XNYzHTTkTL3",
        "outputId": "c74922bd-b699-4459-8e8f-b42df3a99782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked_mnist_train.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create target directory\n",
        "import os\n",
        "target_dir = '/content/drive/MyDrive/masked_mnist'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "# Parameters\n",
        "mask_size = 14\n",
        "image_size = 28\n",
        "\n",
        "# Load MNIST\n",
        "transform = transforms.ToTensor()\n",
        "mnist = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "# Create center mask\n",
        "def create_center_mask(size, mask_size):\n",
        "    mask = np.ones((size, size), dtype=np.uint8)\n",
        "    start = (size - mask_size) // 2\n",
        "    mask[start:start + mask_size, start:start + mask_size] = 0\n",
        "    return mask\n",
        "\n",
        "mask = create_center_mask(image_size, mask_size)\n",
        "mask_tensor = torch.tensor(mask, dtype=torch.float32)  # (28, 28)\n",
        "\n",
        "# Apply mask and store\n",
        "images, masks, labels = [], [], []\n",
        "for img, label in mnist:\n",
        "    img = img.squeeze(0)  # (28, 28)\n",
        "    masked = img * mask_tensor\n",
        "    images.append(masked.unsqueeze(0))        # (1, 28, 28)\n",
        "    masks.append(mask_tensor.unsqueeze(0))    # (1, 28, 28)\n",
        "    labels.append(label)\n",
        "\n",
        "# Stack and save\n",
        "data = {\n",
        "    'images': torch.stack(images),   # (N, 1, 28, 28)\n",
        "    'masks': torch.stack(masks),     # (N, 1, 28, 28)\n",
        "    'labels': torch.tensor(labels)   # (N,)\n",
        "}\n",
        "torch.save(data, os.path.join(target_dir, 'masked_mnist_train.pt'))\n",
        "print(\"masked_mnist_train.pt saved to Google Drive with correct shape.\")\n"
      ],
      "metadata": {
        "id": "aYM0DdtTBxHG",
        "outputId": "6d0d32f6-1910-4a37-dfcf-c2cfc93f914d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 135MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 13.7MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 77.0MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.68MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked_mnist_train.pt saved to Google Drive with correct shape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "target_dir = '/content/flow_matching/examples/image/data/image_generation'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Path in your Google Drive\n",
        "drive_path = '/content/drive/MyDrive/masked_mnist/masked_mnist_train.pt'\n",
        "\n",
        "# Local destination\n",
        "local_path = os.path.join(target_dir, 'masked_mnist_train.pt')\n",
        "\n",
        "# Copy file\n",
        "shutil.copy(drive_path, local_path)\n",
        "\n",
        "\n",
        "data = torch.load('./data/image_generation/masked_mnist_train.pt')\n",
        "\n",
        "# Extract contents\n",
        "images = data['images']     # shape: (N, 1, 28, 28)\n",
        "masks = data['masks']       # shape: (N, 1, 28, 28)\n",
        "labels = data['labels']     # shape: (N,)"
      ],
      "metadata": {
        "id": "h9DY53bcp0bN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --dataset=mnist \\\n",
        "    --batch_size=32 \\\n",
        "    --accum_iter=1 \\\n",
        "    --eval_frequency=5 \\\n",
        "    --save_fid_samples \\\n",
        "    --num_workers=2 \\\n",
        "    --epochs=500 \\\n",
        "    --use_ema \\\n",
        "    --fid_samples=1000 \\\n",
        "    --discrete_flow_matching \\\n",
        "    --cfg_scale=0.0 \\\n",
        "    --data_path=\"./data/image_generation\" \\\n",
        "    --output_dir='/content/drive/MyDrive/flow_matching_checkpoints_29M' \\\n",
        "    --compute_fid \\\n",
        "    --test_run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib_cNzfWkg9_",
        "outputId": "d3e330fd-8f5e-425c-dc4a-b7a07aafee99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "2025-05-28 22:53:20 INFO     job dir: /content/flow_matching/examples/image\n",
            "2025-05-28 22:53:20 INFO     Namespace(batch_size=32,\n",
            "epochs=500,\n",
            "accum_iter=1,\n",
            "lr=0.0001,\n",
            "optimizer_betas=[0.9,\n",
            "0.95],\n",
            "decay_lr=False,\n",
            "class_drop_prob=0.2,\n",
            "skewed_timesteps=False,\n",
            "edm_schedule=False,\n",
            "use_ema=True,\n",
            "dataset='mnist',\n",
            "data_path='./data/image_generation',\n",
            "output_dir='/content/drive/MyDrive/flow_matching_checkpoints_29M',\n",
            "ode_method='midpoint',\n",
            "ode_options={'step_size': 0.01},\n",
            "sym=0.0,\n",
            "temp=1.0,\n",
            "sym_func=False,\n",
            "sampling_dtype='float32',\n",
            "cfg_scale=0.0,\n",
            "fid_samples=1000,\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='',\n",
            "start_epoch=0,\n",
            "eval_only=False,\n",
            "eval_frequency=5,\n",
            "compute_fid=True,\n",
            "save_fid_samples=True,\n",
            "num_workers=2,\n",
            "pin_mem=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "test_run=True,\n",
            "discrete_flow_matching=True,\n",
            "discrete_fm_steps=1024,\n",
            "distributed=False)\n",
            "2025-05-28 22:53:20 INFO     Saving args to /content/drive/MyDrive/flow_matching_checkpoints_29M/args.json\n",
            "2025-05-28 22:53:21 INFO     Initializing Dataset: mnist\n",
            "2025-05-28 22:53:21 INFO     Applied transform to masked image\n",
            "2025-05-28 22:53:21 INFO     <__main__.MaskedMNISTDataset object at 0x7d80c47e9e50>\n",
            "2025-05-28 22:53:21 INFO     Intializing DataLoader\n",
            "2025-05-28 22:53:21 INFO     <torch.utils.data.distributed.DistributedSampler object at 0x7d80c450a790>\n",
            "100% 9.91M/9.91M [00:01<00:00, 5.09MB/s]\n",
            "100% 28.9k/28.9k [00:00<00:00, 133kB/s]\n",
            "100% 1.65M/1.65M [00:01<00:00, 1.28MB/s]\n",
            "100% 4.54k/4.54k [00:00<00:00, 13.9MB/s]\n",
            "2025-05-28 22:53:32 INFO     Initializing Model\n",
            "2025-05-28 22:53:33 INFO     EMA(\n",
            "  (model): DiscreteUNetModel(vocab_size=257, in_channels=3, model_channels=96, out_channels=3, num_res_blocks=2, attention_resolutions=[], dropout=0.1, channel_mult=[1, 2, 2], conv_resample=False, dims=2, num_classes=None, use_checkpoint=False, num_heads=-1, num_head_channels=16, num_heads_upsample=-1, use_scale_shift_norm=True, resblock_updown=False, use_new_attention_order=True, with_fourier_features=False)\n",
            "  (shadow_params): ParameterList(\n",
            "      (0): Parameter containing: [torch.float32 of size 257x32 (cuda:0)]\n",
            "      (1): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (2): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (3): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (4): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (5): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (6): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (7): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (8): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (9): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (10): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (11): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (12): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (13): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (14): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (15): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (16): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (17): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (18): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (19): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (20): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (21): Parameter containing: [torch.float32 of size 192x96x3x3 (cuda:0)]\n",
            "      (22): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (23): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (24): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (25): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (26): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (27): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (28): Parameter containing: [torch.float32 of size 192x96x1x1 (cuda:0)]\n",
            "      (29): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (30): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (31): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (32): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (33): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (34): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (35): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (36): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (37): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (38): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (39): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (40): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (41): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (42): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (43): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (44): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (45): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (46): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (47): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (48): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (49): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (50): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (51): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (52): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (53): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (54): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (55): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (56): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (57): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (58): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (59): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (60): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (61): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (62): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (63): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (64): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (65): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (66): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (67): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (68): Parameter containing: [torch.float32 of size 576x192x1 (cuda:0)]\n",
            "      (69): Parameter containing: [torch.float32 of size 576 (cuda:0)]\n",
            "      (70): Parameter containing: [torch.float32 of size 192x192x1 (cuda:0)]\n",
            "      (71): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (72): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (73): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (74): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (75): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (76): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (77): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (78): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (79): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (80): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (81): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (82): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (83): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (84): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (85): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (86): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (87): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (88): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (89): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (90): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (91): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (92): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (93): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (94): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (95): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (96): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (97): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (98): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (99): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (100): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (101): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (102): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (103): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (104): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (105): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (106): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (107): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (108): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (109): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (110): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (111): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (112): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (113): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (114): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (115): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (116): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (117): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (118): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (119): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (120): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (121): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (122): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (123): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (124): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (125): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (126): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (127): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (128): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (129): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (130): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (131): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (132): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (133): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (134): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (135): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (136): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (137): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (138): Parameter containing: [torch.float32 of size 192x288x3x3 (cuda:0)]\n",
            "      (139): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (140): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (141): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (142): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (143): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (144): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (145): Parameter containing: [torch.float32 of size 192x288x1x1 (cuda:0)]\n",
            "      (146): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (147): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (148): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (149): Parameter containing: [torch.float32 of size 96x288x3x3 (cuda:0)]\n",
            "      (150): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (151): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (152): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (153): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (154): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (155): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (156): Parameter containing: [torch.float32 of size 96x288x1x1 (cuda:0)]\n",
            "      (157): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (158): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (159): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (160): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (161): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (162): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (163): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (164): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (165): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (166): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (167): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (168): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (169): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (170): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (171): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (172): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (173): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (174): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (175): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (176): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (177): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (178): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (179): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (180): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (181): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (182): Parameter containing: [torch.float32 of size 771x96x3x3 (cuda:0)]\n",
            "      (183): Parameter containing: [torch.float32 of size 771 (cuda:0)]\n",
            "  )\n",
            ")\n",
            "2025-05-28 22:53:33 INFO     Learning rate: 1.00e-04\n",
            "2025-05-28 22:53:33 INFO     Accumulate grad iterations: 1\n",
            "2025-05-28 22:53:33 INFO     Effective batch size: 32\n",
            "2025-05-28 22:53:33 INFO     Optimizer: AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.95]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "2025-05-28 22:53:33 INFO     Learning-Rate Schedule: <torch.optim.lr_scheduler.ConstantLR object at 0x7d80a912a590>\n",
            "/content/flow_matching/examples/image/training/grad_scaler.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "2025-05-28 22:53:33 INFO     Start from 0 to 500 epochs\n",
            "2025-05-28 22:53:37 INFO     Epoch 0 [0/1875]: loss = 5.549057483673096, lr = 0.0001\n",
            "2025-05-28 22:53:38 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100% 91.2M/91.2M [00:02<00:00, 35.1MB/s]\n",
            "/content/flow_matching/examples/image/training/eval_loop.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(), torch.no_grad():\n",
            "2025-05-28 22:54:25 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 22:54:31 INFO     Evaluating [0/313] samples generated [32/1000] running fid 386.48114013671875\n",
            "2025-05-28 22:54:31 INFO     Training time 0:00:57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --dataset=mnist \\\n",
        "    --batch_size=32 \\\n",
        "    --accum_iter=1 \\\n",
        "    --eval_frequency=5 \\\n",
        "    --save_fid_samples \\\n",
        "    --num_workers=2 \\\n",
        "    --epochs=500 \\\n",
        "    --use_ema \\\n",
        "    --fid_samples=1000 \\\n",
        "    --discrete_flow_matching \\\n",
        "    --cfg_scale=0.0 \\\n",
        "    --data_path=\"./data/image_generation\" \\\n",
        "    --output_dir='/content/drive/MyDrive/flow_matching_checkpoints_29M' \\\n",
        "    --compute_fid --resume='/content/drive/MyDrive/flow_matching_checkpoints_29M/checkpoint.pth'"
      ],
      "metadata": {
        "outputId": "93b2de45-8280-4d0c-d3ce-4ec4fe1684b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikyJG2ip1p8j"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using distributed mode\n",
            "2025-05-28 22:56:07 INFO     job dir: /content/flow_matching/examples/image\n",
            "2025-05-28 22:56:07 INFO     Namespace(batch_size=32,\n",
            "epochs=500,\n",
            "accum_iter=1,\n",
            "lr=0.0001,\n",
            "optimizer_betas=[0.9,\n",
            "0.95],\n",
            "decay_lr=False,\n",
            "class_drop_prob=0.2,\n",
            "skewed_timesteps=False,\n",
            "edm_schedule=False,\n",
            "use_ema=True,\n",
            "dataset='mnist',\n",
            "data_path='./data/image_generation',\n",
            "output_dir='/content/drive/MyDrive/flow_matching_checkpoints_29M',\n",
            "ode_method='midpoint',\n",
            "ode_options={'step_size': 0.01},\n",
            "sym=0.0,\n",
            "temp=1.0,\n",
            "sym_func=False,\n",
            "sampling_dtype='float32',\n",
            "cfg_scale=0.0,\n",
            "fid_samples=1000,\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='/content/drive/MyDrive/flow_matching_checkpoints_29M/checkpoint.pth',\n",
            "start_epoch=0,\n",
            "eval_only=False,\n",
            "eval_frequency=5,\n",
            "compute_fid=True,\n",
            "save_fid_samples=True,\n",
            "num_workers=2,\n",
            "pin_mem=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "test_run=False,\n",
            "discrete_flow_matching=True,\n",
            "discrete_fm_steps=1024,\n",
            "distributed=False)\n",
            "2025-05-28 22:56:07 INFO     Saving args to /content/drive/MyDrive/flow_matching_checkpoints_29M/args.json\n",
            "2025-05-28 22:56:07 INFO     Initializing Dataset: mnist\n",
            "2025-05-28 22:56:07 INFO     Applied transform to masked image\n",
            "2025-05-28 22:56:07 INFO     <__main__.MaskedMNISTDataset object at 0x7ca1706e7d10>\n",
            "2025-05-28 22:56:07 INFO     Intializing DataLoader\n",
            "2025-05-28 22:56:07 INFO     <torch.utils.data.distributed.DistributedSampler object at 0x7ca17062f210>\n",
            "2025-05-28 22:56:07 INFO     Initializing Model\n",
            "2025-05-28 22:56:08 INFO     EMA(\n",
            "  (model): DiscreteUNetModel(vocab_size=257, in_channels=3, model_channels=96, out_channels=3, num_res_blocks=2, attention_resolutions=[], dropout=0.1, channel_mult=[1, 2, 2], conv_resample=False, dims=2, num_classes=None, use_checkpoint=False, num_heads=-1, num_head_channels=16, num_heads_upsample=-1, use_scale_shift_norm=True, resblock_updown=False, use_new_attention_order=True, with_fourier_features=False)\n",
            "  (shadow_params): ParameterList(\n",
            "      (0): Parameter containing: [torch.float32 of size 257x32 (cuda:0)]\n",
            "      (1): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (2): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (3): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (4): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (5): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (6): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (7): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (8): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (9): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (10): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (11): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (12): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (13): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (14): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (15): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (16): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (17): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (18): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (19): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (20): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (21): Parameter containing: [torch.float32 of size 192x96x3x3 (cuda:0)]\n",
            "      (22): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (23): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (24): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (25): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (26): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (27): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (28): Parameter containing: [torch.float32 of size 192x96x1x1 (cuda:0)]\n",
            "      (29): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (30): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (31): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (32): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (33): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (34): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (35): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (36): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (37): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (38): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (39): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (40): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (41): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (42): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (43): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (44): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (45): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (46): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (47): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (48): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (49): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (50): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (51): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (52): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (53): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (54): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (55): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (56): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (57): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (58): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (59): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (60): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (61): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (62): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (63): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (64): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (65): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (66): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (67): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (68): Parameter containing: [torch.float32 of size 576x192x1 (cuda:0)]\n",
            "      (69): Parameter containing: [torch.float32 of size 576 (cuda:0)]\n",
            "      (70): Parameter containing: [torch.float32 of size 192x192x1 (cuda:0)]\n",
            "      (71): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (72): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (73): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (74): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (75): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (76): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (77): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (78): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (79): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (80): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (81): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (82): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (83): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (84): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (85): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (86): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (87): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (88): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (89): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (90): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (91): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (92): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (93): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (94): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (95): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (96): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (97): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (98): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (99): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (100): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (101): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (102): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (103): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (104): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (105): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (106): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (107): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (108): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (109): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (110): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (111): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (112): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (113): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (114): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (115): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (116): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (117): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (118): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (119): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (120): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (121): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (122): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (123): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (124): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (125): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (126): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (127): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (128): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (129): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (130): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (131): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (132): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (133): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (134): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (135): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (136): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (137): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (138): Parameter containing: [torch.float32 of size 192x288x3x3 (cuda:0)]\n",
            "      (139): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (140): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (141): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (142): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (143): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (144): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (145): Parameter containing: [torch.float32 of size 192x288x1x1 (cuda:0)]\n",
            "      (146): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (147): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (148): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (149): Parameter containing: [torch.float32 of size 96x288x3x3 (cuda:0)]\n",
            "      (150): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (151): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (152): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (153): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (154): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (155): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (156): Parameter containing: [torch.float32 of size 96x288x1x1 (cuda:0)]\n",
            "      (157): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (158): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (159): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (160): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (161): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (162): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (163): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (164): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (165): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (166): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (167): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (168): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (169): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (170): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (171): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (172): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (173): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (174): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (175): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (176): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (177): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (178): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (179): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (180): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (181): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (182): Parameter containing: [torch.float32 of size 771x96x3x3 (cuda:0)]\n",
            "      (183): Parameter containing: [torch.float32 of size 771 (cuda:0)]\n",
            "  )\n",
            ")\n",
            "2025-05-28 22:56:08 INFO     Learning rate: 1.00e-04\n",
            "2025-05-28 22:56:08 INFO     Accumulate grad iterations: 1\n",
            "2025-05-28 22:56:08 INFO     Effective batch size: 32\n",
            "2025-05-28 22:56:08 INFO     Optimizer: AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.95]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "2025-05-28 22:56:08 INFO     Learning-Rate Schedule: <torch.optim.lr_scheduler.ConstantLR object at 0x7ca16ba23410>\n",
            "/content/flow_matching/examples/image/training/grad_scaler.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "Resume checkpoint /content/drive/MyDrive/flow_matching_checkpoints_29M/checkpoint.pth\n",
            "With optim & sched!\n",
            "2025-05-28 22:56:08 INFO     Start from 1 to 500 epochs\n",
            "2025-05-28 22:56:11 INFO     Epoch 1 [0/1875]: loss = 5.495797634124756, lr = 0.0001\n",
            "2025-05-28 22:56:18 INFO     Epoch 1 [50/1875]: loss = 1.8657158613204956, lr = 0.0001\n",
            "2025-05-28 22:56:24 INFO     Epoch 1 [100/1875]: loss = 0.5508366823196411, lr = 0.0001\n",
            "2025-05-28 22:56:31 INFO     Epoch 1 [150/1875]: loss = 0.4266854226589203, lr = 0.0001\n",
            "2025-05-28 22:56:38 INFO     Epoch 1 [200/1875]: loss = 0.3380511999130249, lr = 0.0001\n",
            "2025-05-28 22:56:44 INFO     Epoch 1 [250/1875]: loss = 0.37016892433166504, lr = 0.0001\n",
            "2025-05-28 22:56:51 INFO     Epoch 1 [300/1875]: loss = 0.4008573889732361, lr = 0.0001\n",
            "2025-05-28 22:56:57 INFO     Epoch 1 [350/1875]: loss = 0.3196432590484619, lr = 0.0001\n",
            "2025-05-28 22:57:04 INFO     Epoch 1 [400/1875]: loss = 0.3242473900318146, lr = 0.0001\n",
            "2025-05-28 22:57:11 INFO     Epoch 1 [450/1875]: loss = 0.34642601013183594, lr = 0.0001\n",
            "2025-05-28 22:57:17 INFO     Epoch 1 [500/1875]: loss = 0.33755165338516235, lr = 0.0001\n",
            "2025-05-28 22:57:24 INFO     Epoch 1 [550/1875]: loss = 0.2943335473537445, lr = 0.0001\n",
            "2025-05-28 22:57:31 INFO     Epoch 1 [600/1875]: loss = 0.3257301151752472, lr = 0.0001\n",
            "2025-05-28 22:57:38 INFO     Epoch 1 [650/1875]: loss = 0.35383275151252747, lr = 0.0001\n",
            "2025-05-28 22:57:44 INFO     Epoch 1 [700/1875]: loss = 0.34746789932250977, lr = 0.0001\n",
            "2025-05-28 22:57:51 INFO     Epoch 1 [750/1875]: loss = 0.2965478301048279, lr = 0.0001\n",
            "2025-05-28 22:57:58 INFO     Epoch 1 [800/1875]: loss = 0.3274267315864563, lr = 0.0001\n",
            "2025-05-28 22:58:05 INFO     Epoch 1 [850/1875]: loss = 0.3312246799468994, lr = 0.0001\n",
            "2025-05-28 22:58:11 INFO     Epoch 1 [900/1875]: loss = 0.3074815273284912, lr = 0.0001\n",
            "2025-05-28 22:58:18 INFO     Epoch 1 [950/1875]: loss = 0.3615601658821106, lr = 0.0001\n",
            "2025-05-28 22:58:25 INFO     Epoch 1 [1000/1875]: loss = 0.3177466094493866, lr = 0.0001\n",
            "2025-05-28 22:58:32 INFO     Epoch 1 [1050/1875]: loss = 0.3369000256061554, lr = 0.0001\n",
            "2025-05-28 22:58:39 INFO     Epoch 1 [1100/1875]: loss = 0.3204338550567627, lr = 0.0001\n",
            "2025-05-28 22:58:45 INFO     Epoch 1 [1150/1875]: loss = 0.299235999584198, lr = 0.0001\n",
            "2025-05-28 22:58:52 INFO     Epoch 1 [1200/1875]: loss = 0.34030500054359436, lr = 0.0001\n",
            "2025-05-28 22:58:59 INFO     Epoch 1 [1250/1875]: loss = 0.29008832573890686, lr = 0.0001\n",
            "2025-05-28 22:59:06 INFO     Epoch 1 [1300/1875]: loss = 0.2877475619316101, lr = 0.0001\n",
            "2025-05-28 22:59:13 INFO     Epoch 1 [1350/1875]: loss = 0.26427513360977173, lr = 0.0001\n",
            "2025-05-28 22:59:20 INFO     Epoch 1 [1400/1875]: loss = 0.27593696117401123, lr = 0.0001\n",
            "2025-05-28 22:59:26 INFO     Epoch 1 [1450/1875]: loss = 0.31922149658203125, lr = 0.0001\n",
            "2025-05-28 22:59:33 INFO     Epoch 1 [1500/1875]: loss = 0.2581949532032013, lr = 0.0001\n",
            "2025-05-28 22:59:40 INFO     Epoch 1 [1550/1875]: loss = 0.28607508540153503, lr = 0.0001\n",
            "2025-05-28 22:59:47 INFO     Epoch 1 [1600/1875]: loss = 0.26845085620880127, lr = 0.0001\n",
            "2025-05-28 22:59:54 INFO     Epoch 1 [1650/1875]: loss = 0.2977391183376312, lr = 0.0001\n",
            "2025-05-28 23:00:01 INFO     Epoch 1 [1700/1875]: loss = 0.3183140158653259, lr = 0.0001\n",
            "2025-05-28 23:00:07 INFO     Epoch 1 [1750/1875]: loss = 0.3287712633609772, lr = 0.0001\n",
            "2025-05-28 23:00:15 INFO     Epoch 1 [1800/1875]: loss = 0.21227379143238068, lr = 0.0001\n",
            "2025-05-28 23:00:22 INFO     Epoch 1 [1850/1875]: loss = 0.28008195757865906, lr = 0.0001\n",
            "2025-05-28 23:00:25 INFO     Epoch 2 [0/1875]: loss = 0.25077271461486816, lr = 0.0001\n",
            "2025-05-28 23:00:32 INFO     Epoch 2 [50/1875]: loss = 0.27830761671066284, lr = 0.0001\n",
            "2025-05-28 23:00:39 INFO     Epoch 2 [100/1875]: loss = 0.28013086318969727, lr = 0.0001\n",
            "2025-05-28 23:00:46 INFO     Epoch 2 [150/1875]: loss = 0.2503674626350403, lr = 0.0001\n",
            "2025-05-28 23:00:53 INFO     Epoch 2 [200/1875]: loss = 0.2717350423336029, lr = 0.0001\n",
            "2025-05-28 23:01:00 INFO     Epoch 2 [250/1875]: loss = 0.3077413737773895, lr = 0.0001\n",
            "2025-05-28 23:01:07 INFO     Epoch 2 [300/1875]: loss = 0.3021695017814636, lr = 0.0001\n",
            "2025-05-28 23:01:13 INFO     Epoch 2 [350/1875]: loss = 0.15635590255260468, lr = 0.0001\n",
            "2025-05-28 23:01:20 INFO     Epoch 2 [400/1875]: loss = 0.252714067697525, lr = 0.0001\n",
            "2025-05-28 23:01:27 INFO     Epoch 2 [450/1875]: loss = 0.23599012196063995, lr = 0.0001\n",
            "2025-05-28 23:01:34 INFO     Epoch 2 [500/1875]: loss = 0.29725581407546997, lr = 0.0001\n",
            "2025-05-28 23:01:41 INFO     Epoch 2 [550/1875]: loss = 0.23603036999702454, lr = 0.0001\n",
            "2025-05-28 23:01:48 INFO     Epoch 2 [600/1875]: loss = 0.22881481051445007, lr = 0.0001\n",
            "2025-05-28 23:01:55 INFO     Epoch 2 [650/1875]: loss = 0.24216775596141815, lr = 0.0001\n",
            "2025-05-28 23:02:02 INFO     Epoch 2 [700/1875]: loss = 0.22838445007801056, lr = 0.0001\n",
            "2025-05-28 23:02:08 INFO     Epoch 2 [750/1875]: loss = 0.20623792707920074, lr = 0.0001\n",
            "2025-05-28 23:02:15 INFO     Epoch 2 [800/1875]: loss = 0.22241239249706268, lr = 0.0001\n",
            "2025-05-28 23:02:22 INFO     Epoch 2 [850/1875]: loss = 0.265638142824173, lr = 0.0001\n",
            "2025-05-28 23:02:29 INFO     Epoch 2 [900/1875]: loss = 0.21038690209388733, lr = 0.0001\n",
            "2025-05-28 23:02:36 INFO     Epoch 2 [950/1875]: loss = 0.2535448372364044, lr = 0.0001\n",
            "2025-05-28 23:02:43 INFO     Epoch 2 [1000/1875]: loss = 0.23036116361618042, lr = 0.0001\n",
            "2025-05-28 23:02:50 INFO     Epoch 2 [1050/1875]: loss = 0.2173936367034912, lr = 0.0001\n",
            "2025-05-28 23:02:57 INFO     Epoch 2 [1100/1875]: loss = 0.2428886741399765, lr = 0.0001\n",
            "2025-05-28 23:03:04 INFO     Epoch 2 [1150/1875]: loss = 0.20972922444343567, lr = 0.0001\n",
            "2025-05-28 23:03:11 INFO     Epoch 2 [1200/1875]: loss = 0.2544845640659332, lr = 0.0001\n",
            "2025-05-28 23:03:18 INFO     Epoch 2 [1250/1875]: loss = 0.17912538349628448, lr = 0.0001\n",
            "2025-05-28 23:03:24 INFO     Epoch 2 [1300/1875]: loss = 0.2370583415031433, lr = 0.0001\n",
            "2025-05-28 23:03:31 INFO     Epoch 2 [1350/1875]: loss = 0.22886209189891815, lr = 0.0001\n",
            "2025-05-28 23:03:38 INFO     Epoch 2 [1400/1875]: loss = 0.1984703093767166, lr = 0.0001\n",
            "2025-05-28 23:03:45 INFO     Epoch 2 [1450/1875]: loss = 0.25806716084480286, lr = 0.0001\n",
            "2025-05-28 23:03:52 INFO     Epoch 2 [1500/1875]: loss = 0.20310817658901215, lr = 0.0001\n",
            "2025-05-28 23:03:59 INFO     Epoch 2 [1550/1875]: loss = 0.23328685760498047, lr = 0.0001\n",
            "2025-05-28 23:04:06 INFO     Epoch 2 [1600/1875]: loss = 0.2524271309375763, lr = 0.0001\n",
            "2025-05-28 23:04:13 INFO     Epoch 2 [1650/1875]: loss = 0.279944509267807, lr = 0.0001\n",
            "2025-05-28 23:04:19 INFO     Epoch 2 [1700/1875]: loss = 0.2548080384731293, lr = 0.0001\n",
            "2025-05-28 23:04:26 INFO     Epoch 2 [1750/1875]: loss = 0.21026615798473358, lr = 0.0001\n",
            "2025-05-28 23:04:33 INFO     Epoch 2 [1800/1875]: loss = 0.19228407740592957, lr = 0.0001\n",
            "2025-05-28 23:04:40 INFO     Epoch 2 [1850/1875]: loss = 0.25356006622314453, lr = 0.0001\n",
            "2025-05-28 23:04:44 INFO     Epoch 3 [0/1875]: loss = 0.16673189401626587, lr = 0.0001\n",
            "2025-05-28 23:04:51 INFO     Epoch 3 [50/1875]: loss = 0.150888592004776, lr = 0.0001\n",
            "2025-05-28 23:04:58 INFO     Epoch 3 [100/1875]: loss = 0.2214074730873108, lr = 0.0001\n",
            "2025-05-28 23:05:04 INFO     Epoch 3 [150/1875]: loss = 0.25582244992256165, lr = 0.0001\n",
            "2025-05-28 23:05:11 INFO     Epoch 3 [200/1875]: loss = 0.25560280680656433, lr = 0.0001\n",
            "2025-05-28 23:05:18 INFO     Epoch 3 [250/1875]: loss = 0.25084584951400757, lr = 0.0001\n",
            "2025-05-28 23:05:25 INFO     Epoch 3 [300/1875]: loss = 0.185169979929924, lr = 0.0001\n",
            "2025-05-28 23:05:32 INFO     Epoch 3 [350/1875]: loss = 0.15940500795841217, lr = 0.0001\n",
            "2025-05-28 23:05:39 INFO     Epoch 3 [400/1875]: loss = 0.21572278439998627, lr = 0.0001\n",
            "2025-05-28 23:05:46 INFO     Epoch 3 [450/1875]: loss = 0.20163187384605408, lr = 0.0001\n",
            "2025-05-28 23:05:53 INFO     Epoch 3 [500/1875]: loss = 0.21281737089157104, lr = 0.0001\n",
            "2025-05-28 23:06:00 INFO     Epoch 3 [550/1875]: loss = 0.1939060240983963, lr = 0.0001\n",
            "2025-05-28 23:06:07 INFO     Epoch 3 [600/1875]: loss = 0.1595262736082077, lr = 0.0001\n",
            "2025-05-28 23:06:14 INFO     Epoch 3 [650/1875]: loss = 0.1959400624036789, lr = 0.0001\n",
            "2025-05-28 23:06:20 INFO     Epoch 3 [700/1875]: loss = 0.15864907205104828, lr = 0.0001\n",
            "2025-05-28 23:06:27 INFO     Epoch 3 [750/1875]: loss = 0.19832992553710938, lr = 0.0001\n",
            "2025-05-28 23:06:34 INFO     Epoch 3 [800/1875]: loss = 0.21563532948493958, lr = 0.0001\n",
            "2025-05-28 23:06:41 INFO     Epoch 3 [850/1875]: loss = 0.22722218930721283, lr = 0.0001\n",
            "2025-05-28 23:06:48 INFO     Epoch 3 [900/1875]: loss = 0.1891552209854126, lr = 0.0001\n",
            "2025-05-28 23:06:55 INFO     Epoch 3 [950/1875]: loss = 0.21994465589523315, lr = 0.0001\n",
            "2025-05-28 23:07:02 INFO     Epoch 3 [1000/1875]: loss = 0.2291382998228073, lr = 0.0001\n",
            "2025-05-28 23:07:09 INFO     Epoch 3 [1050/1875]: loss = 0.21028706431388855, lr = 0.0001\n",
            "2025-05-28 23:07:16 INFO     Epoch 3 [1100/1875]: loss = 0.2255229353904724, lr = 0.0001\n",
            "2025-05-28 23:07:22 INFO     Epoch 3 [1150/1875]: loss = 0.22882528603076935, lr = 0.0001\n",
            "2025-05-28 23:07:29 INFO     Epoch 3 [1200/1875]: loss = 0.2042345404624939, lr = 0.0001\n",
            "2025-05-28 23:07:36 INFO     Epoch 3 [1250/1875]: loss = 0.18415763974189758, lr = 0.0001\n",
            "2025-05-28 23:07:43 INFO     Epoch 3 [1300/1875]: loss = 0.16748636960983276, lr = 0.0001\n",
            "2025-05-28 23:07:50 INFO     Epoch 3 [1350/1875]: loss = 0.30891844630241394, lr = 0.0001\n",
            "2025-05-28 23:07:57 INFO     Epoch 3 [1400/1875]: loss = 0.20403718948364258, lr = 0.0001\n",
            "2025-05-28 23:08:04 INFO     Epoch 3 [1450/1875]: loss = 0.2400287389755249, lr = 0.0001\n",
            "2025-05-28 23:08:11 INFO     Epoch 3 [1500/1875]: loss = 0.1931251585483551, lr = 0.0001\n",
            "2025-05-28 23:08:17 INFO     Epoch 3 [1550/1875]: loss = 0.1615114063024521, lr = 0.0001\n",
            "2025-05-28 23:08:24 INFO     Epoch 3 [1600/1875]: loss = 0.2320639193058014, lr = 0.0001\n",
            "2025-05-28 23:08:31 INFO     Epoch 3 [1650/1875]: loss = 0.2447848916053772, lr = 0.0001\n",
            "2025-05-28 23:08:38 INFO     Epoch 3 [1700/1875]: loss = 0.19747820496559143, lr = 0.0001\n",
            "2025-05-28 23:08:45 INFO     Epoch 3 [1750/1875]: loss = 0.22219239175319672, lr = 0.0001\n",
            "2025-05-28 23:08:52 INFO     Epoch 3 [1800/1875]: loss = 0.1645798236131668, lr = 0.0001\n",
            "2025-05-28 23:08:59 INFO     Epoch 3 [1850/1875]: loss = 0.17147214710712433, lr = 0.0001\n",
            "2025-05-28 23:09:03 INFO     Epoch 4 [0/1875]: loss = 0.18691977858543396, lr = 0.0001\n",
            "2025-05-28 23:09:10 INFO     Epoch 4 [50/1875]: loss = 0.21688787639141083, lr = 0.0001\n",
            "2025-05-28 23:09:16 INFO     Epoch 4 [100/1875]: loss = 0.2700200378894806, lr = 0.0001\n",
            "2025-05-28 23:09:23 INFO     Epoch 4 [150/1875]: loss = 0.21713565289974213, lr = 0.0001\n",
            "2025-05-28 23:09:30 INFO     Epoch 4 [200/1875]: loss = 0.18424852192401886, lr = 0.0001\n",
            "2025-05-28 23:09:37 INFO     Epoch 4 [250/1875]: loss = 0.19241328537464142, lr = 0.0001\n",
            "2025-05-28 23:09:44 INFO     Epoch 4 [300/1875]: loss = 0.2568046450614929, lr = 0.0001\n",
            "2025-05-28 23:09:51 INFO     Epoch 4 [350/1875]: loss = 0.15458975732326508, lr = 0.0001\n",
            "2025-05-28 23:09:58 INFO     Epoch 4 [400/1875]: loss = 0.16578349471092224, lr = 0.0001\n",
            "2025-05-28 23:10:05 INFO     Epoch 4 [450/1875]: loss = 0.207939013838768, lr = 0.0001\n",
            "2025-05-28 23:10:12 INFO     Epoch 4 [500/1875]: loss = 0.21101854741573334, lr = 0.0001\n",
            "2025-05-28 23:10:18 INFO     Epoch 4 [550/1875]: loss = 0.20706166326999664, lr = 0.0001\n",
            "2025-05-28 23:10:25 INFO     Epoch 4 [600/1875]: loss = 0.23706156015396118, lr = 0.0001\n",
            "2025-05-28 23:10:32 INFO     Epoch 4 [650/1875]: loss = 0.15502138435840607, lr = 0.0001\n",
            "2025-05-28 23:10:39 INFO     Epoch 4 [700/1875]: loss = 0.1979510486125946, lr = 0.0001\n",
            "2025-05-28 23:10:46 INFO     Epoch 4 [750/1875]: loss = 0.24888908863067627, lr = 0.0001\n",
            "2025-05-28 23:10:53 INFO     Epoch 4 [800/1875]: loss = 0.19533079862594604, lr = 0.0001\n",
            "2025-05-28 23:10:59 INFO     Epoch 4 [850/1875]: loss = 0.18614909052848816, lr = 0.0001\n",
            "2025-05-28 23:11:06 INFO     Epoch 4 [900/1875]: loss = 0.1579579859972, lr = 0.0001\n",
            "2025-05-28 23:11:13 INFO     Epoch 4 [950/1875]: loss = 0.20248505473136902, lr = 0.0001\n",
            "2025-05-28 23:11:20 INFO     Epoch 4 [1000/1875]: loss = 0.17199403047561646, lr = 0.0001\n",
            "2025-05-28 23:11:27 INFO     Epoch 4 [1050/1875]: loss = 0.19095070660114288, lr = 0.0001\n",
            "2025-05-28 23:11:34 INFO     Epoch 4 [1100/1875]: loss = 0.18555070459842682, lr = 0.0001\n",
            "2025-05-28 23:11:41 INFO     Epoch 4 [1150/1875]: loss = 0.17616286873817444, lr = 0.0001\n",
            "2025-05-28 23:11:48 INFO     Epoch 4 [1200/1875]: loss = 0.24144510924816132, lr = 0.0001\n",
            "2025-05-28 23:11:55 INFO     Epoch 4 [1250/1875]: loss = 0.14948520064353943, lr = 0.0001\n",
            "2025-05-28 23:12:02 INFO     Epoch 4 [1300/1875]: loss = 0.19062083959579468, lr = 0.0001\n",
            "2025-05-28 23:12:08 INFO     Epoch 4 [1350/1875]: loss = 0.2539803683757782, lr = 0.0001\n",
            "2025-05-28 23:12:15 INFO     Epoch 4 [1400/1875]: loss = 0.22581404447555542, lr = 0.0001\n",
            "2025-05-28 23:12:22 INFO     Epoch 4 [1450/1875]: loss = 0.1763375997543335, lr = 0.0001\n",
            "2025-05-28 23:12:29 INFO     Epoch 4 [1500/1875]: loss = 0.18117739260196686, lr = 0.0001\n",
            "2025-05-28 23:12:36 INFO     Epoch 4 [1550/1875]: loss = 0.24177324771881104, lr = 0.0001\n",
            "2025-05-28 23:12:43 INFO     Epoch 4 [1600/1875]: loss = 0.14920532703399658, lr = 0.0001\n",
            "2025-05-28 23:12:50 INFO     Epoch 4 [1650/1875]: loss = 0.2672554850578308, lr = 0.0001\n",
            "2025-05-28 23:12:56 INFO     Epoch 4 [1700/1875]: loss = 0.25303176045417786, lr = 0.0001\n",
            "2025-05-28 23:13:03 INFO     Epoch 4 [1750/1875]: loss = 0.2138081043958664, lr = 0.0001\n",
            "2025-05-28 23:13:10 INFO     Epoch 4 [1800/1875]: loss = 0.185430109500885, lr = 0.0001\n",
            "2025-05-28 23:13:17 INFO     Epoch 4 [1850/1875]: loss = 0.22979611158370972, lr = 0.0001\n",
            "2025-05-28 23:13:25 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "/content/flow_matching/examples/image/training/eval_loop.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(), torch.no_grad():\n",
            "2025-05-28 23:14:10 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:14:15 INFO     Evaluating [0/313] samples generated [32/1000] running fid 231.21194458007812\n",
            "2025-05-28 23:14:55 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:15:35 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:16:16 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:16:56 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:17:36 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:18:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:18:57 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:19:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:20:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:20:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:21:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:22:19 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:22:59 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:23:39 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:24:19 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:25:00 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:25:40 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:26:20 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:27:00 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:27:41 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:28:21 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:29:02 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:29:42 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:30:22 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:31:03 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:31:43 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:32:23 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:33:03 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:33:43 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:34:23 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:35:03 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:35:10 INFO     Evaluating [50/313] samples generated [1000/1000] running fid 184.3208465576172\n",
            "2025-05-28 23:35:21 INFO     Evaluating [100/313] samples generated [1000/1000] running fid 183.8041229248047\n",
            "2025-05-28 23:35:31 INFO     Evaluating [150/313] samples generated [1000/1000] running fid 183.43228149414062\n",
            "2025-05-28 23:35:42 INFO     Evaluating [200/313] samples generated [1000/1000] running fid 183.47775268554688\n",
            "2025-05-28 23:35:53 INFO     Evaluating [250/313] samples generated [1000/1000] running fid 183.9716033935547\n",
            "2025-05-28 23:36:03 INFO     Evaluating [300/313] samples generated [1000/1000] running fid 184.06797790527344\n",
            "2025-05-28 23:36:11 INFO     EMA: Switching from eval to train, restoring saved parameters\n",
            "2025-05-28 23:36:11 INFO     Epoch 5 [0/1875]: loss = 0.17379093170166016, lr = 0.0001\n",
            "2025-05-28 23:36:18 INFO     Epoch 5 [50/1875]: loss = 0.21653532981872559, lr = 0.0001\n",
            "2025-05-28 23:36:25 INFO     Epoch 5 [100/1875]: loss = 0.17937253415584564, lr = 0.0001\n",
            "2025-05-28 23:36:31 INFO     Epoch 5 [150/1875]: loss = 0.24896305799484253, lr = 0.0001\n",
            "2025-05-28 23:36:38 INFO     Epoch 5 [200/1875]: loss = 0.16472162306308746, lr = 0.0001\n",
            "2025-05-28 23:36:45 INFO     Epoch 5 [250/1875]: loss = 0.22719275951385498, lr = 0.0001\n",
            "2025-05-28 23:36:52 INFO     Epoch 5 [300/1875]: loss = 0.246846541762352, lr = 0.0001\n",
            "2025-05-28 23:36:59 INFO     Epoch 5 [350/1875]: loss = 0.17856164276599884, lr = 0.0001\n",
            "2025-05-28 23:37:06 INFO     Epoch 5 [400/1875]: loss = 0.19406290352344513, lr = 0.0001\n",
            "2025-05-28 23:37:12 INFO     Epoch 5 [450/1875]: loss = 0.20673006772994995, lr = 0.0001\n",
            "2025-05-28 23:37:19 INFO     Epoch 5 [500/1875]: loss = 0.2668211758136749, lr = 0.0001\n",
            "2025-05-28 23:37:26 INFO     Epoch 5 [550/1875]: loss = 0.20689643919467926, lr = 0.0001\n",
            "2025-05-28 23:37:33 INFO     Epoch 5 [600/1875]: loss = 0.20749031007289886, lr = 0.0001\n",
            "2025-05-28 23:37:40 INFO     Epoch 5 [650/1875]: loss = 0.23689015209674835, lr = 0.0001\n",
            "2025-05-28 23:37:47 INFO     Epoch 5 [700/1875]: loss = 0.22653242945671082, lr = 0.0001\n",
            "2025-05-28 23:37:53 INFO     Epoch 5 [750/1875]: loss = 0.24719172716140747, lr = 0.0001\n",
            "2025-05-28 23:38:00 INFO     Epoch 5 [800/1875]: loss = 0.27703362703323364, lr = 0.0001\n",
            "2025-05-28 23:38:07 INFO     Epoch 5 [850/1875]: loss = 0.23157213628292084, lr = 0.0001\n",
            "2025-05-28 23:38:14 INFO     Epoch 5 [900/1875]: loss = 0.19818881154060364, lr = 0.0001\n",
            "2025-05-28 23:38:21 INFO     Epoch 5 [950/1875]: loss = 0.17863179743289948, lr = 0.0001\n",
            "2025-05-28 23:38:28 INFO     Epoch 5 [1000/1875]: loss = 0.29483166337013245, lr = 0.0001\n",
            "2025-05-28 23:38:35 INFO     Epoch 5 [1050/1875]: loss = 0.17820683121681213, lr = 0.0001\n",
            "2025-05-28 23:38:42 INFO     Epoch 5 [1100/1875]: loss = 0.2330387383699417, lr = 0.0001\n",
            "2025-05-28 23:38:49 INFO     Epoch 5 [1150/1875]: loss = 0.20617838203907013, lr = 0.0001\n",
            "2025-05-28 23:38:55 INFO     Epoch 5 [1200/1875]: loss = 0.2149946391582489, lr = 0.0001\n",
            "2025-05-28 23:39:02 INFO     Epoch 5 [1250/1875]: loss = 0.25453731417655945, lr = 0.0001\n",
            "2025-05-28 23:39:09 INFO     Epoch 5 [1300/1875]: loss = 0.1956346333026886, lr = 0.0001\n",
            "2025-05-28 23:39:16 INFO     Epoch 5 [1350/1875]: loss = 0.23780986666679382, lr = 0.0001\n",
            "2025-05-28 23:39:23 INFO     Epoch 5 [1400/1875]: loss = 0.17721985280513763, lr = 0.0001\n",
            "2025-05-28 23:39:30 INFO     Epoch 5 [1450/1875]: loss = 0.2953609824180603, lr = 0.0001\n",
            "2025-05-28 23:39:36 INFO     Epoch 5 [1500/1875]: loss = 0.2104768306016922, lr = 0.0001\n",
            "2025-05-28 23:39:43 INFO     Epoch 5 [1550/1875]: loss = 0.12423642724752426, lr = 0.0001\n",
            "2025-05-28 23:39:50 INFO     Epoch 5 [1600/1875]: loss = 0.1652807593345642, lr = 0.0001\n",
            "2025-05-28 23:39:57 INFO     Epoch 5 [1650/1875]: loss = 0.2135716825723648, lr = 0.0001\n",
            "2025-05-28 23:40:04 INFO     Epoch 5 [1700/1875]: loss = 0.2546100914478302, lr = 0.0001\n",
            "2025-05-28 23:40:11 INFO     Epoch 5 [1750/1875]: loss = 0.19816939532756805, lr = 0.0001\n",
            "2025-05-28 23:40:17 INFO     Epoch 5 [1800/1875]: loss = 0.15102770924568176, lr = 0.0001\n",
            "2025-05-28 23:40:24 INFO     Epoch 5 [1850/1875]: loss = 0.22642947733402252, lr = 0.0001\n",
            "2025-05-28 23:40:28 INFO     Epoch 6 [0/1875]: loss = 0.15336573123931885, lr = 0.0001\n",
            "2025-05-28 23:40:35 INFO     Epoch 6 [50/1875]: loss = 0.15400144457817078, lr = 0.0001\n",
            "2025-05-28 23:40:42 INFO     Epoch 6 [100/1875]: loss = 0.19294743239879608, lr = 0.0001\n",
            "2025-05-28 23:40:49 INFO     Epoch 6 [150/1875]: loss = 0.26477962732315063, lr = 0.0001\n",
            "2025-05-28 23:40:56 INFO     Epoch 6 [200/1875]: loss = 0.18351195752620697, lr = 0.0001\n",
            "2025-05-28 23:41:02 INFO     Epoch 6 [250/1875]: loss = 0.194522425532341, lr = 0.0001\n",
            "2025-05-28 23:41:09 INFO     Epoch 6 [300/1875]: loss = 0.22136642038822174, lr = 0.0001\n",
            "2025-05-28 23:41:16 INFO     Epoch 6 [350/1875]: loss = 0.19901162385940552, lr = 0.0001\n",
            "2025-05-28 23:41:23 INFO     Epoch 6 [400/1875]: loss = 0.2300771325826645, lr = 0.0001\n",
            "2025-05-28 23:41:30 INFO     Epoch 6 [450/1875]: loss = 0.1968139261007309, lr = 0.0001\n",
            "2025-05-28 23:41:37 INFO     Epoch 6 [500/1875]: loss = 0.2176857441663742, lr = 0.0001\n",
            "2025-05-28 23:41:44 INFO     Epoch 6 [550/1875]: loss = 0.20544518530368805, lr = 0.0001\n",
            "2025-05-28 23:41:50 INFO     Epoch 6 [600/1875]: loss = 0.25532111525535583, lr = 0.0001\n",
            "2025-05-28 23:41:57 INFO     Epoch 6 [650/1875]: loss = 0.1790057122707367, lr = 0.0001\n",
            "2025-05-28 23:42:04 INFO     Epoch 6 [700/1875]: loss = 0.19336815178394318, lr = 0.0001\n",
            "2025-05-28 23:42:11 INFO     Epoch 6 [750/1875]: loss = 0.2604447305202484, lr = 0.0001\n",
            "2025-05-28 23:42:18 INFO     Epoch 6 [800/1875]: loss = 0.2385191172361374, lr = 0.0001\n",
            "2025-05-28 23:42:25 INFO     Epoch 6 [850/1875]: loss = 0.20574340224266052, lr = 0.0001\n",
            "2025-05-28 23:42:31 INFO     Epoch 6 [900/1875]: loss = 0.20804773271083832, lr = 0.0001\n",
            "2025-05-28 23:42:38 INFO     Epoch 6 [950/1875]: loss = 0.2424754947423935, lr = 0.0001\n",
            "2025-05-28 23:42:45 INFO     Epoch 6 [1000/1875]: loss = 0.10117904841899872, lr = 0.0001\n",
            "2025-05-28 23:42:52 INFO     Epoch 6 [1050/1875]: loss = 0.23496036231517792, lr = 0.0001\n",
            "2025-05-28 23:42:59 INFO     Epoch 6 [1100/1875]: loss = 0.2560316324234009, lr = 0.0001\n",
            "2025-05-28 23:43:05 INFO     Epoch 6 [1150/1875]: loss = 0.1489402949810028, lr = 0.0001\n",
            "2025-05-28 23:43:12 INFO     Epoch 6 [1200/1875]: loss = 0.20775407552719116, lr = 0.0001\n",
            "2025-05-28 23:43:19 INFO     Epoch 6 [1250/1875]: loss = 0.17516151070594788, lr = 0.0001\n",
            "2025-05-28 23:43:26 INFO     Epoch 6 [1300/1875]: loss = 0.19900135695934296, lr = 0.0001\n",
            "2025-05-28 23:43:33 INFO     Epoch 6 [1350/1875]: loss = 0.21099959313869476, lr = 0.0001\n",
            "2025-05-28 23:43:40 INFO     Epoch 6 [1400/1875]: loss = 0.22793921828269958, lr = 0.0001\n",
            "2025-05-28 23:43:46 INFO     Epoch 6 [1450/1875]: loss = 0.22237245738506317, lr = 0.0001\n",
            "2025-05-28 23:43:53 INFO     Epoch 6 [1500/1875]: loss = 0.20483674108982086, lr = 0.0001\n",
            "2025-05-28 23:44:00 INFO     Epoch 6 [1550/1875]: loss = 0.20709262788295746, lr = 0.0001\n",
            "2025-05-28 23:44:07 INFO     Epoch 6 [1600/1875]: loss = 0.15266568958759308, lr = 0.0001\n",
            "2025-05-28 23:44:14 INFO     Epoch 6 [1650/1875]: loss = 0.20448781549930573, lr = 0.0001\n",
            "2025-05-28 23:44:21 INFO     Epoch 6 [1700/1875]: loss = 0.32127007842063904, lr = 0.0001\n",
            "2025-05-28 23:44:28 INFO     Epoch 6 [1750/1875]: loss = 0.17757752537727356, lr = 0.0001\n",
            "2025-05-28 23:44:35 INFO     Epoch 6 [1800/1875]: loss = 0.18660321831703186, lr = 0.0001\n",
            "2025-05-28 23:44:41 INFO     Epoch 6 [1850/1875]: loss = 0.17220650613307953, lr = 0.0001\n",
            "2025-05-28 23:44:45 INFO     Epoch 7 [0/1875]: loss = 0.19034676253795624, lr = 0.0001\n",
            "2025-05-28 23:44:52 INFO     Epoch 7 [50/1875]: loss = 0.23891589045524597, lr = 0.0001\n",
            "2025-05-28 23:44:59 INFO     Epoch 7 [100/1875]: loss = 0.16548117995262146, lr = 0.0001\n",
            "2025-05-28 23:45:06 INFO     Epoch 7 [150/1875]: loss = 0.22732722759246826, lr = 0.0001\n",
            "2025-05-28 23:45:13 INFO     Epoch 7 [200/1875]: loss = 0.19474206864833832, lr = 0.0001\n",
            "2025-05-28 23:45:19 INFO     Epoch 7 [250/1875]: loss = 0.2466772198677063, lr = 0.0001\n",
            "2025-05-28 23:45:26 INFO     Epoch 7 [300/1875]: loss = 0.20874862372875214, lr = 0.0001\n",
            "2025-05-28 23:45:33 INFO     Epoch 7 [350/1875]: loss = 0.16633901000022888, lr = 0.0001\n",
            "2025-05-28 23:45:40 INFO     Epoch 7 [400/1875]: loss = 0.21586552262306213, lr = 0.0001\n",
            "2025-05-28 23:45:47 INFO     Epoch 7 [450/1875]: loss = 0.19295752048492432, lr = 0.0001\n",
            "2025-05-28 23:45:54 INFO     Epoch 7 [500/1875]: loss = 0.1724836379289627, lr = 0.0001\n",
            "2025-05-28 23:46:00 INFO     Epoch 7 [550/1875]: loss = 0.2151893824338913, lr = 0.0001\n",
            "2025-05-28 23:46:07 INFO     Epoch 7 [600/1875]: loss = 0.20680080354213715, lr = 0.0001\n",
            "2025-05-28 23:46:14 INFO     Epoch 7 [650/1875]: loss = 0.23919272422790527, lr = 0.0001\n",
            "2025-05-28 23:46:21 INFO     Epoch 7 [700/1875]: loss = 0.18131545186042786, lr = 0.0001\n",
            "2025-05-28 23:46:28 INFO     Epoch 7 [750/1875]: loss = 0.2296515256166458, lr = 0.0001\n",
            "2025-05-28 23:46:35 INFO     Epoch 7 [800/1875]: loss = 0.18432708084583282, lr = 0.0001\n",
            "2025-05-28 23:46:41 INFO     Epoch 7 [850/1875]: loss = 0.18601174652576447, lr = 0.0001\n",
            "2025-05-28 23:46:48 INFO     Epoch 7 [900/1875]: loss = 0.21294990181922913, lr = 0.0001\n",
            "2025-05-28 23:46:55 INFO     Epoch 7 [950/1875]: loss = 0.24229805171489716, lr = 0.0001\n",
            "2025-05-28 23:47:02 INFO     Epoch 7 [1000/1875]: loss = 0.21703940629959106, lr = 0.0001\n",
            "2025-05-28 23:47:09 INFO     Epoch 7 [1050/1875]: loss = 0.20760281383991241, lr = 0.0001\n",
            "2025-05-28 23:47:16 INFO     Epoch 7 [1100/1875]: loss = 0.20548151433467865, lr = 0.0001\n",
            "2025-05-28 23:47:23 INFO     Epoch 7 [1150/1875]: loss = 0.17034220695495605, lr = 0.0001\n",
            "2025-05-28 23:47:30 INFO     Epoch 7 [1200/1875]: loss = 0.1594640016555786, lr = 0.0001\n",
            "2025-05-28 23:47:36 INFO     Epoch 7 [1250/1875]: loss = 0.18100818991661072, lr = 0.0001\n",
            "2025-05-28 23:47:43 INFO     Epoch 7 [1300/1875]: loss = 0.2273179441690445, lr = 0.0001\n",
            "2025-05-28 23:47:50 INFO     Epoch 7 [1350/1875]: loss = 0.19481049478054047, lr = 0.0001\n",
            "2025-05-28 23:47:57 INFO     Epoch 7 [1400/1875]: loss = 0.16428792476654053, lr = 0.0001\n",
            "2025-05-28 23:48:04 INFO     Epoch 7 [1450/1875]: loss = 0.25859618186950684, lr = 0.0001\n",
            "2025-05-28 23:48:10 INFO     Epoch 7 [1500/1875]: loss = 0.16863657534122467, lr = 0.0001\n",
            "2025-05-28 23:48:17 INFO     Epoch 7 [1550/1875]: loss = 0.2022305577993393, lr = 0.0001\n",
            "2025-05-28 23:48:24 INFO     Epoch 7 [1600/1875]: loss = 0.213963121175766, lr = 0.0001\n",
            "2025-05-28 23:48:31 INFO     Epoch 7 [1650/1875]: loss = 0.2190280258655548, lr = 0.0001\n",
            "2025-05-28 23:48:38 INFO     Epoch 7 [1700/1875]: loss = 0.21058864891529083, lr = 0.0001\n",
            "2025-05-28 23:48:45 INFO     Epoch 7 [1750/1875]: loss = 0.19080445170402527, lr = 0.0001\n",
            "2025-05-28 23:48:51 INFO     Epoch 7 [1800/1875]: loss = 0.18038928508758545, lr = 0.0001\n",
            "2025-05-28 23:48:58 INFO     Epoch 7 [1850/1875]: loss = 0.18890367448329926, lr = 0.0001\n",
            "2025-05-28 23:49:02 INFO     Epoch 8 [0/1875]: loss = 0.14501890540122986, lr = 0.0001\n",
            "2025-05-28 23:49:09 INFO     Epoch 8 [50/1875]: loss = 0.23901382088661194, lr = 0.0001\n",
            "2025-05-28 23:49:16 INFO     Epoch 8 [100/1875]: loss = 0.21547512710094452, lr = 0.0001\n",
            "2025-05-28 23:49:23 INFO     Epoch 8 [150/1875]: loss = 0.22055760025978088, lr = 0.0001\n",
            "2025-05-28 23:49:29 INFO     Epoch 8 [200/1875]: loss = 0.21495307981967926, lr = 0.0001\n",
            "2025-05-28 23:49:36 INFO     Epoch 8 [250/1875]: loss = 0.22145888209342957, lr = 0.0001\n",
            "2025-05-28 23:49:43 INFO     Epoch 8 [300/1875]: loss = 0.2845897674560547, lr = 0.0001\n",
            "2025-05-28 23:49:50 INFO     Epoch 8 [350/1875]: loss = 0.166701540350914, lr = 0.0001\n",
            "2025-05-28 23:49:57 INFO     Epoch 8 [400/1875]: loss = 0.20175306499004364, lr = 0.0001\n",
            "2025-05-28 23:50:04 INFO     Epoch 8 [450/1875]: loss = 0.2571144998073578, lr = 0.0001\n",
            "2025-05-28 23:50:11 INFO     Epoch 8 [500/1875]: loss = 0.1458432674407959, lr = 0.0001\n",
            "2025-05-28 23:50:17 INFO     Epoch 8 [550/1875]: loss = 0.20300038158893585, lr = 0.0001\n",
            "2025-05-28 23:50:24 INFO     Epoch 8 [600/1875]: loss = 0.20446188747882843, lr = 0.0001\n",
            "2025-05-28 23:50:31 INFO     Epoch 8 [650/1875]: loss = 0.24077551066875458, lr = 0.0001\n",
            "2025-05-28 23:50:38 INFO     Epoch 8 [700/1875]: loss = 0.1805066019296646, lr = 0.0001\n",
            "2025-05-28 23:50:45 INFO     Epoch 8 [750/1875]: loss = 0.2058548480272293, lr = 0.0001\n",
            "2025-05-28 23:50:51 INFO     Epoch 8 [800/1875]: loss = 0.22784769535064697, lr = 0.0001\n",
            "2025-05-28 23:50:58 INFO     Epoch 8 [850/1875]: loss = 0.17208321392536163, lr = 0.0001\n",
            "2025-05-28 23:51:05 INFO     Epoch 8 [900/1875]: loss = 0.2252190262079239, lr = 0.0001\n",
            "2025-05-28 23:51:12 INFO     Epoch 8 [950/1875]: loss = 0.23320293426513672, lr = 0.0001\n",
            "2025-05-28 23:51:19 INFO     Epoch 8 [1000/1875]: loss = 0.23654203116893768, lr = 0.0001\n",
            "2025-05-28 23:51:25 INFO     Epoch 8 [1050/1875]: loss = 0.17033497989177704, lr = 0.0001\n",
            "2025-05-28 23:51:32 INFO     Epoch 8 [1100/1875]: loss = 0.1975538283586502, lr = 0.0001\n",
            "2025-05-28 23:51:39 INFO     Epoch 8 [1150/1875]: loss = 0.19920843839645386, lr = 0.0001\n",
            "2025-05-28 23:51:46 INFO     Epoch 8 [1200/1875]: loss = 0.17982348799705505, lr = 0.0001\n",
            "2025-05-28 23:51:53 INFO     Epoch 8 [1250/1875]: loss = 0.16800521314144135, lr = 0.0001\n",
            "2025-05-28 23:51:59 INFO     Epoch 8 [1300/1875]: loss = 0.20713579654693604, lr = 0.0001\n",
            "2025-05-28 23:52:06 INFO     Epoch 8 [1350/1875]: loss = 0.19315369427204132, lr = 0.0001\n",
            "2025-05-28 23:52:13 INFO     Epoch 8 [1400/1875]: loss = 0.21210898458957672, lr = 0.0001\n",
            "2025-05-28 23:52:20 INFO     Epoch 8 [1450/1875]: loss = 0.20461620390415192, lr = 0.0001\n",
            "2025-05-28 23:52:27 INFO     Epoch 8 [1500/1875]: loss = 0.18148547410964966, lr = 0.0001\n",
            "2025-05-28 23:52:34 INFO     Epoch 8 [1550/1875]: loss = 0.17427898943424225, lr = 0.0001\n",
            "2025-05-28 23:52:40 INFO     Epoch 8 [1600/1875]: loss = 0.22457535564899445, lr = 0.0001\n",
            "2025-05-28 23:52:47 INFO     Epoch 8 [1650/1875]: loss = 0.20367009937763214, lr = 0.0001\n",
            "2025-05-28 23:52:54 INFO     Epoch 8 [1700/1875]: loss = 0.24014383554458618, lr = 0.0001\n",
            "2025-05-28 23:53:01 INFO     Epoch 8 [1750/1875]: loss = 0.2018425166606903, lr = 0.0001\n",
            "2025-05-28 23:53:08 INFO     Epoch 8 [1800/1875]: loss = 0.19744376838207245, lr = 0.0001\n",
            "2025-05-28 23:53:15 INFO     Epoch 8 [1850/1875]: loss = 0.21565401554107666, lr = 0.0001\n",
            "2025-05-28 23:53:18 INFO     Epoch 9 [0/1875]: loss = 0.19971998035907745, lr = 0.0001\n",
            "2025-05-28 23:53:25 INFO     Epoch 9 [50/1875]: loss = 0.2114604264497757, lr = 0.0001\n",
            "2025-05-28 23:53:32 INFO     Epoch 9 [100/1875]: loss = 0.17671138048171997, lr = 0.0001\n",
            "2025-05-28 23:53:39 INFO     Epoch 9 [150/1875]: loss = 0.24337685108184814, lr = 0.0001\n",
            "2025-05-28 23:53:45 INFO     Epoch 9 [200/1875]: loss = 0.18060524761676788, lr = 0.0001\n",
            "2025-05-28 23:53:52 INFO     Epoch 9 [250/1875]: loss = 0.16171982884407043, lr = 0.0001\n",
            "2025-05-28 23:53:59 INFO     Epoch 9 [300/1875]: loss = 0.23179325461387634, lr = 0.0001\n",
            "2025-05-28 23:54:06 INFO     Epoch 9 [350/1875]: loss = 0.157278910279274, lr = 0.0001\n",
            "2025-05-28 23:54:13 INFO     Epoch 9 [400/1875]: loss = 0.18110457062721252, lr = 0.0001\n",
            "2025-05-28 23:54:19 INFO     Epoch 9 [450/1875]: loss = 0.18632782995700836, lr = 0.0001\n",
            "2025-05-28 23:54:26 INFO     Epoch 9 [500/1875]: loss = 0.1690494567155838, lr = 0.0001\n",
            "2025-05-28 23:54:33 INFO     Epoch 9 [550/1875]: loss = 0.18533886969089508, lr = 0.0001\n",
            "2025-05-28 23:54:40 INFO     Epoch 9 [600/1875]: loss = 0.1770503669977188, lr = 0.0001\n",
            "2025-05-28 23:54:47 INFO     Epoch 9 [650/1875]: loss = 0.24064527451992035, lr = 0.0001\n",
            "2025-05-28 23:54:53 INFO     Epoch 9 [700/1875]: loss = 0.19933804869651794, lr = 0.0001\n",
            "2025-05-28 23:55:00 INFO     Epoch 9 [750/1875]: loss = 0.21288296580314636, lr = 0.0001\n",
            "2025-05-28 23:55:07 INFO     Epoch 9 [800/1875]: loss = 0.2175108939409256, lr = 0.0001\n",
            "2025-05-28 23:55:14 INFO     Epoch 9 [850/1875]: loss = 0.16889896988868713, lr = 0.0001\n",
            "2025-05-28 23:55:21 INFO     Epoch 9 [900/1875]: loss = 0.20116741955280304, lr = 0.0001\n",
            "2025-05-28 23:55:28 INFO     Epoch 9 [950/1875]: loss = 0.2645089328289032, lr = 0.0001\n",
            "2025-05-28 23:55:34 INFO     Epoch 9 [1000/1875]: loss = 0.1724185198545456, lr = 0.0001\n",
            "2025-05-28 23:55:41 INFO     Epoch 9 [1050/1875]: loss = 0.22020284831523895, lr = 0.0001\n",
            "2025-05-28 23:55:48 INFO     Epoch 9 [1100/1875]: loss = 0.19072557985782623, lr = 0.0001\n",
            "2025-05-28 23:55:55 INFO     Epoch 9 [1150/1875]: loss = 0.15738780796527863, lr = 0.0001\n",
            "2025-05-28 23:56:02 INFO     Epoch 9 [1200/1875]: loss = 0.20916017889976501, lr = 0.0001\n",
            "2025-05-28 23:56:08 INFO     Epoch 9 [1250/1875]: loss = 0.20680119097232819, lr = 0.0001\n",
            "2025-05-28 23:56:15 INFO     Epoch 9 [1300/1875]: loss = 0.18748921155929565, lr = 0.0001\n",
            "2025-05-28 23:56:22 INFO     Epoch 9 [1350/1875]: loss = 0.22313223779201508, lr = 0.0001\n",
            "2025-05-28 23:56:29 INFO     Epoch 9 [1400/1875]: loss = 0.19035647809505463, lr = 0.0001\n",
            "2025-05-28 23:56:35 INFO     Epoch 9 [1450/1875]: loss = 0.24665331840515137, lr = 0.0001\n",
            "2025-05-28 23:56:42 INFO     Epoch 9 [1500/1875]: loss = 0.1767575889825821, lr = 0.0001\n",
            "2025-05-28 23:56:49 INFO     Epoch 9 [1550/1875]: loss = 0.24644747376441956, lr = 0.0001\n",
            "2025-05-28 23:56:56 INFO     Epoch 9 [1600/1875]: loss = 0.2213483303785324, lr = 0.0001\n",
            "2025-05-28 23:57:02 INFO     Epoch 9 [1650/1875]: loss = 0.17752934992313385, lr = 0.0001\n",
            "2025-05-28 23:57:09 INFO     Epoch 9 [1700/1875]: loss = 0.2413901835680008, lr = 0.0001\n",
            "2025-05-28 23:57:16 INFO     Epoch 9 [1750/1875]: loss = 0.1796802580356598, lr = 0.0001\n",
            "2025-05-28 23:57:23 INFO     Epoch 9 [1800/1875]: loss = 0.1565413624048233, lr = 0.0001\n",
            "2025-05-28 23:57:29 INFO     Epoch 9 [1850/1875]: loss = 0.21563638746738434, lr = 0.0001\n",
            "2025-05-28 23:57:35 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "2025-05-28 23:58:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:58:22 INFO     Evaluating [0/313] samples generated [32/1000] running fid 241.42208862304688\n",
            "2025-05-28 23:59:02 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:59:41 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:00:21 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:01:00 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:01:40 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:02:20 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:02:59 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:03:39 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:04:19 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:04:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:05:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:06:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:06:57 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:07:37 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:08:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:08:56 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:09:36 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:10:16 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:10:55 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:11:35 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:12:14 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:12:54 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:13:34 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:14:13 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:14:53 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:15:32 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:16:12 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:16:52 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:17:32 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:18:12 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:18:51 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:18:58 INFO     Evaluating [50/313] samples generated [1000/1000] running fid 185.02345275878906\n",
            "2025-05-29 00:19:08 INFO     Evaluating [100/313] samples generated [1000/1000] running fid 184.54310607910156\n",
            "2025-05-29 00:19:19 INFO     Evaluating [150/313] samples generated [1000/1000] running fid 184.1787872314453\n",
            "2025-05-29 00:19:29 INFO     Evaluating [200/313] samples generated [1000/1000] running fid 184.24224853515625\n",
            "2025-05-29 00:19:39 INFO     Evaluating [250/313] samples generated [1000/1000] running fid 184.68490600585938\n",
            "2025-05-29 00:19:49 INFO     Evaluating [300/313] samples generated [1000/1000] running fid 184.75877380371094\n",
            "2025-05-29 00:19:55 INFO     EMA: Switching from eval to train, restoring saved parameters\n",
            "2025-05-29 00:19:55 INFO     Epoch 10 [0/1875]: loss = 0.2012833058834076, lr = 0.0001\n",
            "2025-05-29 00:20:02 INFO     Epoch 10 [50/1875]: loss = 0.15614089369773865, lr = 0.0001\n",
            "2025-05-29 00:20:09 INFO     Epoch 10 [100/1875]: loss = 0.17687952518463135, lr = 0.0001\n",
            "2025-05-29 00:20:16 INFO     Epoch 10 [150/1875]: loss = 0.19590537250041962, lr = 0.0001\n",
            "2025-05-29 00:20:23 INFO     Epoch 10 [200/1875]: loss = 0.17693652212619781, lr = 0.0001\n",
            "2025-05-29 00:20:30 INFO     Epoch 10 [250/1875]: loss = 0.2618074119091034, lr = 0.0001\n",
            "2025-05-29 00:20:36 INFO     Epoch 10 [300/1875]: loss = 0.20122426748275757, lr = 0.0001\n",
            "2025-05-29 00:20:43 INFO     Epoch 10 [350/1875]: loss = 0.16904471814632416, lr = 0.0001\n",
            "2025-05-29 00:20:50 INFO     Epoch 10 [400/1875]: loss = 0.13600113987922668, lr = 0.0001\n",
            "2025-05-29 00:20:57 INFO     Epoch 10 [450/1875]: loss = 0.18859000504016876, lr = 0.0001\n",
            "2025-05-29 00:21:04 INFO     Epoch 10 [500/1875]: loss = 0.2107231169939041, lr = 0.0001\n",
            "2025-05-29 00:21:11 INFO     Epoch 10 [550/1875]: loss = 0.17419610917568207, lr = 0.0001\n",
            "2025-05-29 00:21:18 INFO     Epoch 10 [600/1875]: loss = 0.1323629915714264, lr = 0.0001\n",
            "2025-05-29 00:21:24 INFO     Epoch 10 [650/1875]: loss = 0.2252325415611267, lr = 0.0001\n",
            "2025-05-29 00:21:31 INFO     Epoch 10 [700/1875]: loss = 0.21236111223697662, lr = 0.0001\n",
            "2025-05-29 00:21:38 INFO     Epoch 10 [750/1875]: loss = 0.24539443850517273, lr = 0.0001\n",
            "2025-05-29 00:21:45 INFO     Epoch 10 [800/1875]: loss = 0.17500485479831696, lr = 0.0001\n",
            "2025-05-29 00:21:52 INFO     Epoch 10 [850/1875]: loss = 0.18401263654232025, lr = 0.0001\n",
            "2025-05-29 00:21:58 INFO     Epoch 10 [900/1875]: loss = 0.20162035524845123, lr = 0.0001\n",
            "2025-05-29 00:22:05 INFO     Epoch 10 [950/1875]: loss = 0.14617492258548737, lr = 0.0001\n",
            "2025-05-29 00:22:12 INFO     Epoch 10 [1000/1875]: loss = 0.221321702003479, lr = 0.0001\n",
            "2025-05-29 00:22:19 INFO     Epoch 10 [1050/1875]: loss = 0.2331937998533249, lr = 0.0001\n",
            "2025-05-29 00:22:26 INFO     Epoch 10 [1100/1875]: loss = 0.21627743542194366, lr = 0.0001\n",
            "2025-05-29 00:22:32 INFO     Epoch 10 [1150/1875]: loss = 0.19823119044303894, lr = 0.0001\n",
            "2025-05-29 00:22:39 INFO     Epoch 10 [1200/1875]: loss = 0.19801394641399384, lr = 0.0001\n",
            "2025-05-29 00:22:46 INFO     Epoch 10 [1250/1875]: loss = 0.17437399923801422, lr = 0.0001\n",
            "2025-05-29 00:22:53 INFO     Epoch 10 [1300/1875]: loss = 0.1832556575536728, lr = 0.0001\n",
            "2025-05-29 00:22:59 INFO     Epoch 10 [1350/1875]: loss = 0.1661957949399948, lr = 0.0001\n",
            "2025-05-29 00:23:06 INFO     Epoch 10 [1400/1875]: loss = 0.1543850600719452, lr = 0.0001\n",
            "2025-05-29 00:23:13 INFO     Epoch 10 [1450/1875]: loss = 0.20706774294376373, lr = 0.0001\n",
            "2025-05-29 00:23:20 INFO     Epoch 10 [1500/1875]: loss = 0.1801571398973465, lr = 0.0001\n",
            "2025-05-29 00:23:26 INFO     Epoch 10 [1550/1875]: loss = 0.17672601342201233, lr = 0.0001\n",
            "2025-05-29 00:23:33 INFO     Epoch 10 [1600/1875]: loss = 0.25575709342956543, lr = 0.0001\n",
            "2025-05-29 00:23:40 INFO     Epoch 10 [1650/1875]: loss = 0.22191865742206573, lr = 0.0001\n",
            "2025-05-29 00:23:47 INFO     Epoch 10 [1700/1875]: loss = 0.23125584423542023, lr = 0.0001\n",
            "2025-05-29 00:23:54 INFO     Epoch 10 [1750/1875]: loss = 0.20988872647285461, lr = 0.0001\n",
            "2025-05-29 00:24:01 INFO     Epoch 10 [1800/1875]: loss = 0.14500786364078522, lr = 0.0001\n",
            "2025-05-29 00:24:07 INFO     Epoch 10 [1850/1875]: loss = 0.19298942387104034, lr = 0.0001\n",
            "2025-05-29 00:24:11 INFO     Epoch 11 [0/1875]: loss = 0.13592711091041565, lr = 0.0001\n",
            "2025-05-29 00:24:18 INFO     Epoch 11 [50/1875]: loss = 0.2578516900539398, lr = 0.0001\n",
            "2025-05-29 00:24:25 INFO     Epoch 11 [100/1875]: loss = 0.1513419896364212, lr = 0.0001\n",
            "2025-05-29 00:24:32 INFO     Epoch 11 [150/1875]: loss = 0.22351998090744019, lr = 0.0001\n",
            "2025-05-29 00:24:38 INFO     Epoch 11 [200/1875]: loss = 0.17743659019470215, lr = 0.0001\n",
            "2025-05-29 00:24:45 INFO     Epoch 11 [250/1875]: loss = 0.24468521773815155, lr = 0.0001\n",
            "2025-05-29 00:24:52 INFO     Epoch 11 [300/1875]: loss = 0.15517811477184296, lr = 0.0001\n",
            "2025-05-29 00:24:59 INFO     Epoch 11 [350/1875]: loss = 0.1714903712272644, lr = 0.0001\n",
            "2025-05-29 00:25:05 INFO     Epoch 11 [400/1875]: loss = 0.17835168540477753, lr = 0.0001\n",
            "2025-05-29 00:25:12 INFO     Epoch 11 [450/1875]: loss = 0.21954737603664398, lr = 0.0001\n",
            "2025-05-29 00:25:19 INFO     Epoch 11 [500/1875]: loss = 0.15907011926174164, lr = 0.0001\n",
            "2025-05-29 00:25:26 INFO     Epoch 11 [550/1875]: loss = 0.17118346691131592, lr = 0.0001\n",
            "2025-05-29 00:25:33 INFO     Epoch 11 [600/1875]: loss = 0.14579026401042938, lr = 0.0001\n",
            "2025-05-29 00:25:39 INFO     Epoch 11 [650/1875]: loss = 0.21537922322750092, lr = 0.0001\n",
            "2025-05-29 00:25:46 INFO     Epoch 11 [700/1875]: loss = 0.184968963265419, lr = 0.0001\n",
            "2025-05-29 00:25:53 INFO     Epoch 11 [750/1875]: loss = 0.1764184534549713, lr = 0.0001\n",
            "2025-05-29 00:26:00 INFO     Epoch 11 [800/1875]: loss = 0.25829505920410156, lr = 0.0001\n",
            "2025-05-29 00:26:06 INFO     Epoch 11 [850/1875]: loss = 0.26343047618865967, lr = 0.0001\n",
            "2025-05-29 00:26:13 INFO     Epoch 11 [900/1875]: loss = 0.27314817905426025, lr = 0.0001\n",
            "2025-05-29 00:26:20 INFO     Epoch 11 [950/1875]: loss = 0.2451770305633545, lr = 0.0001\n",
            "2025-05-29 00:26:27 INFO     Epoch 11 [1000/1875]: loss = 0.21900372207164764, lr = 0.0001\n",
            "2025-05-29 00:26:34 INFO     Epoch 11 [1050/1875]: loss = 0.19637572765350342, lr = 0.0001\n",
            "2025-05-29 00:26:40 INFO     Epoch 11 [1100/1875]: loss = 0.23743833601474762, lr = 0.0001\n",
            "2025-05-29 00:26:47 INFO     Epoch 11 [1150/1875]: loss = 0.18769343197345734, lr = 0.0001\n",
            "2025-05-29 00:26:54 INFO     Epoch 11 [1200/1875]: loss = 0.1523445099592209, lr = 0.0001\n",
            "2025-05-29 00:27:01 INFO     Epoch 11 [1250/1875]: loss = 0.16892530024051666, lr = 0.0001\n",
            "2025-05-29 00:27:08 INFO     Epoch 11 [1300/1875]: loss = 0.18289822340011597, lr = 0.0001\n",
            "2025-05-29 00:27:14 INFO     Epoch 11 [1350/1875]: loss = 0.16355007886886597, lr = 0.0001\n",
            "2025-05-29 00:27:21 INFO     Epoch 11 [1400/1875]: loss = 0.18416067957878113, lr = 0.0001\n",
            "2025-05-29 00:27:28 INFO     Epoch 11 [1450/1875]: loss = 0.24850092828273773, lr = 0.0001\n",
            "2025-05-29 00:27:35 INFO     Epoch 11 [1500/1875]: loss = 0.21150673925876617, lr = 0.0001\n",
            "2025-05-29 00:27:41 INFO     Epoch 11 [1550/1875]: loss = 0.19045881927013397, lr = 0.0001\n",
            "2025-05-29 00:27:48 INFO     Epoch 11 [1600/1875]: loss = 0.21536117792129517, lr = 0.0001\n",
            "2025-05-29 00:27:55 INFO     Epoch 11 [1650/1875]: loss = 0.2024475634098053, lr = 0.0001\n",
            "2025-05-29 00:28:02 INFO     Epoch 11 [1700/1875]: loss = 0.22570335865020752, lr = 0.0001\n",
            "2025-05-29 00:28:09 INFO     Epoch 11 [1750/1875]: loss = 0.1626804769039154, lr = 0.0001\n",
            "2025-05-29 00:28:15 INFO     Epoch 11 [1800/1875]: loss = 0.19682499766349792, lr = 0.0001\n",
            "2025-05-29 00:28:22 INFO     Epoch 11 [1850/1875]: loss = 0.23500053584575653, lr = 0.0001\n",
            "2025-05-29 00:28:26 INFO     Epoch 12 [0/1875]: loss = 0.23280023038387299, lr = 0.0001\n",
            "2025-05-29 00:28:33 INFO     Epoch 12 [50/1875]: loss = 0.20719872415065765, lr = 0.0001\n",
            "2025-05-29 00:28:39 INFO     Epoch 12 [100/1875]: loss = 0.1827259510755539, lr = 0.0001\n",
            "2025-05-29 00:28:46 INFO     Epoch 12 [150/1875]: loss = 0.22148573398590088, lr = 0.0001\n",
            "2025-05-29 00:28:53 INFO     Epoch 12 [200/1875]: loss = 0.237028107047081, lr = 0.0001\n",
            "2025-05-29 00:29:00 INFO     Epoch 12 [250/1875]: loss = 0.1608843356370926, lr = 0.0001\n",
            "2025-05-29 00:29:07 INFO     Epoch 12 [300/1875]: loss = 0.2115693986415863, lr = 0.0001\n",
            "2025-05-29 00:29:13 INFO     Epoch 12 [350/1875]: loss = 0.173899844288826, lr = 0.0001\n",
            "2025-05-29 00:29:20 INFO     Epoch 12 [400/1875]: loss = 0.18445834517478943, lr = 0.0001\n",
            "2025-05-29 00:29:27 INFO     Epoch 12 [450/1875]: loss = 0.151956245303154, lr = 0.0001\n",
            "2025-05-29 00:29:34 INFO     Epoch 12 [500/1875]: loss = 0.22351035475730896, lr = 0.0001\n",
            "2025-05-29 00:29:41 INFO     Epoch 12 [550/1875]: loss = 0.14765550196170807, lr = 0.0001\n",
            "2025-05-29 00:29:47 INFO     Epoch 12 [600/1875]: loss = 0.24259702861309052, lr = 0.0001\n",
            "2025-05-29 00:29:54 INFO     Epoch 12 [650/1875]: loss = 0.23098936676979065, lr = 0.0001\n",
            "2025-05-29 00:30:01 INFO     Epoch 12 [700/1875]: loss = 0.2265152484178543, lr = 0.0001\n",
            "2025-05-29 00:30:08 INFO     Epoch 12 [750/1875]: loss = 0.21145059168338776, lr = 0.0001\n",
            "2025-05-29 00:30:15 INFO     Epoch 12 [800/1875]: loss = 0.2342747300863266, lr = 0.0001\n",
            "2025-05-29 00:30:22 INFO     Epoch 12 [850/1875]: loss = 0.23512086272239685, lr = 0.0001\n",
            "2025-05-29 00:30:28 INFO     Epoch 12 [900/1875]: loss = 0.23160254955291748, lr = 0.0001\n",
            "2025-05-29 00:30:35 INFO     Epoch 12 [950/1875]: loss = 0.19217108190059662, lr = 0.0001\n",
            "2025-05-29 00:30:42 INFO     Epoch 12 [1000/1875]: loss = 0.23655039072036743, lr = 0.0001\n",
            "2025-05-29 00:30:49 INFO     Epoch 12 [1050/1875]: loss = 0.21854810416698456, lr = 0.0001\n",
            "2025-05-29 00:30:55 INFO     Epoch 12 [1100/1875]: loss = 0.1979832649230957, lr = 0.0001\n",
            "2025-05-29 00:31:02 INFO     Epoch 12 [1150/1875]: loss = 0.22152888774871826, lr = 0.0001\n",
            "2025-05-29 00:31:09 INFO     Epoch 12 [1200/1875]: loss = 0.1966301053762436, lr = 0.0001\n",
            "2025-05-29 00:31:16 INFO     Epoch 12 [1250/1875]: loss = 0.15380895137786865, lr = 0.0001\n",
            "2025-05-29 00:31:23 INFO     Epoch 12 [1300/1875]: loss = 0.23946598172187805, lr = 0.0001\n",
            "2025-05-29 00:31:29 INFO     Epoch 12 [1350/1875]: loss = 0.18996120989322662, lr = 0.0001\n",
            "2025-05-29 00:31:36 INFO     Epoch 12 [1400/1875]: loss = 0.17809529602527618, lr = 0.0001\n",
            "2025-05-29 00:31:43 INFO     Epoch 12 [1450/1875]: loss = 0.19576989114284515, lr = 0.0001\n",
            "2025-05-29 00:31:50 INFO     Epoch 12 [1500/1875]: loss = 0.16681669652462006, lr = 0.0001\n",
            "2025-05-29 00:31:57 INFO     Epoch 12 [1550/1875]: loss = 0.1712469607591629, lr = 0.0001\n",
            "2025-05-29 00:32:03 INFO     Epoch 12 [1600/1875]: loss = 0.19385024905204773, lr = 0.0001\n",
            "2025-05-29 00:32:10 INFO     Epoch 12 [1650/1875]: loss = 0.23020504415035248, lr = 0.0001\n",
            "2025-05-29 00:32:17 INFO     Epoch 12 [1700/1875]: loss = 0.23942933976650238, lr = 0.0001\n",
            "2025-05-29 00:32:24 INFO     Epoch 12 [1750/1875]: loss = 0.17628242075443268, lr = 0.0001\n",
            "2025-05-29 00:32:30 INFO     Epoch 12 [1800/1875]: loss = 0.2029096782207489, lr = 0.0001\n",
            "2025-05-29 00:32:37 INFO     Epoch 12 [1850/1875]: loss = 0.22934718430042267, lr = 0.0001\n",
            "2025-05-29 00:32:41 INFO     Epoch 13 [0/1875]: loss = 0.18234039843082428, lr = 0.0001\n",
            "2025-05-29 00:32:48 INFO     Epoch 13 [50/1875]: loss = 0.1988803893327713, lr = 0.0001\n",
            "2025-05-29 00:32:55 INFO     Epoch 13 [100/1875]: loss = 0.19166788458824158, lr = 0.0001\n",
            "2025-05-29 00:33:02 INFO     Epoch 13 [150/1875]: loss = 0.22685186564922333, lr = 0.0001\n",
            "2025-05-29 00:33:08 INFO     Epoch 13 [200/1875]: loss = 0.19722092151641846, lr = 0.0001\n",
            "2025-05-29 00:33:15 INFO     Epoch 13 [250/1875]: loss = 0.22247272729873657, lr = 0.0001\n",
            "2025-05-29 00:33:22 INFO     Epoch 13 [300/1875]: loss = 0.16414159536361694, lr = 0.0001\n",
            "2025-05-29 00:33:29 INFO     Epoch 13 [350/1875]: loss = 0.15498089790344238, lr = 0.0001\n",
            "2025-05-29 00:33:35 INFO     Epoch 13 [400/1875]: loss = 0.1373259574174881, lr = 0.0001\n",
            "2025-05-29 00:33:42 INFO     Epoch 13 [450/1875]: loss = 0.1797601878643036, lr = 0.0001\n",
            "2025-05-29 00:33:49 INFO     Epoch 13 [500/1875]: loss = 0.2385658472776413, lr = 0.0001\n",
            "2025-05-29 00:33:56 INFO     Epoch 13 [550/1875]: loss = 0.23205888271331787, lr = 0.0001\n",
            "2025-05-29 00:34:03 INFO     Epoch 13 [600/1875]: loss = 0.13784226775169373, lr = 0.0001\n",
            "2025-05-29 00:34:09 INFO     Epoch 13 [650/1875]: loss = 0.14335153996944427, lr = 0.0001\n",
            "2025-05-29 00:34:16 INFO     Epoch 13 [700/1875]: loss = 0.24554723501205444, lr = 0.0001\n",
            "2025-05-29 00:34:23 INFO     Epoch 13 [750/1875]: loss = 0.23996658623218536, lr = 0.0001\n",
            "2025-05-29 00:34:30 INFO     Epoch 13 [800/1875]: loss = 0.22335615754127502, lr = 0.0001\n",
            "2025-05-29 00:34:36 INFO     Epoch 13 [850/1875]: loss = 0.1813216358423233, lr = 0.0001\n",
            "2025-05-29 00:34:43 INFO     Epoch 13 [900/1875]: loss = 0.16193576157093048, lr = 0.0001\n",
            "2025-05-29 00:34:50 INFO     Epoch 13 [950/1875]: loss = 0.2107241004705429, lr = 0.0001\n",
            "2025-05-29 00:34:57 INFO     Epoch 13 [1000/1875]: loss = 0.16867345571517944, lr = 0.0001\n",
            "2025-05-29 00:35:04 INFO     Epoch 13 [1050/1875]: loss = 0.18270622193813324, lr = 0.0001\n",
            "2025-05-29 00:35:10 INFO     Epoch 13 [1100/1875]: loss = 0.1739233136177063, lr = 0.0001\n",
            "2025-05-29 00:35:17 INFO     Epoch 13 [1150/1875]: loss = 0.19539810717105865, lr = 0.0001\n",
            "2025-05-29 00:35:24 INFO     Epoch 13 [1200/1875]: loss = 0.22404469549655914, lr = 0.0001\n",
            "2025-05-29 00:35:31 INFO     Epoch 13 [1250/1875]: loss = 0.16836833953857422, lr = 0.0001\n",
            "2025-05-29 00:35:38 INFO     Epoch 13 [1300/1875]: loss = 0.21394242346286774, lr = 0.0001\n",
            "2025-05-29 00:35:44 INFO     Epoch 13 [1350/1875]: loss = 0.21702665090560913, lr = 0.0001\n",
            "2025-05-29 00:35:51 INFO     Epoch 13 [1400/1875]: loss = 0.15310388803482056, lr = 0.0001\n",
            "2025-05-29 00:35:58 INFO     Epoch 13 [1450/1875]: loss = 0.2048029750585556, lr = 0.0001\n",
            "2025-05-29 00:36:05 INFO     Epoch 13 [1500/1875]: loss = 0.2018883228302002, lr = 0.0001\n",
            "2025-05-29 00:36:12 INFO     Epoch 13 [1550/1875]: loss = 0.19155091047286987, lr = 0.0001\n",
            "2025-05-29 00:36:18 INFO     Epoch 13 [1600/1875]: loss = 0.20997494459152222, lr = 0.0001\n",
            "2025-05-29 00:36:25 INFO     Epoch 13 [1650/1875]: loss = 0.19844286143779755, lr = 0.0001\n",
            "2025-05-29 00:36:32 INFO     Epoch 13 [1700/1875]: loss = 0.1747780740261078, lr = 0.0001\n",
            "2025-05-29 00:36:39 INFO     Epoch 13 [1750/1875]: loss = 0.2521344721317291, lr = 0.0001\n",
            "2025-05-29 00:36:45 INFO     Epoch 13 [1800/1875]: loss = 0.15176457166671753, lr = 0.0001\n",
            "2025-05-29 00:36:52 INFO     Epoch 13 [1850/1875]: loss = 0.2255486696958542, lr = 0.0001\n",
            "2025-05-29 00:36:56 INFO     Epoch 14 [0/1875]: loss = 0.12869228422641754, lr = 0.0001\n",
            "2025-05-29 00:37:03 INFO     Epoch 14 [50/1875]: loss = 0.185469388961792, lr = 0.0001\n",
            "2025-05-29 00:37:10 INFO     Epoch 14 [100/1875]: loss = 0.21056446433067322, lr = 0.0001\n",
            "2025-05-29 00:37:16 INFO     Epoch 14 [150/1875]: loss = 0.19458705186843872, lr = 0.0001\n",
            "2025-05-29 00:37:23 INFO     Epoch 14 [200/1875]: loss = 0.1739577353000641, lr = 0.0001\n",
            "2025-05-29 00:37:30 INFO     Epoch 14 [250/1875]: loss = 0.2410328984260559, lr = 0.0001\n",
            "2025-05-29 00:37:37 INFO     Epoch 14 [300/1875]: loss = 0.20406904816627502, lr = 0.0001\n",
            "2025-05-29 00:37:43 INFO     Epoch 14 [350/1875]: loss = 0.20251350104808807, lr = 0.0001\n",
            "2025-05-29 00:37:50 INFO     Epoch 14 [400/1875]: loss = 0.16242152452468872, lr = 0.0001\n",
            "2025-05-29 00:37:57 INFO     Epoch 14 [450/1875]: loss = 0.2043239027261734, lr = 0.0001\n",
            "2025-05-29 00:38:04 INFO     Epoch 14 [500/1875]: loss = 0.21227288246154785, lr = 0.0001\n",
            "2025-05-29 00:38:11 INFO     Epoch 14 [550/1875]: loss = 0.17037855088710785, lr = 0.0001\n",
            "2025-05-29 00:38:17 INFO     Epoch 14 [600/1875]: loss = 0.17971856892108917, lr = 0.0001\n",
            "2025-05-29 00:38:24 INFO     Epoch 14 [650/1875]: loss = 0.23838050663471222, lr = 0.0001\n",
            "2025-05-29 00:38:31 INFO     Epoch 14 [700/1875]: loss = 0.17116522789001465, lr = 0.0001\n",
            "2025-05-29 00:38:38 INFO     Epoch 14 [750/1875]: loss = 0.16853749752044678, lr = 0.0001\n",
            "2025-05-29 00:38:45 INFO     Epoch 14 [800/1875]: loss = 0.20243953168392181, lr = 0.0001\n",
            "2025-05-29 00:38:51 INFO     Epoch 14 [850/1875]: loss = 0.2390691041946411, lr = 0.0001\n",
            "2025-05-29 00:38:58 INFO     Epoch 14 [900/1875]: loss = 0.22912642359733582, lr = 0.0001\n",
            "2025-05-29 00:39:05 INFO     Epoch 14 [950/1875]: loss = 0.20760896801948547, lr = 0.0001\n",
            "2025-05-29 00:39:12 INFO     Epoch 14 [1000/1875]: loss = 0.18051739037036896, lr = 0.0001\n",
            "2025-05-29 00:39:19 INFO     Epoch 14 [1050/1875]: loss = 0.13760779798030853, lr = 0.0001\n",
            "2025-05-29 00:39:25 INFO     Epoch 14 [1100/1875]: loss = 0.1511872112751007, lr = 0.0001\n",
            "2025-05-29 00:39:32 INFO     Epoch 14 [1150/1875]: loss = 0.1650419682264328, lr = 0.0001\n",
            "2025-05-29 00:39:39 INFO     Epoch 14 [1200/1875]: loss = 0.1992666870355606, lr = 0.0001\n",
            "2025-05-29 00:39:46 INFO     Epoch 14 [1250/1875]: loss = 0.16961245238780975, lr = 0.0001\n",
            "2025-05-29 00:39:52 INFO     Epoch 14 [1300/1875]: loss = 0.18370553851127625, lr = 0.0001\n",
            "2025-05-29 00:39:59 INFO     Epoch 14 [1350/1875]: loss = 0.17849349975585938, lr = 0.0001\n",
            "2025-05-29 00:40:06 INFO     Epoch 14 [1400/1875]: loss = 0.15741749107837677, lr = 0.0001\n",
            "2025-05-29 00:40:13 INFO     Epoch 14 [1450/1875]: loss = 0.20726346969604492, lr = 0.0001\n",
            "2025-05-29 00:40:20 INFO     Epoch 14 [1500/1875]: loss = 0.19972577691078186, lr = 0.0001\n",
            "2025-05-29 00:40:26 INFO     Epoch 14 [1550/1875]: loss = 0.1608632355928421, lr = 0.0001\n",
            "2025-05-29 00:40:33 INFO     Epoch 14 [1600/1875]: loss = 0.15982693433761597, lr = 0.0001\n",
            "2025-05-29 00:40:40 INFO     Epoch 14 [1650/1875]: loss = 0.21834206581115723, lr = 0.0001\n",
            "2025-05-29 00:40:47 INFO     Epoch 14 [1700/1875]: loss = 0.2569435238838196, lr = 0.0001\n",
            "2025-05-29 00:40:53 INFO     Epoch 14 [1750/1875]: loss = 0.1942824125289917, lr = 0.0001\n",
            "2025-05-29 00:41:00 INFO     Epoch 14 [1800/1875]: loss = 0.1949920803308487, lr = 0.0001\n",
            "2025-05-29 00:41:07 INFO     Epoch 14 [1850/1875]: loss = 0.21701540052890778, lr = 0.0001\n",
            "2025-05-29 00:41:12 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "2025-05-29 00:41:54 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:41:59 INFO     Evaluating [0/313] samples generated [32/1000] running fid 229.87294006347656\n",
            "2025-05-29 00:42:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:43:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:43:57 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:44:37 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:45:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:45:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:46:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:47:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:47:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:48:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:49:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:49:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:50:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:51:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:51:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:52:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:53:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:53:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:54:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:55:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:55:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:56:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:57:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:57:57 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:58:37 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:59:16 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:59:56 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:00:36 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:01:16 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:01:55 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:02:35 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:02:42 INFO     Evaluating [50/313] samples generated [1000/1000] running fid 185.3086700439453\n",
            "2025-05-29 01:02:52 INFO     Evaluating [100/313] samples generated [1000/1000] running fid 184.77159118652344\n",
            "2025-05-29 01:03:03 INFO     Evaluating [150/313] samples generated [1000/1000] running fid 184.4553985595703\n",
            "2025-05-29 01:03:13 INFO     Evaluating [200/313] samples generated [1000/1000] running fid 184.59144592285156\n",
            "2025-05-29 01:03:24 INFO     Evaluating [250/313] samples generated [1000/1000] running fid 185.01858520507812\n",
            "2025-05-29 01:03:35 INFO     Evaluating [300/313] samples generated [1000/1000] running fid 185.11627197265625\n",
            "2025-05-29 01:03:41 INFO     EMA: Switching from eval to train, restoring saved parameters\n",
            "2025-05-29 01:03:42 INFO     Epoch 15 [0/1875]: loss = 0.17802166938781738, lr = 0.0001\n",
            "2025-05-29 01:03:48 INFO     Epoch 15 [50/1875]: loss = 0.1693105846643448, lr = 0.0001\n",
            "2025-05-29 01:03:55 INFO     Epoch 15 [100/1875]: loss = 0.14961761236190796, lr = 0.0001\n",
            "2025-05-29 01:04:02 INFO     Epoch 15 [150/1875]: loss = 0.19957789778709412, lr = 0.0001\n",
            "2025-05-29 01:04:09 INFO     Epoch 15 [200/1875]: loss = 0.1784772127866745, lr = 0.0001\n",
            "2025-05-29 01:04:16 INFO     Epoch 15 [250/1875]: loss = 0.2054334431886673, lr = 0.0001\n",
            "2025-05-29 01:04:23 INFO     Epoch 15 [300/1875]: loss = 0.1808961182832718, lr = 0.0001\n",
            "2025-05-29 01:04:30 INFO     Epoch 15 [350/1875]: loss = 0.21445652842521667, lr = 0.0001\n",
            "2025-05-29 01:04:36 INFO     Epoch 15 [400/1875]: loss = 0.22404272854328156, lr = 0.0001\n",
            "2025-05-29 01:04:43 INFO     Epoch 15 [450/1875]: loss = 0.2533639073371887, lr = 0.0001\n",
            "2025-05-29 01:04:50 INFO     Epoch 15 [500/1875]: loss = 0.14698101580142975, lr = 0.0001\n",
            "2025-05-29 01:04:57 INFO     Epoch 15 [550/1875]: loss = 0.1812136322259903, lr = 0.0001\n",
            "2025-05-29 01:05:04 INFO     Epoch 15 [600/1875]: loss = 0.1632397174835205, lr = 0.0001\n",
            "2025-05-29 01:05:10 INFO     Epoch 15 [650/1875]: loss = 0.22529739141464233, lr = 0.0001\n",
            "2025-05-29 01:05:17 INFO     Epoch 15 [700/1875]: loss = 0.27593910694122314, lr = 0.0001\n",
            "2025-05-29 01:05:24 INFO     Epoch 15 [750/1875]: loss = 0.18653161823749542, lr = 0.0001\n",
            "2025-05-29 01:05:31 INFO     Epoch 15 [800/1875]: loss = 0.2148457169532776, lr = 0.0001\n",
            "2025-05-29 01:05:38 INFO     Epoch 15 [850/1875]: loss = 0.21587856113910675, lr = 0.0001\n",
            "2025-05-29 01:05:45 INFO     Epoch 15 [900/1875]: loss = 0.22797803580760956, lr = 0.0001\n",
            "2025-05-29 01:05:51 INFO     Epoch 15 [950/1875]: loss = 0.1572703868150711, lr = 0.0001\n",
            "2025-05-29 01:05:58 INFO     Epoch 15 [1000/1875]: loss = 0.19666945934295654, lr = 0.0001\n",
            "2025-05-29 01:06:05 INFO     Epoch 15 [1050/1875]: loss = 0.2092577964067459, lr = 0.0001\n",
            "2025-05-29 01:06:12 INFO     Epoch 15 [1100/1875]: loss = 0.18284949660301208, lr = 0.0001\n",
            "2025-05-29 01:06:19 INFO     Epoch 15 [1150/1875]: loss = 0.2057676464319229, lr = 0.0001\n",
            "2025-05-29 01:06:25 INFO     Epoch 15 [1200/1875]: loss = 0.17999544739723206, lr = 0.0001\n",
            "2025-05-29 01:06:32 INFO     Epoch 15 [1250/1875]: loss = 0.14532570540905, lr = 0.0001\n",
            "2025-05-29 01:06:39 INFO     Epoch 15 [1300/1875]: loss = 0.20715056359767914, lr = 0.0001\n",
            "2025-05-29 01:06:46 INFO     Epoch 15 [1350/1875]: loss = 0.22946703433990479, lr = 0.0001\n",
            "2025-05-29 01:06:53 INFO     Epoch 15 [1400/1875]: loss = 0.17105066776275635, lr = 0.0001\n",
            "2025-05-29 01:07:00 INFO     Epoch 15 [1450/1875]: loss = 0.2687132954597473, lr = 0.0001\n",
            "2025-05-29 01:07:06 INFO     Epoch 15 [1500/1875]: loss = 0.17531122267246246, lr = 0.0001\n",
            "2025-05-29 01:07:13 INFO     Epoch 15 [1550/1875]: loss = 0.12877696752548218, lr = 0.0001\n",
            "2025-05-29 01:07:20 INFO     Epoch 15 [1600/1875]: loss = 0.20604932308197021, lr = 0.0001\n",
            "2025-05-29 01:07:27 INFO     Epoch 15 [1650/1875]: loss = 0.1661096066236496, lr = 0.0001\n",
            "2025-05-29 01:07:34 INFO     Epoch 15 [1700/1875]: loss = 0.2483522593975067, lr = 0.0001\n",
            "2025-05-29 01:07:41 INFO     Epoch 15 [1750/1875]: loss = 0.22097967565059662, lr = 0.0001\n",
            "2025-05-29 01:07:47 INFO     Epoch 15 [1800/1875]: loss = 0.2454703003168106, lr = 0.0001\n",
            "2025-05-29 01:07:54 INFO     Epoch 15 [1850/1875]: loss = 0.2719695568084717, lr = 0.0001\n",
            "2025-05-29 01:07:58 INFO     Epoch 16 [0/1875]: loss = 0.17850540578365326, lr = 0.0001\n",
            "2025-05-29 01:08:05 INFO     Epoch 16 [50/1875]: loss = 0.16831286251544952, lr = 0.0001\n",
            "2025-05-29 01:08:12 INFO     Epoch 16 [100/1875]: loss = 0.20304936170578003, lr = 0.0001\n",
            "2025-05-29 01:08:18 INFO     Epoch 16 [150/1875]: loss = 0.2214217633008957, lr = 0.0001\n",
            "2025-05-29 01:08:25 INFO     Epoch 16 [200/1875]: loss = 0.1850171834230423, lr = 0.0001\n",
            "2025-05-29 01:08:32 INFO     Epoch 16 [250/1875]: loss = 0.16372504830360413, lr = 0.0001\n",
            "2025-05-29 01:08:39 INFO     Epoch 16 [300/1875]: loss = 0.2070331573486328, lr = 0.0001\n",
            "2025-05-29 01:08:45 INFO     Epoch 16 [350/1875]: loss = 0.16092976927757263, lr = 0.0001\n",
            "2025-05-29 01:08:52 INFO     Epoch 16 [400/1875]: loss = 0.1944081336259842, lr = 0.0001\n",
            "2025-05-29 01:08:59 INFO     Epoch 16 [450/1875]: loss = 0.2123311161994934, lr = 0.0001\n",
            "2025-05-29 01:09:06 INFO     Epoch 16 [500/1875]: loss = 0.19817201793193817, lr = 0.0001\n",
            "2025-05-29 01:09:13 INFO     Epoch 16 [550/1875]: loss = 0.12497513741254807, lr = 0.0001\n",
            "2025-05-29 01:09:19 INFO     Epoch 16 [600/1875]: loss = 0.1743336021900177, lr = 0.0001\n",
            "2025-05-29 01:09:26 INFO     Epoch 16 [650/1875]: loss = 0.19777745008468628, lr = 0.0001\n",
            "2025-05-29 01:09:33 INFO     Epoch 16 [700/1875]: loss = 0.19356095790863037, lr = 0.0001\n",
            "2025-05-29 01:09:40 INFO     Epoch 16 [750/1875]: loss = 0.2151457965373993, lr = 0.0001\n",
            "2025-05-29 01:09:47 INFO     Epoch 16 [800/1875]: loss = 0.17321893572807312, lr = 0.0001\n",
            "2025-05-29 01:09:54 INFO     Epoch 16 [850/1875]: loss = 0.17414340376853943, lr = 0.0001\n",
            "2025-05-29 01:10:01 INFO     Epoch 16 [900/1875]: loss = 0.20010848343372345, lr = 0.0001\n",
            "2025-05-29 01:10:07 INFO     Epoch 16 [950/1875]: loss = 0.18507571518421173, lr = 0.0001\n",
            "2025-05-29 01:10:14 INFO     Epoch 16 [1000/1875]: loss = 0.17569635808467865, lr = 0.0001\n",
            "2025-05-29 01:10:21 INFO     Epoch 16 [1050/1875]: loss = 0.17757385969161987, lr = 0.0001\n",
            "2025-05-29 01:10:28 INFO     Epoch 16 [1100/1875]: loss = 0.17345941066741943, lr = 0.0001\n",
            "2025-05-29 01:10:35 INFO     Epoch 16 [1150/1875]: loss = 0.2427036315202713, lr = 0.0001\n",
            "2025-05-29 01:10:41 INFO     Epoch 16 [1200/1875]: loss = 0.15152797102928162, lr = 0.0001\n",
            "2025-05-29 01:10:48 INFO     Epoch 16 [1250/1875]: loss = 0.17640098929405212, lr = 0.0001\n",
            "2025-05-29 01:10:55 INFO     Epoch 16 [1300/1875]: loss = 0.18369059264659882, lr = 0.0001\n",
            "2025-05-29 01:11:02 INFO     Epoch 16 [1350/1875]: loss = 0.16304652392864227, lr = 0.0001\n",
            "2025-05-29 01:11:09 INFO     Epoch 16 [1400/1875]: loss = 0.19615428149700165, lr = 0.0001\n",
            "2025-05-29 01:11:15 INFO     Epoch 16 [1450/1875]: loss = 0.16387276351451874, lr = 0.0001\n",
            "2025-05-29 01:11:22 INFO     Epoch 16 [1500/1875]: loss = 0.14067479968070984, lr = 0.0001\n",
            "2025-05-29 01:11:29 INFO     Epoch 16 [1550/1875]: loss = 0.24549022316932678, lr = 0.0001\n",
            "2025-05-29 01:11:36 INFO     Epoch 16 [1600/1875]: loss = 0.2269105464220047, lr = 0.0001\n",
            "2025-05-29 01:11:43 INFO     Epoch 16 [1650/1875]: loss = 0.16876475512981415, lr = 0.0001\n",
            "2025-05-29 01:11:49 INFO     Epoch 16 [1700/1875]: loss = 0.23781718313694, lr = 0.0001\n",
            "2025-05-29 01:11:56 INFO     Epoch 16 [1750/1875]: loss = 0.25807854533195496, lr = 0.0001\n",
            "2025-05-29 01:12:03 INFO     Epoch 16 [1800/1875]: loss = 0.10735383629798889, lr = 0.0001\n",
            "2025-05-29 01:12:10 INFO     Epoch 16 [1850/1875]: loss = 0.220053568482399, lr = 0.0001\n",
            "2025-05-29 01:12:14 INFO     Epoch 17 [0/1875]: loss = 0.19156545400619507, lr = 0.0001\n",
            "2025-05-29 01:12:20 INFO     Epoch 17 [50/1875]: loss = 0.20449525117874146, lr = 0.0001\n",
            "2025-05-29 01:12:27 INFO     Epoch 17 [100/1875]: loss = 0.1241200864315033, lr = 0.0001\n",
            "2025-05-29 01:12:34 INFO     Epoch 17 [150/1875]: loss = 0.2562493681907654, lr = 0.0001\n",
            "2025-05-29 01:12:41 INFO     Epoch 17 [200/1875]: loss = 0.10970330983400345, lr = 0.0001\n",
            "2025-05-29 01:12:48 INFO     Epoch 17 [250/1875]: loss = 0.2521021366119385, lr = 0.0001\n",
            "2025-05-29 01:12:54 INFO     Epoch 17 [300/1875]: loss = 0.2279958873987198, lr = 0.0001\n",
            "2025-05-29 01:13:01 INFO     Epoch 17 [350/1875]: loss = 0.17429646849632263, lr = 0.0001\n",
            "2025-05-29 01:13:08 INFO     Epoch 17 [400/1875]: loss = 0.2073855698108673, lr = 0.0001\n",
            "2025-05-29 01:13:15 INFO     Epoch 17 [450/1875]: loss = 0.2610258460044861, lr = 0.0001\n",
            "2025-05-29 01:13:22 INFO     Epoch 17 [500/1875]: loss = 0.18678133189678192, lr = 0.0001\n",
            "2025-05-29 01:13:29 INFO     Epoch 17 [550/1875]: loss = 0.1583060473203659, lr = 0.0001\n",
            "2025-05-29 01:13:36 INFO     Epoch 17 [600/1875]: loss = 0.17564833164215088, lr = 0.0001\n",
            "2025-05-29 01:13:42 INFO     Epoch 17 [650/1875]: loss = 0.25011900067329407, lr = 0.0001\n",
            "2025-05-29 01:13:49 INFO     Epoch 17 [700/1875]: loss = 0.16323497891426086, lr = 0.0001\n",
            "2025-05-29 01:13:56 INFO     Epoch 17 [750/1875]: loss = 0.18485954403877258, lr = 0.0001\n",
            "2025-05-29 01:14:03 INFO     Epoch 17 [800/1875]: loss = 0.23608216643333435, lr = 0.0001\n",
            "2025-05-29 01:14:10 INFO     Epoch 17 [850/1875]: loss = 0.16608725488185883, lr = 0.0001\n",
            "2025-05-29 01:14:16 INFO     Epoch 17 [900/1875]: loss = 0.19032925367355347, lr = 0.0001\n",
            "2025-05-29 01:14:23 INFO     Epoch 17 [950/1875]: loss = 0.22364716231822968, lr = 0.0001\n",
            "2025-05-29 01:14:30 INFO     Epoch 17 [1000/1875]: loss = 0.23359911143779755, lr = 0.0001\n",
            "2025-05-29 01:14:37 INFO     Epoch 17 [1050/1875]: loss = 0.1956726312637329, lr = 0.0001\n",
            "2025-05-29 01:14:44 INFO     Epoch 17 [1100/1875]: loss = 0.146046981215477, lr = 0.0001\n",
            "2025-05-29 01:14:50 INFO     Epoch 17 [1150/1875]: loss = 0.1653570681810379, lr = 0.0001\n",
            "2025-05-29 01:14:57 INFO     Epoch 17 [1200/1875]: loss = 0.1807355284690857, lr = 0.0001\n",
            "2025-05-29 01:15:04 INFO     Epoch 17 [1250/1875]: loss = 0.1945839822292328, lr = 0.0001\n",
            "2025-05-29 01:15:11 INFO     Epoch 17 [1300/1875]: loss = 0.18897001445293427, lr = 0.0001\n",
            "2025-05-29 01:15:18 INFO     Epoch 17 [1350/1875]: loss = 0.18851181864738464, lr = 0.0001\n",
            "2025-05-29 01:15:24 INFO     Epoch 17 [1400/1875]: loss = 0.14642338454723358, lr = 0.0001\n",
            "2025-05-29 01:15:31 INFO     Epoch 17 [1450/1875]: loss = 0.20283910632133484, lr = 0.0001\n",
            "2025-05-29 01:15:38 INFO     Epoch 17 [1500/1875]: loss = 0.1676541119813919, lr = 0.0001\n",
            "2025-05-29 01:15:45 INFO     Epoch 17 [1550/1875]: loss = 0.1413743495941162, lr = 0.0001\n",
            "2025-05-29 01:15:52 INFO     Epoch 17 [1600/1875]: loss = 0.20833098888397217, lr = 0.0001\n",
            "2025-05-29 01:15:58 INFO     Epoch 17 [1650/1875]: loss = 0.21627379953861237, lr = 0.0001\n",
            "2025-05-29 01:16:05 INFO     Epoch 17 [1700/1875]: loss = 0.22860385477542877, lr = 0.0001\n",
            "2025-05-29 01:16:12 INFO     Epoch 17 [1750/1875]: loss = 0.14657357335090637, lr = 0.0001\n",
            "2025-05-29 01:16:19 INFO     Epoch 17 [1800/1875]: loss = 0.15973693132400513, lr = 0.0001\n",
            "2025-05-29 01:16:26 INFO     Epoch 17 [1850/1875]: loss = 0.2651328444480896, lr = 0.0001\n",
            "2025-05-29 01:16:30 INFO     Epoch 18 [0/1875]: loss = 0.18904881179332733, lr = 0.0001\n",
            "2025-05-29 01:16:36 INFO     Epoch 18 [50/1875]: loss = 0.20288148522377014, lr = 0.0001\n",
            "2025-05-29 01:16:43 INFO     Epoch 18 [100/1875]: loss = 0.2027292400598526, lr = 0.0001\n",
            "2025-05-29 01:16:50 INFO     Epoch 18 [150/1875]: loss = 0.16725976765155792, lr = 0.0001\n",
            "2025-05-29 01:16:57 INFO     Epoch 18 [200/1875]: loss = 0.18673962354660034, lr = 0.0001\n",
            "2025-05-29 01:17:04 INFO     Epoch 18 [250/1875]: loss = 0.2092694342136383, lr = 0.0001\n",
            "2025-05-29 01:17:10 INFO     Epoch 18 [300/1875]: loss = 0.274647057056427, lr = 0.0001\n",
            "2025-05-29 01:17:17 INFO     Epoch 18 [350/1875]: loss = 0.1641090214252472, lr = 0.0001\n",
            "2025-05-29 01:17:24 INFO     Epoch 18 [400/1875]: loss = 0.18923872709274292, lr = 0.0001\n",
            "2025-05-29 01:17:31 INFO     Epoch 18 [450/1875]: loss = 0.1915324479341507, lr = 0.0001\n",
            "2025-05-29 01:17:38 INFO     Epoch 18 [500/1875]: loss = 0.23836153745651245, lr = 0.0001\n",
            "2025-05-29 01:17:45 INFO     Epoch 18 [550/1875]: loss = 0.21045036613941193, lr = 0.0001\n",
            "2025-05-29 01:17:52 INFO     Epoch 18 [600/1875]: loss = 0.166454017162323, lr = 0.0001\n",
            "2025-05-29 01:17:58 INFO     Epoch 18 [650/1875]: loss = 0.20256023108959198, lr = 0.0001\n",
            "2025-05-29 01:18:05 INFO     Epoch 18 [700/1875]: loss = 0.15622521936893463, lr = 0.0001\n",
            "2025-05-29 01:18:12 INFO     Epoch 18 [750/1875]: loss = 0.21333833038806915, lr = 0.0001\n",
            "2025-05-29 01:18:19 INFO     Epoch 18 [800/1875]: loss = 0.21090197563171387, lr = 0.0001\n",
            "2025-05-29 01:18:26 INFO     Epoch 18 [850/1875]: loss = 0.1846800595521927, lr = 0.0001\n",
            "2025-05-29 01:18:32 INFO     Epoch 18 [900/1875]: loss = 0.22776521742343903, lr = 0.0001\n",
            "2025-05-29 01:18:39 INFO     Epoch 18 [950/1875]: loss = 0.2493472695350647, lr = 0.0001\n",
            "2025-05-29 01:18:46 INFO     Epoch 18 [1000/1875]: loss = 0.2074449360370636, lr = 0.0001\n",
            "2025-05-29 01:18:53 INFO     Epoch 18 [1050/1875]: loss = 0.18215319514274597, lr = 0.0001\n",
            "2025-05-29 01:19:00 INFO     Epoch 18 [1100/1875]: loss = 0.1985860913991928, lr = 0.0001\n",
            "2025-05-29 01:19:07 INFO     Epoch 18 [1150/1875]: loss = 0.17451275885105133, lr = 0.0001\n",
            "2025-05-29 01:19:13 INFO     Epoch 18 [1200/1875]: loss = 0.19188933074474335, lr = 0.0001\n",
            "2025-05-29 01:19:20 INFO     Epoch 18 [1250/1875]: loss = 0.20194005966186523, lr = 0.0001\n",
            "2025-05-29 01:19:27 INFO     Epoch 18 [1300/1875]: loss = 0.17643019556999207, lr = 0.0001\n",
            "2025-05-29 01:19:34 INFO     Epoch 18 [1350/1875]: loss = 0.10814963281154633, lr = 0.0001\n",
            "2025-05-29 01:19:41 INFO     Epoch 18 [1400/1875]: loss = 0.209246426820755, lr = 0.0001\n",
            "2025-05-29 01:19:48 INFO     Epoch 18 [1450/1875]: loss = 0.21967561542987823, lr = 0.0001\n",
            "2025-05-29 01:19:55 INFO     Epoch 18 [1500/1875]: loss = 0.1737968921661377, lr = 0.0001\n",
            "2025-05-29 01:20:02 INFO     Epoch 18 [1550/1875]: loss = 0.13362857699394226, lr = 0.0001\n",
            "2025-05-29 01:20:08 INFO     Epoch 18 [1600/1875]: loss = 0.20987163484096527, lr = 0.0001\n",
            "2025-05-29 01:20:15 INFO     Epoch 18 [1650/1875]: loss = 0.23468388617038727, lr = 0.0001\n",
            "2025-05-29 01:20:22 INFO     Epoch 18 [1700/1875]: loss = 0.20023566484451294, lr = 0.0001\n",
            "2025-05-29 01:20:29 INFO     Epoch 18 [1750/1875]: loss = 0.19431403279304504, lr = 0.0001\n",
            "2025-05-29 01:20:36 INFO     Epoch 18 [1800/1875]: loss = 0.20453588664531708, lr = 0.0001\n",
            "2025-05-29 01:20:42 INFO     Epoch 18 [1850/1875]: loss = 0.18249264359474182, lr = 0.0001\n",
            "2025-05-29 01:20:46 INFO     Epoch 19 [0/1875]: loss = 0.20309101045131683, lr = 0.0001\n",
            "2025-05-29 01:20:53 INFO     Epoch 19 [50/1875]: loss = 0.23300370573997498, lr = 0.0001\n",
            "2025-05-29 01:21:00 INFO     Epoch 19 [100/1875]: loss = 0.2312999665737152, lr = 0.0001\n",
            "2025-05-29 01:21:07 INFO     Epoch 19 [150/1875]: loss = 0.1636960655450821, lr = 0.0001\n",
            "2025-05-29 01:21:13 INFO     Epoch 19 [200/1875]: loss = 0.1473740190267563, lr = 0.0001\n",
            "2025-05-29 01:21:20 INFO     Epoch 19 [250/1875]: loss = 0.1734292209148407, lr = 0.0001\n",
            "2025-05-29 01:21:27 INFO     Epoch 19 [300/1875]: loss = 0.21310850977897644, lr = 0.0001\n",
            "2025-05-29 01:21:34 INFO     Epoch 19 [350/1875]: loss = 0.17649364471435547, lr = 0.0001\n",
            "2025-05-29 01:21:41 INFO     Epoch 19 [400/1875]: loss = 0.15983396768569946, lr = 0.0001\n",
            "2025-05-29 01:21:47 INFO     Epoch 19 [450/1875]: loss = 0.18429505825042725, lr = 0.0001\n",
            "2025-05-29 01:21:54 INFO     Epoch 19 [500/1875]: loss = 0.20369023084640503, lr = 0.0001\n",
            "2025-05-29 01:22:01 INFO     Epoch 19 [550/1875]: loss = 0.22217804193496704, lr = 0.0001\n",
            "2025-05-29 01:22:08 INFO     Epoch 19 [600/1875]: loss = 0.21920545399188995, lr = 0.0001\n",
            "2025-05-29 01:22:15 INFO     Epoch 19 [650/1875]: loss = 0.23024775087833405, lr = 0.0001\n",
            "2025-05-29 01:22:21 INFO     Epoch 19 [700/1875]: loss = 0.20944614708423615, lr = 0.0001\n",
            "2025-05-29 01:22:28 INFO     Epoch 19 [750/1875]: loss = 0.24268417060375214, lr = 0.0001\n",
            "2025-05-29 01:22:35 INFO     Epoch 19 [800/1875]: loss = 0.1952369213104248, lr = 0.0001\n",
            "2025-05-29 01:22:42 INFO     Epoch 19 [850/1875]: loss = 0.11916076391935349, lr = 0.0001\n",
            "2025-05-29 01:22:49 INFO     Epoch 19 [900/1875]: loss = 0.17827318608760834, lr = 0.0001\n",
            "2025-05-29 01:22:56 INFO     Epoch 19 [950/1875]: loss = 0.1946074366569519, lr = 0.0001\n",
            "2025-05-29 01:23:02 INFO     Epoch 19 [1000/1875]: loss = 0.22633084654808044, lr = 0.0001\n",
            "2025-05-29 01:23:09 INFO     Epoch 19 [1050/1875]: loss = 0.19544266164302826, lr = 0.0001\n",
            "2025-05-29 01:23:16 INFO     Epoch 19 [1100/1875]: loss = 0.1891765147447586, lr = 0.0001\n",
            "2025-05-29 01:23:23 INFO     Epoch 19 [1150/1875]: loss = 0.1725110113620758, lr = 0.0001\n",
            "2025-05-29 01:23:30 INFO     Epoch 19 [1200/1875]: loss = 0.21491768956184387, lr = 0.0001\n",
            "2025-05-29 01:23:36 INFO     Epoch 19 [1250/1875]: loss = 0.1108384057879448, lr = 0.0001\n",
            "2025-05-29 01:23:43 INFO     Epoch 19 [1300/1875]: loss = 0.23156395554542542, lr = 0.0001\n",
            "2025-05-29 01:23:50 INFO     Epoch 19 [1350/1875]: loss = 0.19407905638217926, lr = 0.0001\n",
            "2025-05-29 01:23:57 INFO     Epoch 19 [1400/1875]: loss = 0.1709010899066925, lr = 0.0001\n",
            "2025-05-29 01:24:04 INFO     Epoch 19 [1450/1875]: loss = 0.2093919962644577, lr = 0.0001\n",
            "2025-05-29 01:24:10 INFO     Epoch 19 [1500/1875]: loss = 0.15256203711032867, lr = 0.0001\n",
            "2025-05-29 01:24:17 INFO     Epoch 19 [1550/1875]: loss = 0.14527389407157898, lr = 0.0001\n",
            "2025-05-29 01:24:24 INFO     Epoch 19 [1600/1875]: loss = 0.16637417674064636, lr = 0.0001\n",
            "2025-05-29 01:24:31 INFO     Epoch 19 [1650/1875]: loss = 0.2230743169784546, lr = 0.0001\n",
            "2025-05-29 01:24:38 INFO     Epoch 19 [1700/1875]: loss = 0.2358899563550949, lr = 0.0001\n",
            "2025-05-29 01:24:44 INFO     Epoch 19 [1750/1875]: loss = 0.13585776090621948, lr = 0.0001\n",
            "2025-05-29 01:24:51 INFO     Epoch 19 [1800/1875]: loss = 0.16328071057796478, lr = 0.0001\n",
            "2025-05-29 01:24:58 INFO     Epoch 19 [1850/1875]: loss = 0.19278806447982788, lr = 0.0001\n",
            "2025-05-29 01:25:03 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "2025-05-29 01:25:46 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:25:50 INFO     Evaluating [0/313] samples generated [32/1000] running fid 246.11891174316406\n"
          ]
        }
      ]
    }
  ]
}