{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fernsrea/flow_matching/blob/main/CIFAR_10_Discreet_Reduced_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jRnYLF-sE8H",
        "outputId": "2740871f-8b15-4d48-dc83-aecac4f9acd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'flow_matching'...\n",
            "remote: Enumerating objects: 317, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (131/131), done.\u001b[K\n",
            "remote: Total 317 (delta 90), reused 21 (delta 21), pack-reused 165 (from 1)\u001b[K\n",
            "Receiving objects: 100% (317/317), 3.29 MiB | 20.90 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n",
            "/content/flow_matching\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Fernsrea/flow_matching.git\n",
        "%cd flow_matching"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd examples/image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1R7FXmqsYCR",
        "outputId": "14f4e3da-5321-455e-d691-8226f706c9f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/flow_matching/examples/image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzyJxPEasbeA",
        "outputId": "2bd4bdc9-a018-44ef-8cf0-3ec9bbb14124"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting submitit (from -r requirements.txt (line 1))\n",
            "  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.21.0+cu124)\n",
            "Collecting torchmetrics[image] (from -r requirements.txt (line 2))\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: scipy>1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (1.15.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 3)) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[image]->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity<=0.4.0->torchmetrics[image]->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.0.2)\n",
            "Downloading submitit-1.5.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m880.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: submitit, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, torch-fidelity\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 submitit-1.5.2 torch-fidelity-0.3.0 torchmetrics-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSbE2uw71gz3",
        "outputId": "a3a76ab7-ab23-43e9-b0eb-ea41f8817eaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.4.0->torchdiffeq) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.2)\n",
            "Downloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flow_matching"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4SivJfO2boL",
        "outputId": "ced56998-0be3-4170-cfaa-c2cc2fa01271"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flow_matching\n",
            "  Downloading flow_matching-1.0.10-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flow_matching) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flow_matching) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.11/dist-packages (from flow_matching) (0.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flow_matching) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq->flow_matching) (1.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flow_matching) (3.0.2)\n",
            "Downloading flow_matching-1.0.10-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flow_matching\n",
            "Successfully installed flow_matching-1.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!python train.py \\\n",
        "    --dataset=cifar10 \\\n",
        "    --discrete_flow_matching \\\n",
        "    --batch_size=32 \\\n",
        "    --accum_iter=1 \\\n",
        "    --cfg_scale=0.0 \\\n",
        "    --use_ema \\\n",
        "    --epochs=500 \\\n",
        "    --class_drop_prob=1.0 \\\n",
        "    --compute_fid \\\n",
        "    --sym_func \\\n",
        "    --output_dir='/content/drive/MyDrive/flow_matching_checkpoints'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rzsI7hIC7hI",
        "outputId": "d275ab9b-dcb9-433e-e3a4-f64af8240d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Not using distributed mode\n",
            "2025-05-07 09:41:54 INFO     job dir: /content/flow_matching/examples/image\n",
            "2025-05-07 09:41:54 INFO     Namespace(batch_size=32,\n",
            "epochs=500,\n",
            "accum_iter=1,\n",
            "lr=0.0001,\n",
            "optimizer_betas=[0.9,\n",
            "0.95],\n",
            "decay_lr=False,\n",
            "class_drop_prob=1.0,\n",
            "skewed_timesteps=False,\n",
            "edm_schedule=False,\n",
            "use_ema=True,\n",
            "dataset='cifar10',\n",
            "data_path='./data/image_generation',\n",
            "output_dir='/content/drive/MyDrive/flow_matching_checkpoints',\n",
            "ode_method='midpoint',\n",
            "ode_options={'step_size': 0.01},\n",
            "sym=0.0,\n",
            "temp=1.0,\n",
            "sym_func=True,\n",
            "sampling_dtype='float32',\n",
            "cfg_scale=0.0,\n",
            "fid_samples=50000,\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='',\n",
            "start_epoch=0,\n",
            "eval_only=False,\n",
            "eval_frequency=50,\n",
            "compute_fid=True,\n",
            "save_fid_samples=False,\n",
            "num_workers=10,\n",
            "pin_mem=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "test_run=False,\n",
            "discrete_flow_matching=True,\n",
            "discrete_fm_steps=1024,\n",
            "distributed=False)\n",
            "2025-05-07 09:41:54 INFO     Saving args to /content/drive/MyDrive/flow_matching_checkpoints/args.json\n",
            "2025-05-07 09:41:54 INFO     Initializing Dataset: cifar10\n",
            "2025-05-07 09:41:55 INFO     Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./data/image_generation\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "                 ToImage()\n",
            "                 RandomHorizontalFlip(p=0.5)\n",
            "                 ToDtype(scale=True)\n",
            "           )\n",
            "2025-05-07 09:41:55 INFO     Intializing DataLoader\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "2025-05-07 09:41:55 INFO     <torch.utils.data.distributed.DistributedSampler object at 0x7fcc6036fe50>\n",
            "2025-05-07 09:41:55 INFO     Initializing Model\n",
            "2025-05-07 09:41:56 INFO     EMA(\n",
            "  (model): DiscreteUNetModel(vocab_size=257, in_channels=3, model_channels=48, out_channels=3, num_res_blocks=2, attention_resolutions=[2], dropout=0.4, channel_mult=[2, 2, 2], conv_resample=False, dims=2, num_classes=None, use_checkpoint=False, num_heads=-1, num_head_channels=32, num_heads_upsample=-1, use_scale_shift_norm=True, resblock_updown=False, use_new_attention_order=True, with_fourier_features=False)\n",
            "  (shadow_params): ParameterList(\n",
            "      (0): Parameter containing: [torch.float32 of size 257x32 (cuda:0)]\n",
            "      (1): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (2): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (3): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (4): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (5): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (6): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (7): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (8): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (9): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (10): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (11): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (12): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (13): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (14): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (15): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (16): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (17): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (18): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (19): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (20): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (21): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (22): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (23): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (24): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (25): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (26): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (27): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (28): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (29): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (30): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (31): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (32): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (33): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (34): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (35): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (36): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (37): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (38): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (39): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (40): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (41): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (42): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (43): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (44): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (45): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (46): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (47): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (48): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (49): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (50): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (51): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (52): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (53): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (54): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (55): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (56): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (57): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (58): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (59): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (60): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (61): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (62): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (63): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (64): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (65): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (66): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (67): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (68): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (69): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (70): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (71): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (72): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (73): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (74): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (75): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (76): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (77): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (78): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (79): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (80): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (81): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (82): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (83): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (84): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (85): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (86): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (87): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (88): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (89): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (90): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (91): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (92): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (93): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (94): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (95): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (96): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (97): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (98): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (99): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (100): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (101): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (102): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (103): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (104): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (105): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (106): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (107): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (108): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (109): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (110): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (111): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (112): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (113): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (114): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (115): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (116): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (117): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (118): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (119): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (120): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (121): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (122): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (123): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (124): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (125): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (126): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (127): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (128): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (129): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (130): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (131): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (132): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (133): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (134): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (135): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (136): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (137): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (138): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (139): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (140): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (141): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (142): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (143): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (144): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (145): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (146): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (147): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (148): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (149): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (150): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (151): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (152): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (153): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (154): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (155): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (156): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (157): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (158): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (159): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (160): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (161): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (162): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (163): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (164): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (165): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (166): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (167): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (168): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (169): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (170): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (171): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (172): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (173): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (174): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (175): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (176): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (177): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (178): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (179): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (180): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (181): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (182): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (183): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (184): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (185): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (186): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (187): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (188): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (189): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (190): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (191): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (192): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (193): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (194): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (195): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (196): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (197): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (198): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (199): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (200): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (201): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (202): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (203): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (204): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (205): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (206): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (207): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (208): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (209): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (210): Parameter containing: [torch.float32 of size 771x96x3x3 (cuda:0)]\n",
            "      (211): Parameter containing: [torch.float32 of size 771 (cuda:0)]\n",
            "  )\n",
            ")\n",
            "2025-05-07 09:41:56 INFO     Learning rate: 1.00e-04\n",
            "2025-05-07 09:41:56 INFO     Accumulate grad iterations: 1\n",
            "2025-05-07 09:41:56 INFO     Effective batch size: 32\n",
            "2025-05-07 09:41:56 INFO     Optimizer: AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.95]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "2025-05-07 09:41:56 INFO     Learning-Rate Schedule: <torch.optim.lr_scheduler.ConstantLR object at 0x7fcc5b6d5990>\n",
            "/content/flow_matching/examples/image/training/grad_scaler.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "2025-05-07 09:41:56 INFO     Start from 0 to 500 epochs\n",
            "2025-05-07 09:41:59 INFO     Epoch 0 [0/1562]: loss = 5.549003601074219, lr = 0.0001\n",
            "2025-05-07 09:42:06 INFO     Epoch 0 [50/1562]: loss = 5.440501689910889, lr = 0.0001\n",
            "2025-05-07 09:42:14 INFO     Epoch 0 [100/1562]: loss = 5.3961663246154785, lr = 0.0001\n",
            "2025-05-07 09:42:21 INFO     Epoch 0 [150/1562]: loss = 5.302513599395752, lr = 0.0001\n",
            "2025-05-07 09:42:28 INFO     Epoch 0 [200/1562]: loss = 5.361532688140869, lr = 0.0001\n",
            "2025-05-07 09:42:35 INFO     Epoch 0 [250/1562]: loss = 5.143256664276123, lr = 0.0001\n",
            "2025-05-07 09:42:43 INFO     Epoch 0 [300/1562]: loss = 5.139333248138428, lr = 0.0001\n",
            "2025-05-07 09:42:50 INFO     Epoch 0 [350/1562]: loss = 5.079286098480225, lr = 0.0001\n",
            "2025-05-07 09:42:57 INFO     Epoch 0 [400/1562]: loss = 4.948858261108398, lr = 0.0001\n",
            "2025-05-07 09:43:05 INFO     Epoch 0 [450/1562]: loss = 4.869612693786621, lr = 0.0001\n",
            "2025-05-07 09:43:12 INFO     Epoch 0 [500/1562]: loss = 4.9375386238098145, lr = 0.0001\n",
            "2025-05-07 09:43:20 INFO     Epoch 0 [550/1562]: loss = 4.938690662384033, lr = 0.0001\n",
            "2025-05-07 09:43:27 INFO     Epoch 0 [600/1562]: loss = 4.833733081817627, lr = 0.0001\n",
            "2025-05-07 09:43:35 INFO     Epoch 0 [650/1562]: loss = 4.796179294586182, lr = 0.0001\n",
            "2025-05-07 09:43:42 INFO     Epoch 0 [700/1562]: loss = 4.728806018829346, lr = 0.0001\n",
            "2025-05-07 09:43:49 INFO     Epoch 0 [750/1562]: loss = 4.805337905883789, lr = 0.0001\n",
            "2025-05-07 09:43:56 INFO     Epoch 0 [800/1562]: loss = 4.847142696380615, lr = 0.0001\n",
            "2025-05-07 09:44:04 INFO     Epoch 0 [850/1562]: loss = 4.769394874572754, lr = 0.0001\n",
            "2025-05-07 09:44:11 INFO     Epoch 0 [900/1562]: loss = 4.444404125213623, lr = 0.0001\n",
            "2025-05-07 09:44:19 INFO     Epoch 0 [950/1562]: loss = 4.415278911590576, lr = 0.0001\n",
            "2025-05-07 09:44:26 INFO     Epoch 0 [1000/1562]: loss = 4.19108772277832, lr = 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Resume training:\n",
        "!python train.py \\\n",
        "    --dataset=cifar10 \\\n",
        "    --nodes=1 \\\n",
        "    --discrete_flow_matching \\\n",
        "    --batch_size=32 \\\n",
        "    --accum_iter=1 \\\n",
        "    --cfg_scale=0.0 \\\n",
        "    --use_ema \\\n",
        "    --epochs=500 \\\n",
        "    --class_drop_prob=1.0 \\\n",
        "    --compute_fid \\\n",
        "    --sym_func \\\n",
        "    --output_dir='/content/drive/MyDrive/flow_matching_checkpoints' \\\n",
        "    --resume='/content/drive/MyDrive/flow_matching_checkpoints/checkpoint-4.pth'"
      ],
      "metadata": {
        "id": "8h5TmsYdm8Zz",
        "outputId": "c51a8a5a-91a1-4f57-c463-4e4d2cfa2b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Not using distributed mode\n",
            "2025-05-07 09:20:34 INFO     job dir: /content/flow_matching/examples/image\n",
            "2025-05-07 09:20:34 INFO     Namespace(batch_size=64,\n",
            "epochs=500,\n",
            "accum_iter=1,\n",
            "lr=0.0001,\n",
            "optimizer_betas=[0.9,\n",
            "0.95],\n",
            "decay_lr=False,\n",
            "class_drop_prob=0.2,\n",
            "skewed_timesteps=False,\n",
            "edm_schedule=False,\n",
            "use_ema=True,\n",
            "dataset='cifar10',\n",
            "data_path='./data/image_generation',\n",
            "output_dir='/content/drive/MyDrive/flow_matching_checkpoints',\n",
            "ode_method='midpoint',\n",
            "ode_options={'step_size': 0.01},\n",
            "sym=0.0,\n",
            "temp=1.0,\n",
            "sym_func=False,\n",
            "sampling_dtype='float32',\n",
            "cfg_scale=0.0,\n",
            "fid_samples=50000,\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='/content/drive/MyDrive/flow_matching_checkpoints/checkpoint-4.pth',\n",
            "start_epoch=0,\n",
            "eval_only=False,\n",
            "eval_frequency=5,\n",
            "compute_fid=False,\n",
            "save_fid_samples=False,\n",
            "num_workers=10,\n",
            "pin_mem=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "test_run=False,\n",
            "discrete_flow_matching=True,\n",
            "discrete_fm_steps=1024,\n",
            "distributed=False)\n",
            "2025-05-07 09:20:34 INFO     Saving args to /content/drive/MyDrive/flow_matching_checkpoints/args.json\n",
            "2025-05-07 09:20:34 INFO     Initializing Dataset: cifar10\n",
            "2025-05-07 09:20:35 INFO     Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./data/image_generation\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "                 ToImage()\n",
            "                 RandomHorizontalFlip(p=0.5)\n",
            "                 ToDtype(scale=True)\n",
            "           )\n",
            "2025-05-07 09:20:35 INFO     Intializing DataLoader\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "2025-05-07 09:20:35 INFO     <torch.utils.data.distributed.DistributedSampler object at 0x7afeffe52ed0>\n",
            "2025-05-07 09:20:35 INFO     Initializing Model\n",
            "2025-05-07 09:20:35 INFO     EMA(\n",
            "  (model): DiscreteUNetModel(vocab_size=257, in_channels=3, model_channels=48, out_channels=3, num_res_blocks=2, attention_resolutions=[2], dropout=0.4, channel_mult=[2, 2, 2], conv_resample=False, dims=2, num_classes=None, use_checkpoint=False, num_heads=-1, num_head_channels=32, num_heads_upsample=-1, use_scale_shift_norm=True, resblock_updown=False, use_new_attention_order=True, with_fourier_features=False)\n",
            "  (shadow_params): ParameterList(\n",
            "      (0): Parameter containing: [torch.float32 of size 257x32 (cuda:0)]\n",
            "      (1): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (2): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (3): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (4): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (5): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (6): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (7): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (8): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (9): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (10): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (11): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (12): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (13): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (14): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (15): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (16): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (17): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (18): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (19): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (20): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (21): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (22): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (23): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (24): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (25): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (26): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (27): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (28): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (29): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (30): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (31): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (32): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (33): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (34): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (35): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (36): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (37): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (38): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (39): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (40): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (41): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (42): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (43): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (44): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (45): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (46): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (47): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (48): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (49): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (50): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (51): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (52): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (53): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (54): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (55): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (56): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (57): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (58): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (59): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (60): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (61): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (62): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (63): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (64): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (65): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (66): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (67): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (68): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (69): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (70): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (71): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (72): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (73): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (74): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (75): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (76): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (77): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (78): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (79): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (80): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (81): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (82): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (83): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (84): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (85): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (86): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (87): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (88): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (89): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (90): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (91): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (92): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (93): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (94): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (95): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (96): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (97): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (98): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (99): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (100): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (101): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (102): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (103): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (104): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (105): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (106): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (107): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (108): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (109): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (110): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (111): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (112): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (113): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (114): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (115): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (116): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (117): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (118): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (119): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (120): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (121): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (122): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (123): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (124): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (125): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (126): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (127): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (128): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (129): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (130): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (131): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (132): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (133): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (134): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (135): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (136): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (137): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (138): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (139): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (140): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (141): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (142): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (143): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (144): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (145): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (146): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (147): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (148): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (149): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (150): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (151): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (152): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (153): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (154): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (155): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (156): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (157): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (158): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (159): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (160): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (161): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (162): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (163): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (164): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (165): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (166): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (167): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (168): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (169): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (170): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (171): Parameter containing: [torch.float32 of size 288x96x1 (cuda:0)]\n",
            "      (172): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (173): Parameter containing: [torch.float32 of size 96x96x1 (cuda:0)]\n",
            "      (174): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (175): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (176): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (177): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (178): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (179): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (180): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (181): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (182): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (183): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (184): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (185): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (186): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (187): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (188): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (189): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (190): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (191): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (192): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (193): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (194): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (195): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (196): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (197): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (198): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (199): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (200): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (201): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (202): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (203): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (204): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (205): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (206): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (207): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (208): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (209): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (210): Parameter containing: [torch.float32 of size 771x96x3x3 (cuda:0)]\n",
            "      (211): Parameter containing: [torch.float32 of size 771 (cuda:0)]\n",
            "  )\n",
            ")\n",
            "2025-05-07 09:20:35 INFO     Learning rate: 1.00e-04\n",
            "2025-05-07 09:20:35 INFO     Accumulate grad iterations: 1\n",
            "2025-05-07 09:20:35 INFO     Effective batch size: 64\n",
            "2025-05-07 09:20:35 INFO     Optimizer: AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.95]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "2025-05-07 09:20:35 INFO     Learning-Rate Schedule: <torch.optim.lr_scheduler.ConstantLR object at 0x7afefed16d90>\n",
            "/content/flow_matching/examples/image/training/grad_scaler.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "Resume checkpoint /content/drive/MyDrive/flow_matching_checkpoints/checkpoint-4.pth\n",
            "With optim & sched!\n",
            "2025-05-07 09:20:35 INFO     Start from 5 to 500 epochs\n",
            "2025-05-07 09:20:41 INFO     Epoch 5 [0/781]: loss = 3.3819990158081055, lr = 0.0001\n",
            "2025-05-07 09:20:53 INFO     Epoch 5 [50/781]: loss = 3.2938106060028076, lr = 0.0001\n",
            "2025-05-07 09:21:06 INFO     Epoch 5 [100/781]: loss = 3.209210157394409, lr = 0.0001\n",
            "2025-05-07 09:21:19 INFO     Epoch 5 [150/781]: loss = 3.3562262058258057, lr = 0.0001\n",
            "2025-05-07 09:21:33 INFO     Epoch 5 [200/781]: loss = 3.333897829055786, lr = 0.0001\n",
            "2025-05-07 09:21:46 INFO     Epoch 5 [250/781]: loss = 3.4969942569732666, lr = 0.0001\n",
            "2025-05-07 09:22:00 INFO     Epoch 5 [300/781]: loss = 3.265996217727661, lr = 0.0001\n",
            "2025-05-07 09:22:13 INFO     Epoch 5 [350/781]: loss = 3.351478338241577, lr = 0.0001\n",
            "2025-05-07 09:22:27 INFO     Epoch 5 [400/781]: loss = 3.641618013381958, lr = 0.0001\n",
            "2025-05-07 09:22:40 INFO     Epoch 5 [450/781]: loss = 3.371121406555176, lr = 0.0001\n",
            "2025-05-07 09:22:53 INFO     Epoch 5 [500/781]: loss = 3.8795528411865234, lr = 0.0001\n",
            "2025-05-07 09:23:07 INFO     Epoch 5 [550/781]: loss = 3.7455084323883057, lr = 0.0001\n",
            "2025-05-07 09:23:20 INFO     Epoch 5 [600/781]: loss = 3.5238630771636963, lr = 0.0001\n",
            "2025-05-07 09:23:33 INFO     Epoch 5 [650/781]: loss = 3.337693452835083, lr = 0.0001\n",
            "2025-05-07 09:23:47 INFO     Epoch 5 [700/781]: loss = 3.5223209857940674, lr = 0.0001\n",
            "2025-05-07 09:24:00 INFO     Epoch 5 [750/781]: loss = 3.3564231395721436, lr = 0.0001\n",
            "2025-05-07 09:24:09 INFO     Epoch 6 [0/781]: loss = 3.7472193241119385, lr = 0.0001\n",
            "2025-05-07 09:24:23 INFO     Epoch 6 [50/781]: loss = 3.2237493991851807, lr = 0.0001\n",
            "2025-05-07 09:24:36 INFO     Epoch 6 [100/781]: loss = 3.3258602619171143, lr = 0.0001\n",
            "2025-05-07 09:24:49 INFO     Epoch 6 [150/781]: loss = 3.766636610031128, lr = 0.0001\n",
            "2025-05-07 09:25:03 INFO     Epoch 6 [200/781]: loss = 3.500889539718628, lr = 0.0001\n",
            "2025-05-07 09:25:16 INFO     Epoch 6 [250/781]: loss = 3.5044500827789307, lr = 0.0001\n",
            "2025-05-07 09:25:29 INFO     Epoch 6 [300/781]: loss = 3.39609432220459, lr = 0.0001\n",
            "2025-05-07 09:25:43 INFO     Epoch 6 [350/781]: loss = 3.422727346420288, lr = 0.0001\n",
            "2025-05-07 09:25:56 INFO     Epoch 6 [400/781]: loss = 3.386471748352051, lr = 0.0001\n",
            "2025-05-07 09:26:10 INFO     Epoch 6 [450/781]: loss = 3.392246961593628, lr = 0.0001\n",
            "2025-05-07 09:26:23 INFO     Epoch 6 [500/781]: loss = 3.111035108566284, lr = 0.0001\n",
            "2025-05-07 09:26:36 INFO     Epoch 6 [550/781]: loss = 3.602323293685913, lr = 0.0001\n",
            "2025-05-07 09:26:50 INFO     Epoch 6 [600/781]: loss = 3.3620643615722656, lr = 0.0001\n",
            "2025-05-07 09:27:03 INFO     Epoch 6 [650/781]: loss = 3.301954507827759, lr = 0.0001\n",
            "2025-05-07 09:27:17 INFO     Epoch 6 [700/781]: loss = 3.6264917850494385, lr = 0.0001\n",
            "2025-05-07 09:27:30 INFO     Epoch 6 [750/781]: loss = 3.476971387863159, lr = 0.0001\n",
            "2025-05-07 09:27:39 INFO     Epoch 7 [0/781]: loss = 3.403815507888794, lr = 0.0001\n",
            "2025-05-07 09:27:53 INFO     Epoch 7 [50/781]: loss = 3.420321464538574, lr = 0.0001\n",
            "2025-05-07 09:28:06 INFO     Epoch 7 [100/781]: loss = 3.677234411239624, lr = 0.0001\n",
            "2025-05-07 09:28:19 INFO     Epoch 7 [150/781]: loss = 3.484055280685425, lr = 0.0001\n",
            "2025-05-07 09:28:33 INFO     Epoch 7 [200/781]: loss = 3.597160577774048, lr = 0.0001\n",
            "2025-05-07 09:28:46 INFO     Epoch 7 [250/781]: loss = 3.5825135707855225, lr = 0.0001\n",
            "2025-05-07 09:29:00 INFO     Epoch 7 [300/781]: loss = 3.2156198024749756, lr = 0.0001\n",
            "2025-05-07 09:29:13 INFO     Epoch 7 [350/781]: loss = 3.6404693126678467, lr = 0.0001\n",
            "2025-05-07 09:29:26 INFO     Epoch 7 [400/781]: loss = 3.415297746658325, lr = 0.0001\n",
            "2025-05-07 09:29:39 INFO     Epoch 7 [450/781]: loss = 3.5545055866241455, lr = 0.0001\n",
            "2025-05-07 09:29:53 INFO     Epoch 7 [500/781]: loss = 3.379258871078491, lr = 0.0001\n",
            "2025-05-07 09:30:06 INFO     Epoch 7 [550/781]: loss = 3.4433534145355225, lr = 0.0001\n",
            "2025-05-07 09:30:20 INFO     Epoch 7 [600/781]: loss = 3.4479310512542725, lr = 0.0001\n",
            "2025-05-07 09:30:33 INFO     Epoch 7 [650/781]: loss = 3.0145018100738525, lr = 0.0001\n",
            "2025-05-07 09:30:46 INFO     Epoch 7 [700/781]: loss = 3.937258005142212, lr = 0.0001\n",
            "2025-05-07 09:31:00 INFO     Epoch 7 [750/781]: loss = 3.4203615188598633, lr = 0.0001\n",
            "2025-05-07 09:31:09 INFO     Epoch 8 [0/781]: loss = 2.9888343811035156, lr = 0.0001\n",
            "2025-05-07 09:31:22 INFO     Epoch 8 [50/781]: loss = 3.32830548286438, lr = 0.0001\n",
            "2025-05-07 09:31:35 INFO     Epoch 8 [100/781]: loss = 3.353632926940918, lr = 0.0001\n",
            "2025-05-07 09:31:49 INFO     Epoch 8 [150/781]: loss = 3.4410321712493896, lr = 0.0001\n",
            "2025-05-07 09:32:02 INFO     Epoch 8 [200/781]: loss = 3.1790542602539062, lr = 0.0001\n",
            "2025-05-07 09:32:16 INFO     Epoch 8 [250/781]: loss = 3.841717481613159, lr = 0.0001\n",
            "2025-05-07 09:32:29 INFO     Epoch 8 [300/781]: loss = 3.243454694747925, lr = 0.0001\n",
            "2025-05-07 09:32:42 INFO     Epoch 8 [350/781]: loss = 3.5223801136016846, lr = 0.0001\n",
            "2025-05-07 09:32:56 INFO     Epoch 8 [400/781]: loss = 3.4884862899780273, lr = 0.0001\n",
            "2025-05-07 09:33:09 INFO     Epoch 8 [450/781]: loss = 3.379932403564453, lr = 0.0001\n",
            "2025-05-07 09:33:22 INFO     Epoch 8 [500/781]: loss = 3.5243775844573975, lr = 0.0001\n",
            "2025-05-07 09:33:36 INFO     Epoch 8 [550/781]: loss = 3.618525266647339, lr = 0.0001\n",
            "2025-05-07 09:33:49 INFO     Epoch 8 [600/781]: loss = 3.4245100021362305, lr = 0.0001\n",
            "2025-05-07 09:34:02 INFO     Epoch 8 [650/781]: loss = 3.359130620956421, lr = 0.0001\n",
            "2025-05-07 09:34:16 INFO     Epoch 8 [700/781]: loss = 3.488173484802246, lr = 0.0001\n",
            "2025-05-07 09:34:29 INFO     Epoch 8 [750/781]: loss = 3.578298807144165, lr = 0.0001\n",
            "2025-05-07 09:34:38 INFO     Epoch 9 [0/781]: loss = 3.6429946422576904, lr = 0.0001\n",
            "2025-05-07 09:34:52 INFO     Epoch 9 [50/781]: loss = 3.4145591259002686, lr = 0.0001\n",
            "2025-05-07 09:35:05 INFO     Epoch 9 [100/781]: loss = 3.2477619647979736, lr = 0.0001\n",
            "2025-05-07 09:35:18 INFO     Epoch 9 [150/781]: loss = 3.283524513244629, lr = 0.0001\n",
            "2025-05-07 09:35:32 INFO     Epoch 9 [200/781]: loss = 3.0895864963531494, lr = 0.0001\n",
            "2025-05-07 09:35:45 INFO     Epoch 9 [250/781]: loss = 3.214240789413452, lr = 0.0001\n",
            "2025-05-07 09:35:58 INFO     Epoch 9 [300/781]: loss = 3.30517840385437, lr = 0.0001\n",
            "2025-05-07 09:36:12 INFO     Epoch 9 [350/781]: loss = 3.4397218227386475, lr = 0.0001\n",
            "2025-05-07 09:36:25 INFO     Epoch 9 [400/781]: loss = 3.2428739070892334, lr = 0.0001\n",
            "2025-05-07 09:36:38 INFO     Epoch 9 [450/781]: loss = 3.369626760482788, lr = 0.0001\n",
            "2025-05-07 09:36:52 INFO     Epoch 9 [500/781]: loss = 3.417215347290039, lr = 0.0001\n",
            "2025-05-07 09:37:05 INFO     Epoch 9 [550/781]: loss = 3.4261903762817383, lr = 0.0001\n",
            "2025-05-07 09:37:18 INFO     Epoch 9 [600/781]: loss = 3.2011988162994385, lr = 0.0001\n",
            "2025-05-07 09:37:32 INFO     Epoch 9 [650/781]: loss = 3.163597345352173, lr = 0.0001\n",
            "2025-05-07 09:37:45 INFO     Epoch 9 [700/781]: loss = 3.355113983154297, lr = 0.0001\n",
            "2025-05-07 09:37:58 INFO     Epoch 9 [750/781]: loss = 3.473282814025879, lr = 0.0001\n",
            "2025-05-07 09:38:08 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "/content/flow_matching/examples/image/training/eval_loop.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(), torch.no_grad():\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/flow_matching/examples/image/train.py\", line 231, in <module>\n",
            "    main(args)\n",
            "  File \"/content/flow_matching/examples/image/train.py\", line 202, in main\n",
            "    eval_stats = eval_model(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/training/eval_loop.py\", line 145, in eval_model\n",
            "    synthetic_samples = solver.sample(\n",
            "                        ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/flow_matching/solver/discrete_solver.py\", line 186, in sample\n",
            "    p_1t = self.model(x=x_t, t=t.repeat(x_t.shape[0]), **model_extras)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/training/eval_loop.py\", line 65, in forward\n",
            "    result = self.model(x, t, extra={\"label\": label})\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/models/ema.py\", line 60, in forward\n",
            "    return self.model(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/models/discrete_unet.py\", line 93, in forward\n",
            "    self.unet(self.pixel_embedding(x_t), t, extra)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/models/unet.py\", line 702, in forward\n",
            "    h = module(h, emb)\n",
            "        ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/models/unet.py\", line 97, in forward\n",
            "    x = layer(x)\n",
            "        ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/models/unet.py\", line 325, in forward\n",
            "    return checkpoint(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/models/nn.py\", line 148, in checkpoint\n",
            "    return func(*inputs)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/models/unet.py\", line 336, in _forward\n",
            "    h = self.attention(qkv)\n",
            "        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/flow_matching/examples/image/models/unet.py\", line 413, in forward\n",
            "    weight = torch.einsum(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 407, in einsum\n",
            "    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 176.27 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.87 GiB is free. Process 103311 has 9.87 GiB memory in use. Of the allocated memory 9.08 GiB is allocated by PyTorch, and 655.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset cifar10 --output_dir output_cifar10 --batch_size 64 --accum_iter=1 --eval_frequency=100 --epochs=3000 --class_drop_prob=1.0 --cfg_scale=0.0 --compute_fid --ode_method heun2 --ode_options '{\"nfe\": 50}' --use_ema --edm_schedule --skewed_timesteps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JvqNqDftWtS",
        "outputId": "02975d3f-6d60-49a2-80a4-5d3e43f7fe92"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using distributed mode\n",
            "2025-04-17 08:53:07 INFO     job dir: /content/flow_matching/examples/image\n",
            "2025-04-17 08:53:07 INFO     Namespace(batch_size=64,\n",
            "epochs=3000,\n",
            "accum_iter=1,\n",
            "lr=0.0001,\n",
            "optimizer_betas=[0.9,\n",
            "0.95],\n",
            "decay_lr=False,\n",
            "class_drop_prob=1.0,\n",
            "skewed_timesteps=True,\n",
            "edm_schedule=True,\n",
            "use_ema=True,\n",
            "dataset='cifar10',\n",
            "data_path='./data/image_generation',\n",
            "output_dir='output_cifar10',\n",
            "ode_method='heun2',\n",
            "ode_options={'nfe': 50},\n",
            "sym=0.0,\n",
            "temp=1.0,\n",
            "sym_func=False,\n",
            "sampling_dtype='float32',\n",
            "cfg_scale=0.0,\n",
            "fid_samples=50000,\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='',\n",
            "start_epoch=0,\n",
            "eval_only=False,\n",
            "eval_frequency=100,\n",
            "compute_fid=True,\n",
            "save_fid_samples=False,\n",
            "num_workers=10,\n",
            "pin_mem=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "test_run=False,\n",
            "discrete_flow_matching=False,\n",
            "discrete_fm_steps=1024,\n",
            "distributed=False)\n",
            "2025-04-17 08:53:07 INFO     Saving args to output_cifar10/args.json\n",
            "2025-04-17 08:53:07 INFO     Initializing Dataset: cifar10\n",
            "100% 170M/170M [00:04<00:00, 41.0MB/s]\n",
            "2025-04-17 08:53:14 INFO     Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./data/image_generation\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "                 ToImage()\n",
            "                 RandomHorizontalFlip(p=0.5)\n",
            "                 ToDtype(scale=True)\n",
            "           )\n",
            "2025-04-17 08:53:14 INFO     Intializing DataLoader\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "2025-04-17 08:53:14 INFO     <torch.utils.data.distributed.DistributedSampler object at 0x7bc46a52b410>\n",
            "2025-04-17 08:53:14 INFO     Initializing Model\n",
            "2025-04-17 08:53:15 INFO     EMA(\n",
            "  (model): UNetModel(in_channels=3, model_channels=128, out_channels=3, num_res_blocks=4, attention_resolutions=[2], dropout=0.3, channel_mult=[2, 2, 2], conv_resample=False, dims=2, num_classes=None, use_checkpoint=False, num_heads=1, num_head_channels=-1, num_heads_upsample=1, use_scale_shift_norm=True, resblock_updown=False, use_new_attention_order=True, with_fourier_features=False, ignore_time=False, input_projection=True, image_size=-1, _target_='lib.models.gd_unet.UNetModel')\n",
            "  (shadow_params): ParameterList(\n",
            "      (0): Parameter containing: [torch.float32 of size 512x128 (cuda:0)]\n",
            "      (1): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (2): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (3): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (4): Parameter containing: [torch.float32 of size 256x3x3x3 (cuda:0)]\n",
            "      (5): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (6): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (7): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (8): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (9): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (10): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (11): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (12): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (13): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (14): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (15): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (16): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (17): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (18): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (19): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (20): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (21): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (22): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (23): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (24): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (25): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (26): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (27): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (28): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (29): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (30): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (31): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (32): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (33): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (34): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (35): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (36): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (37): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (38): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (39): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (40): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (41): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (42): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (43): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (44): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (45): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (46): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (47): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (48): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (49): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (50): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (51): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (52): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (53): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (54): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (55): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (56): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (57): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (58): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (59): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (60): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (61): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (62): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (63): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (64): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (65): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (66): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (67): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (68): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (69): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (70): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (71): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (72): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (73): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (74): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (75): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (76): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (77): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (78): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (79): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (80): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (81): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (82): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (83): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (84): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (85): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (86): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (87): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (88): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (89): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (90): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (91): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (92): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (93): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (94): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (95): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (96): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (97): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (98): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (99): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (100): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (101): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (102): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (103): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (104): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (105): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (106): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (107): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (108): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (109): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (110): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (111): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (112): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (113): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (114): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (115): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (116): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (117): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (118): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (119): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (120): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (121): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (122): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (123): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (124): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (125): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (126): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (127): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (128): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (129): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (130): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (131): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (132): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (133): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (134): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (135): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (136): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (137): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (138): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (139): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (140): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (141): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (142): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (143): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (144): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (145): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (146): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (147): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (148): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (149): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (150): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (151): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (152): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (153): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (154): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (155): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (156): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (157): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (158): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (159): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (160): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (161): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (162): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (163): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (164): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (165): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (166): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (167): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (168): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (169): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (170): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (171): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (172): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (173): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (174): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (175): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (176): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (177): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (178): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (179): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (180): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (181): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (182): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (183): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (184): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (185): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (186): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (187): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (188): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (189): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (190): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (191): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (192): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (193): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (194): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (195): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (196): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (197): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (198): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (199): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (200): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (201): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (202): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (203): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (204): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (205): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (206): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (207): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (208): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (209): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (210): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (211): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (212): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (213): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (214): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (215): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (216): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (217): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (218): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (219): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (220): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (221): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (222): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (223): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (224): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (225): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (226): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (227): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (228): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (229): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (230): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (231): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (232): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (233): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (234): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (235): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (236): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (237): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (238): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (239): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (240): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (241): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (242): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (243): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (244): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (245): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (246): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (247): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (248): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (249): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (250): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (251): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (252): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (253): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (254): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (255): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (256): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (257): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (258): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (259): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (260): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (261): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (262): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (263): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (264): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (265): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (266): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (267): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (268): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (269): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (270): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (271): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (272): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (273): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (274): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (275): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (276): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (277): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (278): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (279): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (280): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (281): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (282): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (283): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (284): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (285): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (286): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (287): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (288): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (289): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (290): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (291): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (292): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (293): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (294): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (295): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (296): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (297): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (298): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (299): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (300): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (301): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (302): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (303): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (304): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (305): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (306): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (307): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (308): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (309): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (310): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (311): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (312): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (313): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (314): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (315): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (316): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (317): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (318): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (319): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (320): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (321): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (322): Parameter containing: [torch.float32 of size 768x256x1 (cuda:0)]\n",
            "      (323): Parameter containing: [torch.float32 of size 768 (cuda:0)]\n",
            "      (324): Parameter containing: [torch.float32 of size 256x256x1 (cuda:0)]\n",
            "      (325): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (326): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (327): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (328): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (329): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (330): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (331): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (332): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (333): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (334): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (335): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (336): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (337): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (338): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (339): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (340): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (341): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (342): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (343): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (344): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (345): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (346): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (347): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (348): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (349): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (350): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (351): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (352): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (353): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (354): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (355): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (356): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (357): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (358): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (359): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (360): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (361): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (362): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (363): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (364): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (365): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (366): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (367): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (368): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (369): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (370): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (371): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (372): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (373): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (374): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (375): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (376): Parameter containing: [torch.float32 of size 256x512x3x3 (cuda:0)]\n",
            "      (377): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (378): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
            "      (379): Parameter containing: [torch.float32 of size 512 (cuda:0)]\n",
            "      (380): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (381): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (382): Parameter containing: [torch.float32 of size 256x256x3x3 (cuda:0)]\n",
            "      (383): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (384): Parameter containing: [torch.float32 of size 256x512x1x1 (cuda:0)]\n",
            "      (385): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (386): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (387): Parameter containing: [torch.float32 of size 256 (cuda:0)]\n",
            "      (388): Parameter containing: [torch.float32 of size 3x256x3x3 (cuda:0)]\n",
            "      (389): Parameter containing: [torch.float32 of size 3 (cuda:0)]\n",
            "  )\n",
            ")\n",
            "2025-04-17 08:53:15 INFO     Learning rate: 1.00e-04\n",
            "2025-04-17 08:53:15 INFO     Accumulate grad iterations: 1\n",
            "2025-04-17 08:53:15 INFO     Effective batch size: 64\n",
            "2025-04-17 08:53:15 INFO     Optimizer: AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.95]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "2025-04-17 08:53:15 INFO     Learning-Rate Schedule: <torch.optim.lr_scheduler.ConstantLR object at 0x7bc461a04190>\n",
            "/content/flow_matching/examples/image/training/grad_scaler.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "2025-04-17 08:53:15 INFO     Start from 0 to 3000 epochs\n",
            "/content/flow_matching/examples/image/training/train_loop.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "2025-04-17 08:53:20 INFO     Epoch 0 [0/781]: loss = 1.2690505981445312, lr = 0.0001\n",
            "2025-04-17 08:53:53 INFO     Epoch 0 [50/781]: loss = 0.39588260650634766, lr = 0.0001\n",
            "2025-04-17 08:54:26 INFO     Epoch 0 [100/781]: loss = 0.3167325258255005, lr = 0.0001\n",
            "2025-04-17 08:55:00 INFO     Epoch 0 [150/781]: loss = 0.2574637532234192, lr = 0.0001\n",
            "2025-04-17 08:55:34 INFO     Epoch 0 [200/781]: loss = 0.21366429328918457, lr = 0.0001\n",
            "2025-04-17 08:56:08 INFO     Epoch 0 [250/781]: loss = 0.2729591727256775, lr = 0.0001\n",
            "2025-04-17 08:56:42 INFO     Epoch 0 [300/781]: loss = 0.2141873836517334, lr = 0.0001\n",
            "2025-04-17 08:57:17 INFO     Epoch 0 [350/781]: loss = 0.22606389224529266, lr = 0.0001\n",
            "2025-04-17 08:57:51 INFO     Epoch 0 [400/781]: loss = 0.2152235209941864, lr = 0.0001\n",
            "2025-04-17 08:58:25 INFO     Epoch 0 [450/781]: loss = 0.22886864840984344, lr = 0.0001\n",
            "2025-04-17 08:58:59 INFO     Epoch 0 [500/781]: loss = 0.23081204295158386, lr = 0.0001\n",
            "2025-04-17 08:59:33 INFO     Epoch 0 [550/781]: loss = 0.2134130895137787, lr = 0.0001\n",
            "2025-04-17 09:00:07 INFO     Epoch 0 [600/781]: loss = 0.20946133136749268, lr = 0.0001\n",
            "2025-04-17 09:00:42 INFO     Epoch 0 [650/781]: loss = 0.2337268441915512, lr = 0.0001\n",
            "2025-04-17 09:01:16 INFO     Epoch 0 [700/781]: loss = 0.22273653745651245, lr = 0.0001\n",
            "2025-04-17 09:01:50 INFO     Epoch 0 [750/781]: loss = 0.2008974850177765, lr = 0.0001\n",
            "2025-04-17 09:02:12 INFO     Epoch 1 [0/781]: loss = 0.19051897525787354, lr = 0.0001\n",
            "2025-04-17 09:02:46 INFO     Epoch 1 [50/781]: loss = 0.21678398549556732, lr = 0.0001\n",
            "2025-04-17 09:03:20 INFO     Epoch 1 [100/781]: loss = 0.20141810178756714, lr = 0.0001\n",
            "2025-04-17 09:03:55 INFO     Epoch 1 [150/781]: loss = 0.20764145255088806, lr = 0.0001\n",
            "2025-04-17 09:04:29 INFO     Epoch 1 [200/781]: loss = 0.18706795573234558, lr = 0.0001\n",
            "2025-04-17 09:05:03 INFO     Epoch 1 [250/781]: loss = 0.23258349299430847, lr = 0.0001\n",
            "2025-04-17 09:05:37 INFO     Epoch 1 [300/781]: loss = 0.22095182538032532, lr = 0.0001\n",
            "2025-04-17 09:06:11 INFO     Epoch 1 [350/781]: loss = 0.20588165521621704, lr = 0.0001\n",
            "2025-04-17 09:06:45 INFO     Epoch 1 [400/781]: loss = 0.18197381496429443, lr = 0.0001\n",
            "2025-04-17 09:07:19 INFO     Epoch 1 [450/781]: loss = 0.19729745388031006, lr = 0.0001\n",
            "2025-04-17 09:07:53 INFO     Epoch 1 [500/781]: loss = 0.18707603216171265, lr = 0.0001\n",
            "2025-04-17 09:08:28 INFO     Epoch 1 [550/781]: loss = 0.2105555534362793, lr = 0.0001\n",
            "2025-04-17 09:09:02 INFO     Epoch 1 [600/781]: loss = 0.1953573077917099, lr = 0.0001\n",
            "2025-04-17 09:09:36 INFO     Epoch 1 [650/781]: loss = 0.18791460990905762, lr = 0.0001\n",
            "2025-04-17 09:10:10 INFO     Epoch 1 [700/781]: loss = 0.20253634452819824, lr = 0.0001\n",
            "2025-04-17 09:10:44 INFO     Epoch 1 [750/781]: loss = 0.23993274569511414, lr = 0.0001\n",
            "2025-04-17 09:11:07 INFO     Epoch 2 [0/781]: loss = 0.1836549937725067, lr = 0.0001\n",
            "2025-04-17 09:11:41 INFO     Epoch 2 [50/781]: loss = 0.19532375037670135, lr = 0.0001\n",
            "2025-04-17 09:12:15 INFO     Epoch 2 [100/781]: loss = 0.18377122282981873, lr = 0.0001\n",
            "2025-04-17 09:12:50 INFO     Epoch 2 [150/781]: loss = 0.16717159748077393, lr = 0.0001\n",
            "2025-04-17 09:13:24 INFO     Epoch 2 [200/781]: loss = 0.20628076791763306, lr = 0.0001\n",
            "2025-04-17 09:13:58 INFO     Epoch 2 [250/781]: loss = 0.19085656106472015, lr = 0.0001\n",
            "2025-04-17 09:14:32 INFO     Epoch 2 [300/781]: loss = 0.17578762769699097, lr = 0.0001\n",
            "2025-04-17 09:15:06 INFO     Epoch 2 [350/781]: loss = 0.2072685956954956, lr = 0.0001\n",
            "2025-04-17 09:15:40 INFO     Epoch 2 [400/781]: loss = 0.19337467849254608, lr = 0.0001\n",
            "2025-04-17 09:16:14 INFO     Epoch 2 [450/781]: loss = 0.21467474102973938, lr = 0.0001\n",
            "2025-04-17 09:16:48 INFO     Epoch 2 [500/781]: loss = 0.20417504012584686, lr = 0.0001\n",
            "2025-04-17 09:17:22 INFO     Epoch 2 [550/781]: loss = 0.20916643738746643, lr = 0.0001\n",
            "2025-04-17 09:17:57 INFO     Epoch 2 [600/781]: loss = 0.19796261191368103, lr = 0.0001\n",
            "2025-04-17 09:18:31 INFO     Epoch 2 [650/781]: loss = 0.22070658206939697, lr = 0.0001\n",
            "2025-04-17 09:19:05 INFO     Epoch 2 [700/781]: loss = 0.194630965590477, lr = 0.0001\n",
            "2025-04-17 09:19:39 INFO     Epoch 2 [750/781]: loss = 0.2237108200788498, lr = 0.0001\n",
            "2025-04-17 09:20:01 INFO     Epoch 3 [0/781]: loss = 0.21703355014324188, lr = 0.0001\n",
            "2025-04-17 09:20:35 INFO     Epoch 3 [50/781]: loss = 0.20931218564510345, lr = 0.0001\n",
            "2025-04-17 09:21:09 INFO     Epoch 3 [100/781]: loss = 0.1962609738111496, lr = 0.0001\n",
            "2025-04-17 09:21:43 INFO     Epoch 3 [150/781]: loss = 0.18569374084472656, lr = 0.0001\n",
            "2025-04-17 09:22:17 INFO     Epoch 3 [200/781]: loss = 0.19049876928329468, lr = 0.0001\n",
            "2025-04-17 09:22:51 INFO     Epoch 3 [250/781]: loss = 0.18748316168785095, lr = 0.0001\n",
            "2025-04-17 09:23:25 INFO     Epoch 3 [300/781]: loss = 0.19134342670440674, lr = 0.0001\n",
            "2025-04-17 09:23:59 INFO     Epoch 3 [350/781]: loss = 0.18596872687339783, lr = 0.0001\n",
            "2025-04-17 09:24:34 INFO     Epoch 3 [400/781]: loss = 0.18249867856502533, lr = 0.0001\n",
            "2025-04-17 09:25:08 INFO     Epoch 3 [450/781]: loss = 0.20058941841125488, lr = 0.0001\n",
            "2025-04-17 09:25:42 INFO     Epoch 3 [500/781]: loss = 0.2027120292186737, lr = 0.0001\n",
            "2025-04-17 09:26:16 INFO     Epoch 3 [550/781]: loss = 0.20231735706329346, lr = 0.0001\n",
            "2025-04-17 09:26:50 INFO     Epoch 3 [600/781]: loss = 0.20439958572387695, lr = 0.0001\n",
            "2025-04-17 09:27:24 INFO     Epoch 3 [650/781]: loss = 0.18566159904003143, lr = 0.0001\n",
            "2025-04-17 09:27:58 INFO     Epoch 3 [700/781]: loss = 0.2061748504638672, lr = 0.0001\n",
            "2025-04-17 09:28:32 INFO     Epoch 3 [750/781]: loss = 0.20343220233917236, lr = 0.0001\n",
            "2025-04-17 09:28:55 INFO     Epoch 4 [0/781]: loss = 0.19190876185894012, lr = 0.0001\n",
            "2025-04-17 09:29:29 INFO     Epoch 4 [50/781]: loss = 0.1891046166419983, lr = 0.0001\n",
            "2025-04-17 09:30:03 INFO     Epoch 4 [100/781]: loss = 0.1839616298675537, lr = 0.0001\n",
            "2025-04-17 09:30:37 INFO     Epoch 4 [150/781]: loss = 0.18303261697292328, lr = 0.0001\n",
            "2025-04-17 09:31:11 INFO     Epoch 4 [200/781]: loss = 0.18782281875610352, lr = 0.0001\n",
            "2025-04-17 09:31:45 INFO     Epoch 4 [250/781]: loss = 0.20793776214122772, lr = 0.0001\n",
            "2025-04-17 09:32:19 INFO     Epoch 4 [300/781]: loss = 0.175978422164917, lr = 0.0001\n",
            "2025-04-17 09:32:54 INFO     Epoch 4 [350/781]: loss = 0.1807139366865158, lr = 0.0001\n",
            "2025-04-17 09:33:28 INFO     Epoch 4 [400/781]: loss = 0.19041812419891357, lr = 0.0001\n",
            "2025-04-17 09:34:02 INFO     Epoch 4 [450/781]: loss = 0.16084079444408417, lr = 0.0001\n",
            "2025-04-17 09:34:36 INFO     Epoch 4 [500/781]: loss = 0.20923976600170135, lr = 0.0001\n",
            "2025-04-17 09:35:10 INFO     Epoch 4 [550/781]: loss = 0.19188305735588074, lr = 0.0001\n",
            "2025-04-17 09:35:44 INFO     Epoch 4 [600/781]: loss = 0.18772852420806885, lr = 0.0001\n",
            "2025-04-17 09:36:18 INFO     Epoch 4 [650/781]: loss = 0.19194619357585907, lr = 0.0001\n",
            "2025-04-17 09:36:52 INFO     Epoch 4 [700/781]: loss = 0.18795692920684814, lr = 0.0001\n",
            "2025-04-17 09:37:27 INFO     Epoch 4 [750/781]: loss = 0.191559836268425, lr = 0.0001\n",
            "2025-04-17 09:37:49 INFO     Epoch 5 [0/781]: loss = 0.19172146916389465, lr = 0.0001\n",
            "2025-04-17 09:38:23 INFO     Epoch 5 [50/781]: loss = 0.1819886416196823, lr = 0.0001\n",
            "2025-04-17 09:38:57 INFO     Epoch 5 [100/781]: loss = 0.18365070223808289, lr = 0.0001\n",
            "2025-04-17 09:39:31 INFO     Epoch 5 [150/781]: loss = 0.18155574798583984, lr = 0.0001\n",
            "2025-04-17 09:40:05 INFO     Epoch 5 [200/781]: loss = 0.20613281428813934, lr = 0.0001\n",
            "2025-04-17 09:40:40 INFO     Epoch 5 [250/781]: loss = 0.19818627834320068, lr = 0.0001\n",
            "2025-04-17 09:41:14 INFO     Epoch 5 [300/781]: loss = 0.1705714762210846, lr = 0.0001\n",
            "2025-04-17 09:41:48 INFO     Epoch 5 [350/781]: loss = 0.1712859869003296, lr = 0.0001\n",
            "2025-04-17 09:42:22 INFO     Epoch 5 [400/781]: loss = 0.18512040376663208, lr = 0.0001\n",
            "2025-04-17 09:42:56 INFO     Epoch 5 [450/781]: loss = 0.17423617839813232, lr = 0.0001\n",
            "2025-04-17 09:43:30 INFO     Epoch 5 [500/781]: loss = 0.20096322894096375, lr = 0.0001\n",
            "2025-04-17 09:44:05 INFO     Epoch 5 [550/781]: loss = 0.2231142818927765, lr = 0.0001\n",
            "2025-04-17 09:44:39 INFO     Epoch 5 [600/781]: loss = 0.18500111997127533, lr = 0.0001\n",
            "2025-04-17 09:45:13 INFO     Epoch 5 [650/781]: loss = 0.19109341502189636, lr = 0.0001\n",
            "2025-04-17 09:45:47 INFO     Epoch 5 [700/781]: loss = 0.206669420003891, lr = 0.0001\n",
            "2025-04-17 09:46:21 INFO     Epoch 5 [750/781]: loss = 0.17921707034111023, lr = 0.0001\n",
            "2025-04-17 09:46:43 INFO     Epoch 6 [0/781]: loss = 0.19302301108837128, lr = 0.0001\n",
            "2025-04-17 09:47:18 INFO     Epoch 6 [50/781]: loss = 0.18664507567882538, lr = 0.0001\n",
            "2025-04-17 09:47:52 INFO     Epoch 6 [100/781]: loss = 0.19506633281707764, lr = 0.0001\n",
            "2025-04-17 09:48:26 INFO     Epoch 6 [150/781]: loss = 0.18421494960784912, lr = 0.0001\n",
            "2025-04-17 09:49:00 INFO     Epoch 6 [200/781]: loss = 0.18035152554512024, lr = 0.0001\n",
            "2025-04-17 09:49:34 INFO     Epoch 6 [250/781]: loss = 0.19770073890686035, lr = 0.0001\n",
            "2025-04-17 09:50:08 INFO     Epoch 6 [300/781]: loss = 0.1943836808204651, lr = 0.0001\n",
            "2025-04-17 09:50:42 INFO     Epoch 6 [350/781]: loss = 0.1712065190076828, lr = 0.0001\n",
            "2025-04-17 09:51:16 INFO     Epoch 6 [400/781]: loss = 0.1639152467250824, lr = 0.0001\n",
            "2025-04-17 09:51:50 INFO     Epoch 6 [450/781]: loss = 0.17477063834667206, lr = 0.0001\n",
            "2025-04-17 09:52:24 INFO     Epoch 6 [500/781]: loss = 0.19122302532196045, lr = 0.0001\n",
            "2025-04-17 09:52:58 INFO     Epoch 6 [550/781]: loss = 0.1961708664894104, lr = 0.0001\n",
            "2025-04-17 09:53:32 INFO     Epoch 6 [600/781]: loss = 0.1814488172531128, lr = 0.0001\n",
            "2025-04-17 09:54:06 INFO     Epoch 6 [650/781]: loss = 0.18622741103172302, lr = 0.0001\n",
            "2025-04-17 09:54:40 INFO     Epoch 6 [700/781]: loss = 0.19513118267059326, lr = 0.0001\n",
            "2025-04-17 09:55:14 INFO     Epoch 6 [750/781]: loss = 0.19307929277420044, lr = 0.0001\n",
            "2025-04-17 09:55:36 INFO     Epoch 7 [0/781]: loss = 0.17231541872024536, lr = 0.0001\n",
            "2025-04-17 09:56:10 INFO     Epoch 7 [50/781]: loss = 0.18340007960796356, lr = 0.0001\n",
            "2025-04-17 09:56:44 INFO     Epoch 7 [100/781]: loss = 0.20542728900909424, lr = 0.0001\n",
            "2025-04-17 09:57:18 INFO     Epoch 7 [150/781]: loss = 0.17269472777843475, lr = 0.0001\n",
            "2025-04-17 09:57:52 INFO     Epoch 7 [200/781]: loss = 0.21998058259487152, lr = 0.0001\n",
            "2025-04-17 09:58:26 INFO     Epoch 7 [250/781]: loss = 0.19040118157863617, lr = 0.0001\n",
            "2025-04-17 09:59:00 INFO     Epoch 7 [300/781]: loss = 0.1739916056394577, lr = 0.0001\n",
            "2025-04-17 09:59:34 INFO     Epoch 7 [350/781]: loss = 0.19322258234024048, lr = 0.0001\n",
            "2025-04-17 10:00:09 INFO     Epoch 7 [400/781]: loss = 0.18947991728782654, lr = 0.0001\n",
            "2025-04-17 10:00:43 INFO     Epoch 7 [450/781]: loss = 0.16746798157691956, lr = 0.0001\n",
            "2025-04-17 10:01:17 INFO     Epoch 7 [500/781]: loss = 0.17832587659358978, lr = 0.0001\n",
            "2025-04-17 10:01:51 INFO     Epoch 7 [550/781]: loss = 0.18300525844097137, lr = 0.0001\n",
            "2025-04-17 10:02:25 INFO     Epoch 7 [600/781]: loss = 0.18154005706310272, lr = 0.0001\n",
            "2025-04-17 10:02:59 INFO     Epoch 7 [650/781]: loss = 0.20559941232204437, lr = 0.0001\n",
            "2025-04-17 10:03:33 INFO     Epoch 7 [700/781]: loss = 0.1907215118408203, lr = 0.0001\n",
            "2025-04-17 10:04:07 INFO     Epoch 7 [750/781]: loss = 0.1854335069656372, lr = 0.0001\n",
            "2025-04-17 10:04:29 INFO     Epoch 8 [0/781]: loss = 0.20283488929271698, lr = 0.0001\n",
            "2025-04-17 10:05:03 INFO     Epoch 8 [50/781]: loss = 0.18546846508979797, lr = 0.0001\n",
            "2025-04-17 10:05:37 INFO     Epoch 8 [100/781]: loss = 0.1961940973997116, lr = 0.0001\n",
            "2025-04-17 10:06:11 INFO     Epoch 8 [150/781]: loss = 0.18177299201488495, lr = 0.0001\n",
            "2025-04-17 10:06:44 INFO     Epoch 8 [200/781]: loss = 0.17395305633544922, lr = 0.0001\n",
            "2025-04-17 10:07:18 INFO     Epoch 8 [250/781]: loss = 0.19678586721420288, lr = 0.0001\n",
            "2025-04-17 10:07:52 INFO     Epoch 8 [300/781]: loss = 0.2043527066707611, lr = 0.0001\n",
            "2025-04-17 10:08:26 INFO     Epoch 8 [350/781]: loss = 0.17867235839366913, lr = 0.0001\n",
            "2025-04-17 10:09:00 INFO     Epoch 8 [400/781]: loss = 0.18099266290664673, lr = 0.0001\n",
            "2025-04-17 10:09:34 INFO     Epoch 8 [450/781]: loss = 0.15543508529663086, lr = 0.0001\n",
            "2025-04-17 10:10:08 INFO     Epoch 8 [500/781]: loss = 0.19663546979427338, lr = 0.0001\n",
            "2025-04-17 10:10:42 INFO     Epoch 8 [550/781]: loss = 0.1810828149318695, lr = 0.0001\n",
            "2025-04-17 10:11:15 INFO     Epoch 8 [600/781]: loss = 0.16483554244041443, lr = 0.0001\n",
            "2025-04-17 10:11:49 INFO     Epoch 8 [650/781]: loss = 0.20616072416305542, lr = 0.0001\n",
            "2025-04-17 10:12:23 INFO     Epoch 8 [700/781]: loss = 0.18997815251350403, lr = 0.0001\n",
            "2025-04-17 10:12:57 INFO     Epoch 8 [750/781]: loss = 0.1936555802822113, lr = 0.0001\n",
            "2025-04-17 10:13:19 INFO     Epoch 9 [0/781]: loss = 0.1767980307340622, lr = 0.0001\n",
            "2025-04-17 10:13:53 INFO     Epoch 9 [50/781]: loss = 0.1783907264471054, lr = 0.0001\n",
            "2025-04-17 10:14:27 INFO     Epoch 9 [100/781]: loss = 0.20202267169952393, lr = 0.0001\n",
            "2025-04-17 10:15:01 INFO     Epoch 9 [150/781]: loss = 0.18218904733657837, lr = 0.0001\n",
            "2025-04-17 10:15:35 INFO     Epoch 9 [200/781]: loss = 0.16778044402599335, lr = 0.0001\n",
            "2025-04-17 10:16:09 INFO     Epoch 9 [250/781]: loss = 0.19565360248088837, lr = 0.0001\n",
            "2025-04-17 10:16:43 INFO     Epoch 9 [300/781]: loss = 0.1887447088956833, lr = 0.0001\n",
            "2025-04-17 10:17:16 INFO     Epoch 9 [350/781]: loss = 0.1834096908569336, lr = 0.0001\n",
            "2025-04-17 10:17:50 INFO     Epoch 9 [400/781]: loss = 0.19651281833648682, lr = 0.0001\n",
            "2025-04-17 10:18:24 INFO     Epoch 9 [450/781]: loss = 0.16701172292232513, lr = 0.0001\n",
            "2025-04-17 10:18:58 INFO     Epoch 9 [500/781]: loss = 0.1977374255657196, lr = 0.0001\n",
            "2025-04-17 10:19:32 INFO     Epoch 9 [550/781]: loss = 0.17805258929729462, lr = 0.0001\n",
            "2025-04-17 10:20:06 INFO     Epoch 9 [600/781]: loss = 0.17720097303390503, lr = 0.0001\n",
            "2025-04-17 10:20:40 INFO     Epoch 9 [650/781]: loss = 0.19044332206249237, lr = 0.0001\n",
            "2025-04-17 10:21:14 INFO     Epoch 9 [700/781]: loss = 0.19674187898635864, lr = 0.0001\n",
            "2025-04-17 10:21:48 INFO     Epoch 9 [750/781]: loss = 0.19547538459300995, lr = 0.0001\n",
            "2025-04-17 10:22:10 INFO     Epoch 10 [0/781]: loss = 0.17076699435710907, lr = 0.0001\n",
            "2025-04-17 10:22:44 INFO     Epoch 10 [50/781]: loss = 0.1862916201353073, lr = 0.0001\n",
            "2025-04-17 10:23:18 INFO     Epoch 10 [100/781]: loss = 0.18606525659561157, lr = 0.0001\n",
            "2025-04-17 10:23:52 INFO     Epoch 10 [150/781]: loss = 0.18241676688194275, lr = 0.0001\n",
            "2025-04-17 10:24:26 INFO     Epoch 10 [200/781]: loss = 0.20786850154399872, lr = 0.0001\n",
            "2025-04-17 10:25:01 INFO     Epoch 10 [250/781]: loss = 0.19596070051193237, lr = 0.0001\n",
            "2025-04-17 10:25:35 INFO     Epoch 10 [300/781]: loss = 0.17397837340831757, lr = 0.0001\n",
            "2025-04-17 10:26:09 INFO     Epoch 10 [350/781]: loss = 0.18165133893489838, lr = 0.0001\n",
            "2025-04-17 10:26:43 INFO     Epoch 10 [400/781]: loss = 0.20199429988861084, lr = 0.0001\n",
            "2025-04-17 10:27:17 INFO     Epoch 10 [450/781]: loss = 0.18205194175243378, lr = 0.0001\n",
            "2025-04-17 10:27:51 INFO     Epoch 10 [500/781]: loss = 0.20143213868141174, lr = 0.0001\n",
            "2025-04-17 10:28:26 INFO     Epoch 10 [550/781]: loss = 0.21959921717643738, lr = 0.0001\n",
            "2025-04-17 10:29:00 INFO     Epoch 10 [600/781]: loss = 0.189347505569458, lr = 0.0001\n",
            "2025-04-17 10:29:34 INFO     Epoch 10 [650/781]: loss = 0.19349059462547302, lr = 0.0001\n",
            "2025-04-17 10:30:08 INFO     Epoch 10 [700/781]: loss = 0.1924455612897873, lr = 0.0001\n",
            "2025-04-17 10:30:42 INFO     Epoch 10 [750/781]: loss = 0.18420687317848206, lr = 0.0001\n",
            "2025-04-17 10:31:04 INFO     Epoch 11 [0/781]: loss = 0.1970309466123581, lr = 0.0001\n",
            "2025-04-17 10:31:39 INFO     Epoch 11 [50/781]: loss = 0.19105744361877441, lr = 0.0001\n",
            "2025-04-17 10:32:13 INFO     Epoch 11 [100/781]: loss = 0.1946110725402832, lr = 0.0001\n",
            "2025-04-17 10:32:47 INFO     Epoch 11 [150/781]: loss = 0.20001018047332764, lr = 0.0001\n",
            "2025-04-17 10:33:21 INFO     Epoch 11 [200/781]: loss = 0.18796232342720032, lr = 0.0001\n",
            "2025-04-17 10:33:55 INFO     Epoch 11 [250/781]: loss = 0.20737320184707642, lr = 0.0001\n",
            "2025-04-17 10:34:29 INFO     Epoch 11 [300/781]: loss = 0.17302647233009338, lr = 0.0001\n",
            "2025-04-17 10:35:04 INFO     Epoch 11 [350/781]: loss = 0.20097756385803223, lr = 0.0001\n",
            "2025-04-17 10:35:38 INFO     Epoch 11 [400/781]: loss = 0.18393635749816895, lr = 0.0001\n",
            "2025-04-17 10:36:12 INFO     Epoch 11 [450/781]: loss = 0.1680130958557129, lr = 0.0001\n",
            "2025-04-17 10:36:46 INFO     Epoch 11 [500/781]: loss = 0.1749754548072815, lr = 0.0001\n",
            "2025-04-17 10:37:21 INFO     Epoch 11 [550/781]: loss = 0.2028127759695053, lr = 0.0001\n",
            "2025-04-17 10:37:55 INFO     Epoch 11 [600/781]: loss = 0.18078309297561646, lr = 0.0001\n",
            "2025-04-17 10:38:29 INFO     Epoch 11 [650/781]: loss = 0.19002699851989746, lr = 0.0001\n",
            "2025-04-17 10:39:03 INFO     Epoch 11 [700/781]: loss = 0.18963387608528137, lr = 0.0001\n",
            "2025-04-17 10:39:37 INFO     Epoch 11 [750/781]: loss = 0.19054758548736572, lr = 0.0001\n",
            "2025-04-17 10:39:59 INFO     Epoch 12 [0/781]: loss = 0.18299934267997742, lr = 0.0001\n",
            "2025-04-17 10:40:34 INFO     Epoch 12 [50/781]: loss = 0.19226083159446716, lr = 0.0001\n",
            "2025-04-17 10:41:08 INFO     Epoch 12 [100/781]: loss = 0.18165633082389832, lr = 0.0001\n",
            "2025-04-17 10:41:42 INFO     Epoch 12 [150/781]: loss = 0.19421568512916565, lr = 0.0001\n",
            "2025-04-17 10:42:16 INFO     Epoch 12 [200/781]: loss = 0.1772063672542572, lr = 0.0001\n",
            "2025-04-17 10:42:51 INFO     Epoch 12 [250/781]: loss = 0.17164520919322968, lr = 0.0001\n",
            "2025-04-17 10:43:25 INFO     Epoch 12 [300/781]: loss = 0.17352989315986633, lr = 0.0001\n",
            "2025-04-17 10:43:59 INFO     Epoch 12 [350/781]: loss = 0.16142290830612183, lr = 0.0001\n",
            "2025-04-17 10:44:33 INFO     Epoch 12 [400/781]: loss = 0.1648297756910324, lr = 0.0001\n",
            "2025-04-17 10:45:07 INFO     Epoch 12 [450/781]: loss = 0.17734408378601074, lr = 0.0001\n",
            "2025-04-17 10:45:41 INFO     Epoch 12 [500/781]: loss = 0.17843878269195557, lr = 0.0001\n",
            "2025-04-17 10:46:16 INFO     Epoch 12 [550/781]: loss = 0.21346363425254822, lr = 0.0001\n",
            "2025-04-17 10:46:50 INFO     Epoch 12 [600/781]: loss = 0.16787652671337128, lr = 0.0001\n",
            "2025-04-17 10:47:24 INFO     Epoch 12 [650/781]: loss = 0.20377986133098602, lr = 0.0001\n",
            "2025-04-17 10:47:58 INFO     Epoch 12 [700/781]: loss = 0.19246743619441986, lr = 0.0001\n",
            "2025-04-17 10:48:33 INFO     Epoch 12 [750/781]: loss = 0.18310964107513428, lr = 0.0001\n",
            "2025-04-17 10:48:55 INFO     Epoch 13 [0/781]: loss = 0.18727481365203857, lr = 0.0001\n",
            "2025-04-17 10:49:29 INFO     Epoch 13 [50/781]: loss = 0.20609116554260254, lr = 0.0001\n",
            "2025-04-17 10:50:03 INFO     Epoch 13 [100/781]: loss = 0.15930838882923126, lr = 0.0001\n",
            "2025-04-17 10:50:38 INFO     Epoch 13 [150/781]: loss = 0.1911160945892334, lr = 0.0001\n",
            "2025-04-17 10:51:12 INFO     Epoch 13 [200/781]: loss = 0.1832084357738495, lr = 0.0001\n",
            "2025-04-17 10:51:46 INFO     Epoch 13 [250/781]: loss = 0.2035561203956604, lr = 0.0001\n",
            "2025-04-17 10:52:20 INFO     Epoch 13 [300/781]: loss = 0.17981939017772675, lr = 0.0001\n",
            "2025-04-17 10:52:54 INFO     Epoch 13 [350/781]: loss = 0.18905887007713318, lr = 0.0001\n",
            "2025-04-17 10:53:29 INFO     Epoch 13 [400/781]: loss = 0.15683454275131226, lr = 0.0001\n",
            "2025-04-17 10:54:03 INFO     Epoch 13 [450/781]: loss = 0.17625534534454346, lr = 0.0001\n",
            "2025-04-17 10:54:37 INFO     Epoch 13 [500/781]: loss = 0.18138353526592255, lr = 0.0001\n",
            "2025-04-17 10:55:11 INFO     Epoch 13 [550/781]: loss = 0.19378070533275604, lr = 0.0001\n",
            "2025-04-17 10:55:45 INFO     Epoch 13 [600/781]: loss = 0.18098410964012146, lr = 0.0001\n",
            "2025-04-17 10:56:20 INFO     Epoch 13 [650/781]: loss = 0.19760063290596008, lr = 0.0001\n",
            "2025-04-17 10:56:54 INFO     Epoch 13 [700/781]: loss = 0.1794654130935669, lr = 0.0001\n",
            "2025-04-17 10:57:28 INFO     Epoch 13 [750/781]: loss = 0.18214258551597595, lr = 0.0001\n",
            "2025-04-17 10:57:50 INFO     Epoch 14 [0/781]: loss = 0.18179833889007568, lr = 0.0001\n",
            "2025-04-17 10:58:24 INFO     Epoch 14 [50/781]: loss = 0.18179862201213837, lr = 0.0001\n",
            "2025-04-17 10:58:59 INFO     Epoch 14 [100/781]: loss = 0.17488645017147064, lr = 0.0001\n",
            "2025-04-17 10:59:33 INFO     Epoch 14 [150/781]: loss = 0.18776550889015198, lr = 0.0001\n",
            "2025-04-17 11:00:07 INFO     Epoch 14 [200/781]: loss = 0.18034110963344574, lr = 0.0001\n",
            "2025-04-17 11:00:41 INFO     Epoch 14 [250/781]: loss = 0.20491795241832733, lr = 0.0001\n",
            "2025-04-17 11:01:15 INFO     Epoch 14 [300/781]: loss = 0.17326991260051727, lr = 0.0001\n",
            "2025-04-17 11:01:49 INFO     Epoch 14 [350/781]: loss = 0.1834104061126709, lr = 0.0001\n",
            "2025-04-17 11:02:24 INFO     Epoch 14 [400/781]: loss = 0.16781489551067352, lr = 0.0001\n",
            "2025-04-17 11:02:58 INFO     Epoch 14 [450/781]: loss = 0.18514135479927063, lr = 0.0001\n",
            "2025-04-17 11:03:32 INFO     Epoch 14 [500/781]: loss = 0.19530169665813446, lr = 0.0001\n",
            "2025-04-17 11:04:06 INFO     Epoch 14 [550/781]: loss = 0.1928226351737976, lr = 0.0001\n",
            "2025-04-17 11:04:40 INFO     Epoch 14 [600/781]: loss = 0.1801549643278122, lr = 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "KW301QW7RqwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir output_cifar10\n"
      ],
      "metadata": {
        "id": "GGufxAzLRsmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install submitit"
      ],
      "metadata": {
        "id": "EC8oAM6hRv7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4350b6a-d7b1-4099-c828-66d81b9315c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: submitit in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit) (3.1.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.11/dist-packages (from submitit) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import uuid  # Import the uuid module (although it's already in your submitit_train.py)\n",
        "\n",
        "shared_dir = \"/content/shared_experiments\" # Choose a path within /content/\n",
        "\n",
        "if not Path(shared_dir).is_dir():\n",
        "    os.makedirs(shared_dir, exist_ok=True)\n",
        "\n",
        "def get_shared_folder(shared_dir: str) -> Path:\n",
        "    user = os.getenv(\"USER\", \"default_user\") # Colab might not always have USER set\n",
        "    p = Path(shared_dir) / user / \"experiments\"\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def get_init_file(shared_dir: str):\n",
        "    shared_folder = get_shared_folder(shared_dir)\n",
        "    init_file = shared_folder / f\"{uuid.uuid4().hex}_init\"\n",
        "    if init_file.exists():\n",
        "        os.remove(str(init_file))\n",
        "    return init_file\n",
        "\n",
        "# You can optionally run these lines to see the paths that will be used\n",
        "job_dir = get_shared_folder(shared_dir) / \"my_job\" # Example job directory\n",
        "print(f\"Job directory: {job_dir}\")\n",
        "init_file = get_init_file(shared_dir)\n",
        "print(f\"Init file: {init_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opZ2B51vax81",
        "outputId": "f8aea8ec-25ed-4eeb-8dc8-8d7c6079d1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job directory: /content/shared_experiments/default_user/experiments/my_job\n",
            "Init file: /content/shared_experiments/default_user/experiments/af6edabdc1e248e18abdf42794a7eaf6_init\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['USER'] = 'thesis_dfki'"
      ],
      "metadata": {
        "id": "H1_GpDqxniqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python submitit_train.py \\\n",
        "    --dataset=cifar10 \\\n",
        "    --batch_size=64 \\\n",
        "    --nodes=1 \\\n",
        "    --accum_iter=1 \\\n",
        "    --eval_frequency=100 \\\n",
        "    --epochs=3000 \\\n",
        "    --class_drop_prob=1.0 \\\n",
        "    --cfg_scale=0.0 \\\n",
        "    --compute_fid \\\n",
        "    --ode_method heun2 \\\n",
        "    --ode_options '{\"nfe\": 50}' \\\n",
        "    --use_ema \\\n",
        "    --edm_schedule \\\n",
        "    --skewed_timesteps \\\n",
        "    --output_dir output_cifar10_submitit \\\n",
        "    --shared_dir \"/content/shared_experiments\""
      ],
      "metadata": {
        "id": "oAubohbgUUl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09580062-4dc8-4fce-ca93-c5496cb74831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-21 18:40:36 INFO     Submitted job 5326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "args = {\n",
        "    \"output_dir\": Path(\"/tmp/some/path\"),\n",
        "    \"other_param\": 42,\n",
        "}\n",
        "\n",
        "with open(\"args.json\", \"w\") as f:\n",
        "    json.dump({k: str(v) if isinstance(v, Path) else v for k, v in args.items()}, f)\n"
      ],
      "metadata": {
        "id": "17jgxnM_qSPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}