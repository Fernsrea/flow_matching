{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fernsrea/flow_matching/blob/main/MNIST_Discreet_FM_model_Inpainting_CurriculumDeterministicMask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeFku-8ZtT9I",
        "outputId": "e64e1608-f545-4abf-dc0e-79c2e47d73c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jRnYLF-sE8H",
        "outputId": "a16f40c4-b98e-4b4d-d015-1e82400da24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'flow_matching'...\n",
            "remote: Enumerating objects: 496, done.\u001b[K\n",
            "remote: Counting objects: 100% (276/276), done.\u001b[K\n",
            "remote: Compressing objects: 100% (203/203), done.\u001b[K\n",
            "remote: Total 496 (delta 192), reused 73 (delta 73), pack-reused 220 (from 2)\u001b[K\n",
            "Receiving objects: 100% (496/496), 3.48 MiB | 6.71 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n",
            "/content/flow_matching\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Fernsrea/flow_matching.git\n",
        "%cd flow_matching"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd examples/image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1R7FXmqsYCR",
        "outputId": "4ef1988d-9c54-4a5a-e599-c7fad94ec087"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/flow_matching/examples/image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzyJxPEasbeA",
        "outputId": "b034446e-fa21-419f-d801-46df818bda2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting submitit (from -r requirements.txt (line 1))\n",
            "  Downloading submitit-1.5.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.21.0+cu124)\n",
            "Collecting torchdiffeq (from -r requirements.txt (line 4))\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting flow_matching (from -r requirements.txt (line 5))\n",
            "  Downloading flow_matching-1.0.10-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting torchmetrics[image] (from -r requirements.txt (line 2))\n",
            "  Downloading torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torch-fidelity<=0.4.0 (from torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: scipy>1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics[image]->-r requirements.txt (line 2)) (1.15.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 3)) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics[image]->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity<=0.4.0->torchmetrics[image]->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics[image]->-r requirements.txt (line 2)) (3.0.2)\n",
            "Downloading submitit-1.5.3-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Downloading flow_matching-1.0.10-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading torchmetrics-1.7.2-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: submitit, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, torchdiffeq, torch-fidelity, flow_matching\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flow_matching-1.0.10 lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 submitit-1.5.3 torch-fidelity-0.3.0 torchdiffeq-0.2.5 torchmetrics-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSbE2uw71gz3",
        "outputId": "57bd4a0f-5b7a-477d-a7d8-d8de48584abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.4.0->torchdiffeq) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.2)\n",
            "Downloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flow_matching"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4SivJfO2boL",
        "outputId": "8738271e-d971-4ef0-ac03-b345334e3984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flow_matching\n",
            "  Downloading flow_matching-1.0.10-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flow_matching) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flow_matching) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.11/dist-packages (from flow_matching) (0.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flow_matching) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq->flow_matching) (1.15.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flow_matching) (3.0.2)\n",
            "Downloading flow_matching-1.0.10-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flow_matching\n",
            "Successfully installed flow_matching-1.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we7T4aktbEUT",
        "outputId": "9330ddbf-ebe2-4807-8f2c-3fa99f363ad5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/masked_mnist/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XNYzHTTkTL3",
        "outputId": "c74922bd-b699-4459-8e8f-b42df3a99782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked_mnist_train.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create target directory\n",
        "import os\n",
        "target_dir = '/content/drive/MyDrive/masked_mnist'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "# Parameters\n",
        "mask_size = 10\n",
        "image_size = 28\n",
        "\n",
        "# Load MNIST\n",
        "transform = transforms.ToTensor()\n",
        "mnist = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "# Create center mask\n",
        "def create_center_mask(size, mask_size):\n",
        "    mask = np.ones((size, size), dtype=np.uint8)\n",
        "    start = (size - mask_size) // 2\n",
        "    mask[start:start + mask_size, start:start + mask_size] = 0\n",
        "    return mask\n",
        "\n",
        "mask = create_center_mask(image_size, mask_size)\n",
        "mask_tensor = torch.tensor(mask, dtype=torch.float32)  # (28, 28)\n",
        "\n",
        "# Apply mask and store\n",
        "images, masks, labels = [], [], []\n",
        "for img, label in mnist:\n",
        "    img = img.squeeze(0)  # (28, 28)\n",
        "    masked = img * mask_tensor\n",
        "    images.append(masked.unsqueeze(0))        # (1, 28, 28)\n",
        "    masks.append(mask_tensor.unsqueeze(0))    # (1, 28, 28)\n",
        "    labels.append(label)\n",
        "\n",
        "# Stack and save\n",
        "data = {\n",
        "    'images': torch.stack(images),   # (N, 1, 28, 28)\n",
        "    'masks': torch.stack(masks),     # (N, 1, 28, 28)\n",
        "    'labels': torch.tensor(labels)   # (N,)\n",
        "}\n",
        "torch.save(data, os.path.join(target_dir, 'masked_mnist_train.pt'))\n",
        "print(\"masked_mnist_train.pt saved to Google Drive with correct shape.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYM0DdtTBxHG",
        "outputId": "a75cf248-ebb3-4c53-82c3-176b5774989d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "masked_mnist_train.pt saved to Google Drive with correct shape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define target directory for local copies\n",
        "target_dir = '/content/flow_matching/examples/image/data/image_generation'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Define all dataset filenames\n",
        "mask_sizes = [6, 10, 14]\n",
        "datasets = {}\n",
        "\n",
        "for size in mask_sizes:\n",
        "    filename = f'masked_mnist_train_{size}.pt'\n",
        "    drive_path = f'/content/drive/MyDrive/masked_mnist/{filename}'\n",
        "    local_path = os.path.join(target_dir, filename)\n",
        "\n",
        "    # Copy from Google Drive to local target directory\n",
        "    shutil.copy(drive_path, local_path)\n",
        "\n",
        "    # Load the .pt file and store contents in a dictionary\n",
        "    data = torch.load(local_path)\n",
        "    datasets[size] = {\n",
        "        'images': data['images'],   # shape: (N, 1, 28, 28)\n",
        "        'masks': data['masks'],     # shape: (N, 1, 28, 28)\n",
        "        'labels': data['labels'],   # shape: (N,)\n",
        "    }"
      ],
      "metadata": {
        "id": "h9DY53bcp0bN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_samples(datasets, samples_per_size=3):\n",
        "    mask_sizes = sorted(datasets.keys())\n",
        "\n",
        "    fig, axs = plt.subplots(len(mask_sizes), samples_per_size * 2, figsize=(samples_per_size * 2, len(mask_sizes) * 2))\n",
        "    fig.suptitle(\"Masked MNIST Samples by Mask Size\", fontsize=16, y=1.02)\n",
        "\n",
        "    for row, size in enumerate(mask_sizes):\n",
        "        images = datasets[size]['images']\n",
        "        masks = datasets[size]['masks']\n",
        "\n",
        "        for col in range(samples_per_size):\n",
        "            idx = col  # or random index if preferred\n",
        "\n",
        "            mask = masks[idx][0].numpy()\n",
        "            masked = images[idx][0].numpy()\n",
        "\n",
        "            axs[row, col * 2 + 0].imshow(mask, cmap='gray')\n",
        "            axs[row, col * 2 + 0].set_title(\"Mask\")\n",
        "            axs[row, col * 2 + 0].axis('off')\n",
        "\n",
        "            axs[row, col * 2 + 1].imshow(masked, cmap='gray')\n",
        "            axs[row, col * 2 + 1].set_title(\"Masked\")\n",
        "            axs[row, col * 2 + 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the visualization function\n",
        "show_samples(datasets)"
      ],
      "metadata": {
        "id": "qcYBF_aI0Mvh",
        "outputId": "401de469-7764-4732-d57c-392723f957d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 18 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --dataset=mnist \\\n",
        "    --batch_size=32 \\\n",
        "    --accum_iter=1 \\\n",
        "    --eval_frequency=5 \\\n",
        "    --save_fid_samples \\\n",
        "    --num_workers=2 \\\n",
        "    --epochs=500 \\\n",
        "    --use_ema \\\n",
        "    --fid_samples=1000 \\\n",
        "    --discrete_flow_matching \\\n",
        "    --cfg_scale=0.0 \\\n",
        "    --data_path=\"./data/image_generation\" \\\n",
        "    --output_dir='/content/drive/MyDrive/flow_matching_checkpoints_curr_29M' \\\n",
        "    --compute_fid \\\n",
        "    --test_run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib_cNzfWkg9_",
        "outputId": "cfc15cd6-4f22-460c-c6e3-5a326e5b5077"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "2025-05-29 16:18:07 INFO     job dir: /content/flow_matching/examples/image\n",
            "2025-05-29 16:18:07 INFO     Namespace(batch_size=32,\n",
            "epochs=500,\n",
            "accum_iter=1,\n",
            "lr=0.0001,\n",
            "optimizer_betas=[0.9,\n",
            "0.95],\n",
            "decay_lr=False,\n",
            "class_drop_prob=0.2,\n",
            "skewed_timesteps=False,\n",
            "edm_schedule=False,\n",
            "use_ema=True,\n",
            "dataset='mnist',\n",
            "data_path='./data/image_generation',\n",
            "output_dir='/content/drive/MyDrive/flow_matching_checkpoints_curr_29M',\n",
            "ode_method='midpoint',\n",
            "ode_options={'step_size': 0.01},\n",
            "sym=0.0,\n",
            "temp=1.0,\n",
            "sym_func=False,\n",
            "sampling_dtype='float32',\n",
            "cfg_scale=0.0,\n",
            "fid_samples=1000,\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='',\n",
            "start_epoch=0,\n",
            "eval_only=False,\n",
            "eval_frequency=5,\n",
            "compute_fid=True,\n",
            "save_fid_samples=True,\n",
            "num_workers=2,\n",
            "pin_mem=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "test_run=True,\n",
            "discrete_flow_matching=True,\n",
            "discrete_fm_steps=1024,\n",
            "distributed=False)\n",
            "2025-05-29 16:18:07 INFO     Saving args to /content/drive/MyDrive/flow_matching_checkpoints_curr_29M/args.json\n",
            "2025-05-29 16:18:07 INFO     Initializing Dataset: mnist\n",
            "2025-05-29 16:18:08 INFO     Applied transform to masked image: ./data/image_generation/masked_mnist_train_6.pt\n",
            "2025-05-29 16:18:08 INFO     <__main__.MaskedMNISTDataset object at 0x7c236dd29650>\n",
            "2025-05-29 16:18:08 INFO     Intializing DataLoader\n",
            "2025-05-29 16:18:08 INFO     <torch.utils.data.distributed.DistributedSampler object at 0x7c236dccde10>\n",
            "100% 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n",
            "100% 28.9k/28.9k [00:00<00:00, 458kB/s]\n",
            "100% 1.65M/1.65M [00:00<00:00, 4.48MB/s]\n",
            "100% 4.54k/4.54k [00:00<00:00, 9.48MB/s]\n",
            "2025-05-29 16:18:13 INFO     Initializing Model\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/flow_matching/examples/image/train.py\", line 318, in <module>\n",
            "    main(args)\n",
            "  File \"/content/flow_matching/examples/image/train.py\", line 174, in main\n",
            "    model.to(device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1343, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
            "    return t.to(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\", line 319, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --dataset=mnist \\\n",
        "    --batch_size=32 \\\n",
        "    --accum_iter=1 \\\n",
        "    --eval_frequency=5 \\\n",
        "    --save_fid_samples \\\n",
        "    --num_workers=2 \\\n",
        "    --epochs=500 \\\n",
        "    --use_ema \\\n",
        "    --fid_samples=1000 \\\n",
        "    --discrete_flow_matching \\\n",
        "    --cfg_scale=0.0 \\\n",
        "    --data_path=\"./data/image_generation\" \\\n",
        "    --output_dir='/content/drive/MyDrive/flow_matching_checkpoints_curr_29M' \\\n",
        "    --compute_fid --resume='/content/drive/MyDrive/flow_matching_checkpoints_curr_29M/checkpoint.pth'"
      ],
      "metadata": {
        "outputId": "93b2de45-8280-4d0c-d3ce-4ec4fe1684b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikyJG2ip1p8j"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using distributed mode\n",
            "2025-05-28 22:56:07 INFO     job dir: /content/flow_matching/examples/image\n",
            "2025-05-28 22:56:07 INFO     Namespace(batch_size=32,\n",
            "epochs=500,\n",
            "accum_iter=1,\n",
            "lr=0.0001,\n",
            "optimizer_betas=[0.9,\n",
            "0.95],\n",
            "decay_lr=False,\n",
            "class_drop_prob=0.2,\n",
            "skewed_timesteps=False,\n",
            "edm_schedule=False,\n",
            "use_ema=True,\n",
            "dataset='mnist',\n",
            "data_path='./data/image_generation',\n",
            "output_dir='/content/drive/MyDrive/flow_matching_checkpoints_29M',\n",
            "ode_method='midpoint',\n",
            "ode_options={'step_size': 0.01},\n",
            "sym=0.0,\n",
            "temp=1.0,\n",
            "sym_func=False,\n",
            "sampling_dtype='float32',\n",
            "cfg_scale=0.0,\n",
            "fid_samples=1000,\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='/content/drive/MyDrive/flow_matching_checkpoints_29M/checkpoint.pth',\n",
            "start_epoch=0,\n",
            "eval_only=False,\n",
            "eval_frequency=5,\n",
            "compute_fid=True,\n",
            "save_fid_samples=True,\n",
            "num_workers=2,\n",
            "pin_mem=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "test_run=False,\n",
            "discrete_flow_matching=True,\n",
            "discrete_fm_steps=1024,\n",
            "distributed=False)\n",
            "2025-05-28 22:56:07 INFO     Saving args to /content/drive/MyDrive/flow_matching_checkpoints_29M/args.json\n",
            "2025-05-28 22:56:07 INFO     Initializing Dataset: mnist\n",
            "2025-05-28 22:56:07 INFO     Applied transform to masked image\n",
            "2025-05-28 22:56:07 INFO     <__main__.MaskedMNISTDataset object at 0x7ca1706e7d10>\n",
            "2025-05-28 22:56:07 INFO     Intializing DataLoader\n",
            "2025-05-28 22:56:07 INFO     <torch.utils.data.distributed.DistributedSampler object at 0x7ca17062f210>\n",
            "2025-05-28 22:56:07 INFO     Initializing Model\n",
            "2025-05-28 22:56:08 INFO     EMA(\n",
            "  (model): DiscreteUNetModel(vocab_size=257, in_channels=3, model_channels=96, out_channels=3, num_res_blocks=2, attention_resolutions=[], dropout=0.1, channel_mult=[1, 2, 2], conv_resample=False, dims=2, num_classes=None, use_checkpoint=False, num_heads=-1, num_head_channels=16, num_heads_upsample=-1, use_scale_shift_norm=True, resblock_updown=False, use_new_attention_order=True, with_fourier_features=False)\n",
            "  (shadow_params): ParameterList(\n",
            "      (0): Parameter containing: [torch.float32 of size 257x32 (cuda:0)]\n",
            "      (1): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (2): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (3): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (4): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (5): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (6): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (7): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (8): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (9): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (10): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (11): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (12): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (13): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (14): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (15): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (16): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (17): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (18): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (19): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (20): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (21): Parameter containing: [torch.float32 of size 192x96x3x3 (cuda:0)]\n",
            "      (22): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (23): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (24): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (25): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (26): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (27): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (28): Parameter containing: [torch.float32 of size 192x96x1x1 (cuda:0)]\n",
            "      (29): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (30): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (31): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (32): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (33): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (34): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (35): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (36): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (37): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (38): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (39): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (40): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (41): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (42): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (43): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (44): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (45): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (46): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (47): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (48): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (49): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (50): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (51): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (52): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (53): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (54): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (55): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (56): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (57): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (58): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (59): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (60): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (61): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (62): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (63): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (64): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (65): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (66): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (67): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (68): Parameter containing: [torch.float32 of size 576x192x1 (cuda:0)]\n",
            "      (69): Parameter containing: [torch.float32 of size 576 (cuda:0)]\n",
            "      (70): Parameter containing: [torch.float32 of size 192x192x1 (cuda:0)]\n",
            "      (71): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (72): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (73): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (74): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (75): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (76): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (77): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (78): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (79): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (80): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (81): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (82): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (83): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (84): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (85): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (86): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (87): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (88): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (89): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (90): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (91): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (92): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (93): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (94): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (95): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (96): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (97): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (98): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (99): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (100): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (101): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (102): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (103): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (104): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (105): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (106): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (107): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (108): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (109): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (110): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (111): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (112): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (113): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (114): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (115): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (116): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (117): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (118): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (119): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (120): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (121): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (122): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (123): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (124): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (125): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (126): Parameter containing: [torch.float32 of size 384 (cuda:0)]\n",
            "      (127): Parameter containing: [torch.float32 of size 192x384x3x3 (cuda:0)]\n",
            "      (128): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (129): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (130): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (131): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (132): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (133): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (134): Parameter containing: [torch.float32 of size 192x384x1x1 (cuda:0)]\n",
            "      (135): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (136): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (137): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (138): Parameter containing: [torch.float32 of size 192x288x3x3 (cuda:0)]\n",
            "      (139): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (140): Parameter containing: [torch.float32 of size 1x384 (cuda:0)]\n",
            "      (141): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (142): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (143): Parameter containing: [torch.float32 of size 192x192x3x3 (cuda:0)]\n",
            "      (144): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (145): Parameter containing: [torch.float32 of size 192x288x1x1 (cuda:0)]\n",
            "      (146): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (147): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (148): Parameter containing: [torch.float32 of size 288 (cuda:0)]\n",
            "      (149): Parameter containing: [torch.float32 of size 96x288x3x3 (cuda:0)]\n",
            "      (150): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (151): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (152): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (153): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (154): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (155): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (156): Parameter containing: [torch.float32 of size 96x288x1x1 (cuda:0)]\n",
            "      (157): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (158): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (159): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (160): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (161): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (162): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (163): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (164): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (165): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (166): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (167): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (168): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (169): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (170): Parameter containing: [torch.float32 of size 192 (cuda:0)]\n",
            "      (171): Parameter containing: [torch.float32 of size 96x192x3x3 (cuda:0)]\n",
            "      (172): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (173): Parameter containing: [torch.float32 of size 1x192 (cuda:0)]\n",
            "      (174): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (175): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (176): Parameter containing: [torch.float32 of size 96x96x3x3 (cuda:0)]\n",
            "      (177): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (178): Parameter containing: [torch.float32 of size 96x192x1x1 (cuda:0)]\n",
            "      (179): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (180): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (181): Parameter containing: [torch.float32 of size 96 (cuda:0)]\n",
            "      (182): Parameter containing: [torch.float32 of size 771x96x3x3 (cuda:0)]\n",
            "      (183): Parameter containing: [torch.float32 of size 771 (cuda:0)]\n",
            "  )\n",
            ")\n",
            "2025-05-28 22:56:08 INFO     Learning rate: 1.00e-04\n",
            "2025-05-28 22:56:08 INFO     Accumulate grad iterations: 1\n",
            "2025-05-28 22:56:08 INFO     Effective batch size: 32\n",
            "2025-05-28 22:56:08 INFO     Optimizer: AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.95]\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "2025-05-28 22:56:08 INFO     Learning-Rate Schedule: <torch.optim.lr_scheduler.ConstantLR object at 0x7ca16ba23410>\n",
            "/content/flow_matching/examples/image/training/grad_scaler.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "Resume checkpoint /content/drive/MyDrive/flow_matching_checkpoints_29M/checkpoint.pth\n",
            "With optim & sched!\n",
            "2025-05-28 22:56:08 INFO     Start from 1 to 500 epochs\n",
            "2025-05-28 22:56:11 INFO     Epoch 1 [0/1875]: loss = 5.495797634124756, lr = 0.0001\n",
            "2025-05-28 22:56:18 INFO     Epoch 1 [50/1875]: loss = 1.8657158613204956, lr = 0.0001\n",
            "2025-05-28 22:56:24 INFO     Epoch 1 [100/1875]: loss = 0.5508366823196411, lr = 0.0001\n",
            "2025-05-28 22:56:31 INFO     Epoch 1 [150/1875]: loss = 0.4266854226589203, lr = 0.0001\n",
            "2025-05-28 22:56:38 INFO     Epoch 1 [200/1875]: loss = 0.3380511999130249, lr = 0.0001\n",
            "2025-05-28 22:56:44 INFO     Epoch 1 [250/1875]: loss = 0.37016892433166504, lr = 0.0001\n",
            "2025-05-28 22:56:51 INFO     Epoch 1 [300/1875]: loss = 0.4008573889732361, lr = 0.0001\n",
            "2025-05-28 22:56:57 INFO     Epoch 1 [350/1875]: loss = 0.3196432590484619, lr = 0.0001\n",
            "2025-05-28 22:57:04 INFO     Epoch 1 [400/1875]: loss = 0.3242473900318146, lr = 0.0001\n",
            "2025-05-28 22:57:11 INFO     Epoch 1 [450/1875]: loss = 0.34642601013183594, lr = 0.0001\n",
            "2025-05-28 22:57:17 INFO     Epoch 1 [500/1875]: loss = 0.33755165338516235, lr = 0.0001\n",
            "2025-05-28 22:57:24 INFO     Epoch 1 [550/1875]: loss = 0.2943335473537445, lr = 0.0001\n",
            "2025-05-28 22:57:31 INFO     Epoch 1 [600/1875]: loss = 0.3257301151752472, lr = 0.0001\n",
            "2025-05-28 22:57:38 INFO     Epoch 1 [650/1875]: loss = 0.35383275151252747, lr = 0.0001\n",
            "2025-05-28 22:57:44 INFO     Epoch 1 [700/1875]: loss = 0.34746789932250977, lr = 0.0001\n",
            "2025-05-28 22:57:51 INFO     Epoch 1 [750/1875]: loss = 0.2965478301048279, lr = 0.0001\n",
            "2025-05-28 22:57:58 INFO     Epoch 1 [800/1875]: loss = 0.3274267315864563, lr = 0.0001\n",
            "2025-05-28 22:58:05 INFO     Epoch 1 [850/1875]: loss = 0.3312246799468994, lr = 0.0001\n",
            "2025-05-28 22:58:11 INFO     Epoch 1 [900/1875]: loss = 0.3074815273284912, lr = 0.0001\n",
            "2025-05-28 22:58:18 INFO     Epoch 1 [950/1875]: loss = 0.3615601658821106, lr = 0.0001\n",
            "2025-05-28 22:58:25 INFO     Epoch 1 [1000/1875]: loss = 0.3177466094493866, lr = 0.0001\n",
            "2025-05-28 22:58:32 INFO     Epoch 1 [1050/1875]: loss = 0.3369000256061554, lr = 0.0001\n",
            "2025-05-28 22:58:39 INFO     Epoch 1 [1100/1875]: loss = 0.3204338550567627, lr = 0.0001\n",
            "2025-05-28 22:58:45 INFO     Epoch 1 [1150/1875]: loss = 0.299235999584198, lr = 0.0001\n",
            "2025-05-28 22:58:52 INFO     Epoch 1 [1200/1875]: loss = 0.34030500054359436, lr = 0.0001\n",
            "2025-05-28 22:58:59 INFO     Epoch 1 [1250/1875]: loss = 0.29008832573890686, lr = 0.0001\n",
            "2025-05-28 22:59:06 INFO     Epoch 1 [1300/1875]: loss = 0.2877475619316101, lr = 0.0001\n",
            "2025-05-28 22:59:13 INFO     Epoch 1 [1350/1875]: loss = 0.26427513360977173, lr = 0.0001\n",
            "2025-05-28 22:59:20 INFO     Epoch 1 [1400/1875]: loss = 0.27593696117401123, lr = 0.0001\n",
            "2025-05-28 22:59:26 INFO     Epoch 1 [1450/1875]: loss = 0.31922149658203125, lr = 0.0001\n",
            "2025-05-28 22:59:33 INFO     Epoch 1 [1500/1875]: loss = 0.2581949532032013, lr = 0.0001\n",
            "2025-05-28 22:59:40 INFO     Epoch 1 [1550/1875]: loss = 0.28607508540153503, lr = 0.0001\n",
            "2025-05-28 22:59:47 INFO     Epoch 1 [1600/1875]: loss = 0.26845085620880127, lr = 0.0001\n",
            "2025-05-28 22:59:54 INFO     Epoch 1 [1650/1875]: loss = 0.2977391183376312, lr = 0.0001\n",
            "2025-05-28 23:00:01 INFO     Epoch 1 [1700/1875]: loss = 0.3183140158653259, lr = 0.0001\n",
            "2025-05-28 23:00:07 INFO     Epoch 1 [1750/1875]: loss = 0.3287712633609772, lr = 0.0001\n",
            "2025-05-28 23:00:15 INFO     Epoch 1 [1800/1875]: loss = 0.21227379143238068, lr = 0.0001\n",
            "2025-05-28 23:00:22 INFO     Epoch 1 [1850/1875]: loss = 0.28008195757865906, lr = 0.0001\n",
            "2025-05-28 23:00:25 INFO     Epoch 2 [0/1875]: loss = 0.25077271461486816, lr = 0.0001\n",
            "2025-05-28 23:00:32 INFO     Epoch 2 [50/1875]: loss = 0.27830761671066284, lr = 0.0001\n",
            "2025-05-28 23:00:39 INFO     Epoch 2 [100/1875]: loss = 0.28013086318969727, lr = 0.0001\n",
            "2025-05-28 23:00:46 INFO     Epoch 2 [150/1875]: loss = 0.2503674626350403, lr = 0.0001\n",
            "2025-05-28 23:00:53 INFO     Epoch 2 [200/1875]: loss = 0.2717350423336029, lr = 0.0001\n",
            "2025-05-28 23:01:00 INFO     Epoch 2 [250/1875]: loss = 0.3077413737773895, lr = 0.0001\n",
            "2025-05-28 23:01:07 INFO     Epoch 2 [300/1875]: loss = 0.3021695017814636, lr = 0.0001\n",
            "2025-05-28 23:01:13 INFO     Epoch 2 [350/1875]: loss = 0.15635590255260468, lr = 0.0001\n",
            "2025-05-28 23:01:20 INFO     Epoch 2 [400/1875]: loss = 0.252714067697525, lr = 0.0001\n",
            "2025-05-28 23:01:27 INFO     Epoch 2 [450/1875]: loss = 0.23599012196063995, lr = 0.0001\n",
            "2025-05-28 23:01:34 INFO     Epoch 2 [500/1875]: loss = 0.29725581407546997, lr = 0.0001\n",
            "2025-05-28 23:01:41 INFO     Epoch 2 [550/1875]: loss = 0.23603036999702454, lr = 0.0001\n",
            "2025-05-28 23:01:48 INFO     Epoch 2 [600/1875]: loss = 0.22881481051445007, lr = 0.0001\n",
            "2025-05-28 23:01:55 INFO     Epoch 2 [650/1875]: loss = 0.24216775596141815, lr = 0.0001\n",
            "2025-05-28 23:02:02 INFO     Epoch 2 [700/1875]: loss = 0.22838445007801056, lr = 0.0001\n",
            "2025-05-28 23:02:08 INFO     Epoch 2 [750/1875]: loss = 0.20623792707920074, lr = 0.0001\n",
            "2025-05-28 23:02:15 INFO     Epoch 2 [800/1875]: loss = 0.22241239249706268, lr = 0.0001\n",
            "2025-05-28 23:02:22 INFO     Epoch 2 [850/1875]: loss = 0.265638142824173, lr = 0.0001\n",
            "2025-05-28 23:02:29 INFO     Epoch 2 [900/1875]: loss = 0.21038690209388733, lr = 0.0001\n",
            "2025-05-28 23:02:36 INFO     Epoch 2 [950/1875]: loss = 0.2535448372364044, lr = 0.0001\n",
            "2025-05-28 23:02:43 INFO     Epoch 2 [1000/1875]: loss = 0.23036116361618042, lr = 0.0001\n",
            "2025-05-28 23:02:50 INFO     Epoch 2 [1050/1875]: loss = 0.2173936367034912, lr = 0.0001\n",
            "2025-05-28 23:02:57 INFO     Epoch 2 [1100/1875]: loss = 0.2428886741399765, lr = 0.0001\n",
            "2025-05-28 23:03:04 INFO     Epoch 2 [1150/1875]: loss = 0.20972922444343567, lr = 0.0001\n",
            "2025-05-28 23:03:11 INFO     Epoch 2 [1200/1875]: loss = 0.2544845640659332, lr = 0.0001\n",
            "2025-05-28 23:03:18 INFO     Epoch 2 [1250/1875]: loss = 0.17912538349628448, lr = 0.0001\n",
            "2025-05-28 23:03:24 INFO     Epoch 2 [1300/1875]: loss = 0.2370583415031433, lr = 0.0001\n",
            "2025-05-28 23:03:31 INFO     Epoch 2 [1350/1875]: loss = 0.22886209189891815, lr = 0.0001\n",
            "2025-05-28 23:03:38 INFO     Epoch 2 [1400/1875]: loss = 0.1984703093767166, lr = 0.0001\n",
            "2025-05-28 23:03:45 INFO     Epoch 2 [1450/1875]: loss = 0.25806716084480286, lr = 0.0001\n",
            "2025-05-28 23:03:52 INFO     Epoch 2 [1500/1875]: loss = 0.20310817658901215, lr = 0.0001\n",
            "2025-05-28 23:03:59 INFO     Epoch 2 [1550/1875]: loss = 0.23328685760498047, lr = 0.0001\n",
            "2025-05-28 23:04:06 INFO     Epoch 2 [1600/1875]: loss = 0.2524271309375763, lr = 0.0001\n",
            "2025-05-28 23:04:13 INFO     Epoch 2 [1650/1875]: loss = 0.279944509267807, lr = 0.0001\n",
            "2025-05-28 23:04:19 INFO     Epoch 2 [1700/1875]: loss = 0.2548080384731293, lr = 0.0001\n",
            "2025-05-28 23:04:26 INFO     Epoch 2 [1750/1875]: loss = 0.21026615798473358, lr = 0.0001\n",
            "2025-05-28 23:04:33 INFO     Epoch 2 [1800/1875]: loss = 0.19228407740592957, lr = 0.0001\n",
            "2025-05-28 23:04:40 INFO     Epoch 2 [1850/1875]: loss = 0.25356006622314453, lr = 0.0001\n",
            "2025-05-28 23:04:44 INFO     Epoch 3 [0/1875]: loss = 0.16673189401626587, lr = 0.0001\n",
            "2025-05-28 23:04:51 INFO     Epoch 3 [50/1875]: loss = 0.150888592004776, lr = 0.0001\n",
            "2025-05-28 23:04:58 INFO     Epoch 3 [100/1875]: loss = 0.2214074730873108, lr = 0.0001\n",
            "2025-05-28 23:05:04 INFO     Epoch 3 [150/1875]: loss = 0.25582244992256165, lr = 0.0001\n",
            "2025-05-28 23:05:11 INFO     Epoch 3 [200/1875]: loss = 0.25560280680656433, lr = 0.0001\n",
            "2025-05-28 23:05:18 INFO     Epoch 3 [250/1875]: loss = 0.25084584951400757, lr = 0.0001\n",
            "2025-05-28 23:05:25 INFO     Epoch 3 [300/1875]: loss = 0.185169979929924, lr = 0.0001\n",
            "2025-05-28 23:05:32 INFO     Epoch 3 [350/1875]: loss = 0.15940500795841217, lr = 0.0001\n",
            "2025-05-28 23:05:39 INFO     Epoch 3 [400/1875]: loss = 0.21572278439998627, lr = 0.0001\n",
            "2025-05-28 23:05:46 INFO     Epoch 3 [450/1875]: loss = 0.20163187384605408, lr = 0.0001\n",
            "2025-05-28 23:05:53 INFO     Epoch 3 [500/1875]: loss = 0.21281737089157104, lr = 0.0001\n",
            "2025-05-28 23:06:00 INFO     Epoch 3 [550/1875]: loss = 0.1939060240983963, lr = 0.0001\n",
            "2025-05-28 23:06:07 INFO     Epoch 3 [600/1875]: loss = 0.1595262736082077, lr = 0.0001\n",
            "2025-05-28 23:06:14 INFO     Epoch 3 [650/1875]: loss = 0.1959400624036789, lr = 0.0001\n",
            "2025-05-28 23:06:20 INFO     Epoch 3 [700/1875]: loss = 0.15864907205104828, lr = 0.0001\n",
            "2025-05-28 23:06:27 INFO     Epoch 3 [750/1875]: loss = 0.19832992553710938, lr = 0.0001\n",
            "2025-05-28 23:06:34 INFO     Epoch 3 [800/1875]: loss = 0.21563532948493958, lr = 0.0001\n",
            "2025-05-28 23:06:41 INFO     Epoch 3 [850/1875]: loss = 0.22722218930721283, lr = 0.0001\n",
            "2025-05-28 23:06:48 INFO     Epoch 3 [900/1875]: loss = 0.1891552209854126, lr = 0.0001\n",
            "2025-05-28 23:06:55 INFO     Epoch 3 [950/1875]: loss = 0.21994465589523315, lr = 0.0001\n",
            "2025-05-28 23:07:02 INFO     Epoch 3 [1000/1875]: loss = 0.2291382998228073, lr = 0.0001\n",
            "2025-05-28 23:07:09 INFO     Epoch 3 [1050/1875]: loss = 0.21028706431388855, lr = 0.0001\n",
            "2025-05-28 23:07:16 INFO     Epoch 3 [1100/1875]: loss = 0.2255229353904724, lr = 0.0001\n",
            "2025-05-28 23:07:22 INFO     Epoch 3 [1150/1875]: loss = 0.22882528603076935, lr = 0.0001\n",
            "2025-05-28 23:07:29 INFO     Epoch 3 [1200/1875]: loss = 0.2042345404624939, lr = 0.0001\n",
            "2025-05-28 23:07:36 INFO     Epoch 3 [1250/1875]: loss = 0.18415763974189758, lr = 0.0001\n",
            "2025-05-28 23:07:43 INFO     Epoch 3 [1300/1875]: loss = 0.16748636960983276, lr = 0.0001\n",
            "2025-05-28 23:07:50 INFO     Epoch 3 [1350/1875]: loss = 0.30891844630241394, lr = 0.0001\n",
            "2025-05-28 23:07:57 INFO     Epoch 3 [1400/1875]: loss = 0.20403718948364258, lr = 0.0001\n",
            "2025-05-28 23:08:04 INFO     Epoch 3 [1450/1875]: loss = 0.2400287389755249, lr = 0.0001\n",
            "2025-05-28 23:08:11 INFO     Epoch 3 [1500/1875]: loss = 0.1931251585483551, lr = 0.0001\n",
            "2025-05-28 23:08:17 INFO     Epoch 3 [1550/1875]: loss = 0.1615114063024521, lr = 0.0001\n",
            "2025-05-28 23:08:24 INFO     Epoch 3 [1600/1875]: loss = 0.2320639193058014, lr = 0.0001\n",
            "2025-05-28 23:08:31 INFO     Epoch 3 [1650/1875]: loss = 0.2447848916053772, lr = 0.0001\n",
            "2025-05-28 23:08:38 INFO     Epoch 3 [1700/1875]: loss = 0.19747820496559143, lr = 0.0001\n",
            "2025-05-28 23:08:45 INFO     Epoch 3 [1750/1875]: loss = 0.22219239175319672, lr = 0.0001\n",
            "2025-05-28 23:08:52 INFO     Epoch 3 [1800/1875]: loss = 0.1645798236131668, lr = 0.0001\n",
            "2025-05-28 23:08:59 INFO     Epoch 3 [1850/1875]: loss = 0.17147214710712433, lr = 0.0001\n",
            "2025-05-28 23:09:03 INFO     Epoch 4 [0/1875]: loss = 0.18691977858543396, lr = 0.0001\n",
            "2025-05-28 23:09:10 INFO     Epoch 4 [50/1875]: loss = 0.21688787639141083, lr = 0.0001\n",
            "2025-05-28 23:09:16 INFO     Epoch 4 [100/1875]: loss = 0.2700200378894806, lr = 0.0001\n",
            "2025-05-28 23:09:23 INFO     Epoch 4 [150/1875]: loss = 0.21713565289974213, lr = 0.0001\n",
            "2025-05-28 23:09:30 INFO     Epoch 4 [200/1875]: loss = 0.18424852192401886, lr = 0.0001\n",
            "2025-05-28 23:09:37 INFO     Epoch 4 [250/1875]: loss = 0.19241328537464142, lr = 0.0001\n",
            "2025-05-28 23:09:44 INFO     Epoch 4 [300/1875]: loss = 0.2568046450614929, lr = 0.0001\n",
            "2025-05-28 23:09:51 INFO     Epoch 4 [350/1875]: loss = 0.15458975732326508, lr = 0.0001\n",
            "2025-05-28 23:09:58 INFO     Epoch 4 [400/1875]: loss = 0.16578349471092224, lr = 0.0001\n",
            "2025-05-28 23:10:05 INFO     Epoch 4 [450/1875]: loss = 0.207939013838768, lr = 0.0001\n",
            "2025-05-28 23:10:12 INFO     Epoch 4 [500/1875]: loss = 0.21101854741573334, lr = 0.0001\n",
            "2025-05-28 23:10:18 INFO     Epoch 4 [550/1875]: loss = 0.20706166326999664, lr = 0.0001\n",
            "2025-05-28 23:10:25 INFO     Epoch 4 [600/1875]: loss = 0.23706156015396118, lr = 0.0001\n",
            "2025-05-28 23:10:32 INFO     Epoch 4 [650/1875]: loss = 0.15502138435840607, lr = 0.0001\n",
            "2025-05-28 23:10:39 INFO     Epoch 4 [700/1875]: loss = 0.1979510486125946, lr = 0.0001\n",
            "2025-05-28 23:10:46 INFO     Epoch 4 [750/1875]: loss = 0.24888908863067627, lr = 0.0001\n",
            "2025-05-28 23:10:53 INFO     Epoch 4 [800/1875]: loss = 0.19533079862594604, lr = 0.0001\n",
            "2025-05-28 23:10:59 INFO     Epoch 4 [850/1875]: loss = 0.18614909052848816, lr = 0.0001\n",
            "2025-05-28 23:11:06 INFO     Epoch 4 [900/1875]: loss = 0.1579579859972, lr = 0.0001\n",
            "2025-05-28 23:11:13 INFO     Epoch 4 [950/1875]: loss = 0.20248505473136902, lr = 0.0001\n",
            "2025-05-28 23:11:20 INFO     Epoch 4 [1000/1875]: loss = 0.17199403047561646, lr = 0.0001\n",
            "2025-05-28 23:11:27 INFO     Epoch 4 [1050/1875]: loss = 0.19095070660114288, lr = 0.0001\n",
            "2025-05-28 23:11:34 INFO     Epoch 4 [1100/1875]: loss = 0.18555070459842682, lr = 0.0001\n",
            "2025-05-28 23:11:41 INFO     Epoch 4 [1150/1875]: loss = 0.17616286873817444, lr = 0.0001\n",
            "2025-05-28 23:11:48 INFO     Epoch 4 [1200/1875]: loss = 0.24144510924816132, lr = 0.0001\n",
            "2025-05-28 23:11:55 INFO     Epoch 4 [1250/1875]: loss = 0.14948520064353943, lr = 0.0001\n",
            "2025-05-28 23:12:02 INFO     Epoch 4 [1300/1875]: loss = 0.19062083959579468, lr = 0.0001\n",
            "2025-05-28 23:12:08 INFO     Epoch 4 [1350/1875]: loss = 0.2539803683757782, lr = 0.0001\n",
            "2025-05-28 23:12:15 INFO     Epoch 4 [1400/1875]: loss = 0.22581404447555542, lr = 0.0001\n",
            "2025-05-28 23:12:22 INFO     Epoch 4 [1450/1875]: loss = 0.1763375997543335, lr = 0.0001\n",
            "2025-05-28 23:12:29 INFO     Epoch 4 [1500/1875]: loss = 0.18117739260196686, lr = 0.0001\n",
            "2025-05-28 23:12:36 INFO     Epoch 4 [1550/1875]: loss = 0.24177324771881104, lr = 0.0001\n",
            "2025-05-28 23:12:43 INFO     Epoch 4 [1600/1875]: loss = 0.14920532703399658, lr = 0.0001\n",
            "2025-05-28 23:12:50 INFO     Epoch 4 [1650/1875]: loss = 0.2672554850578308, lr = 0.0001\n",
            "2025-05-28 23:12:56 INFO     Epoch 4 [1700/1875]: loss = 0.25303176045417786, lr = 0.0001\n",
            "2025-05-28 23:13:03 INFO     Epoch 4 [1750/1875]: loss = 0.2138081043958664, lr = 0.0001\n",
            "2025-05-28 23:13:10 INFO     Epoch 4 [1800/1875]: loss = 0.185430109500885, lr = 0.0001\n",
            "2025-05-28 23:13:17 INFO     Epoch 4 [1850/1875]: loss = 0.22979611158370972, lr = 0.0001\n",
            "2025-05-28 23:13:25 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "/content/flow_matching/examples/image/training/eval_loop.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(), torch.no_grad():\n",
            "2025-05-28 23:14:10 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:14:15 INFO     Evaluating [0/313] samples generated [32/1000] running fid 231.21194458007812\n",
            "2025-05-28 23:14:55 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:15:35 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:16:16 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:16:56 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:17:36 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:18:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:18:57 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:19:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:20:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:20:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:21:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:22:19 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:22:59 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:23:39 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:24:19 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:25:00 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:25:40 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:26:20 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:27:00 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:27:41 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:28:21 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:29:02 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:29:42 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:30:22 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:31:03 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:31:43 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:32:23 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:33:03 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:33:43 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:34:23 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:35:03 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:35:10 INFO     Evaluating [50/313] samples generated [1000/1000] running fid 184.3208465576172\n",
            "2025-05-28 23:35:21 INFO     Evaluating [100/313] samples generated [1000/1000] running fid 183.8041229248047\n",
            "2025-05-28 23:35:31 INFO     Evaluating [150/313] samples generated [1000/1000] running fid 183.43228149414062\n",
            "2025-05-28 23:35:42 INFO     Evaluating [200/313] samples generated [1000/1000] running fid 183.47775268554688\n",
            "2025-05-28 23:35:53 INFO     Evaluating [250/313] samples generated [1000/1000] running fid 183.9716033935547\n",
            "2025-05-28 23:36:03 INFO     Evaluating [300/313] samples generated [1000/1000] running fid 184.06797790527344\n",
            "2025-05-28 23:36:11 INFO     EMA: Switching from eval to train, restoring saved parameters\n",
            "2025-05-28 23:36:11 INFO     Epoch 5 [0/1875]: loss = 0.17379093170166016, lr = 0.0001\n",
            "2025-05-28 23:36:18 INFO     Epoch 5 [50/1875]: loss = 0.21653532981872559, lr = 0.0001\n",
            "2025-05-28 23:36:25 INFO     Epoch 5 [100/1875]: loss = 0.17937253415584564, lr = 0.0001\n",
            "2025-05-28 23:36:31 INFO     Epoch 5 [150/1875]: loss = 0.24896305799484253, lr = 0.0001\n",
            "2025-05-28 23:36:38 INFO     Epoch 5 [200/1875]: loss = 0.16472162306308746, lr = 0.0001\n",
            "2025-05-28 23:36:45 INFO     Epoch 5 [250/1875]: loss = 0.22719275951385498, lr = 0.0001\n",
            "2025-05-28 23:36:52 INFO     Epoch 5 [300/1875]: loss = 0.246846541762352, lr = 0.0001\n",
            "2025-05-28 23:36:59 INFO     Epoch 5 [350/1875]: loss = 0.17856164276599884, lr = 0.0001\n",
            "2025-05-28 23:37:06 INFO     Epoch 5 [400/1875]: loss = 0.19406290352344513, lr = 0.0001\n",
            "2025-05-28 23:37:12 INFO     Epoch 5 [450/1875]: loss = 0.20673006772994995, lr = 0.0001\n",
            "2025-05-28 23:37:19 INFO     Epoch 5 [500/1875]: loss = 0.2668211758136749, lr = 0.0001\n",
            "2025-05-28 23:37:26 INFO     Epoch 5 [550/1875]: loss = 0.20689643919467926, lr = 0.0001\n",
            "2025-05-28 23:37:33 INFO     Epoch 5 [600/1875]: loss = 0.20749031007289886, lr = 0.0001\n",
            "2025-05-28 23:37:40 INFO     Epoch 5 [650/1875]: loss = 0.23689015209674835, lr = 0.0001\n",
            "2025-05-28 23:37:47 INFO     Epoch 5 [700/1875]: loss = 0.22653242945671082, lr = 0.0001\n",
            "2025-05-28 23:37:53 INFO     Epoch 5 [750/1875]: loss = 0.24719172716140747, lr = 0.0001\n",
            "2025-05-28 23:38:00 INFO     Epoch 5 [800/1875]: loss = 0.27703362703323364, lr = 0.0001\n",
            "2025-05-28 23:38:07 INFO     Epoch 5 [850/1875]: loss = 0.23157213628292084, lr = 0.0001\n",
            "2025-05-28 23:38:14 INFO     Epoch 5 [900/1875]: loss = 0.19818881154060364, lr = 0.0001\n",
            "2025-05-28 23:38:21 INFO     Epoch 5 [950/1875]: loss = 0.17863179743289948, lr = 0.0001\n",
            "2025-05-28 23:38:28 INFO     Epoch 5 [1000/1875]: loss = 0.29483166337013245, lr = 0.0001\n",
            "2025-05-28 23:38:35 INFO     Epoch 5 [1050/1875]: loss = 0.17820683121681213, lr = 0.0001\n",
            "2025-05-28 23:38:42 INFO     Epoch 5 [1100/1875]: loss = 0.2330387383699417, lr = 0.0001\n",
            "2025-05-28 23:38:49 INFO     Epoch 5 [1150/1875]: loss = 0.20617838203907013, lr = 0.0001\n",
            "2025-05-28 23:38:55 INFO     Epoch 5 [1200/1875]: loss = 0.2149946391582489, lr = 0.0001\n",
            "2025-05-28 23:39:02 INFO     Epoch 5 [1250/1875]: loss = 0.25453731417655945, lr = 0.0001\n",
            "2025-05-28 23:39:09 INFO     Epoch 5 [1300/1875]: loss = 0.1956346333026886, lr = 0.0001\n",
            "2025-05-28 23:39:16 INFO     Epoch 5 [1350/1875]: loss = 0.23780986666679382, lr = 0.0001\n",
            "2025-05-28 23:39:23 INFO     Epoch 5 [1400/1875]: loss = 0.17721985280513763, lr = 0.0001\n",
            "2025-05-28 23:39:30 INFO     Epoch 5 [1450/1875]: loss = 0.2953609824180603, lr = 0.0001\n",
            "2025-05-28 23:39:36 INFO     Epoch 5 [1500/1875]: loss = 0.2104768306016922, lr = 0.0001\n",
            "2025-05-28 23:39:43 INFO     Epoch 5 [1550/1875]: loss = 0.12423642724752426, lr = 0.0001\n",
            "2025-05-28 23:39:50 INFO     Epoch 5 [1600/1875]: loss = 0.1652807593345642, lr = 0.0001\n",
            "2025-05-28 23:39:57 INFO     Epoch 5 [1650/1875]: loss = 0.2135716825723648, lr = 0.0001\n",
            "2025-05-28 23:40:04 INFO     Epoch 5 [1700/1875]: loss = 0.2546100914478302, lr = 0.0001\n",
            "2025-05-28 23:40:11 INFO     Epoch 5 [1750/1875]: loss = 0.19816939532756805, lr = 0.0001\n",
            "2025-05-28 23:40:17 INFO     Epoch 5 [1800/1875]: loss = 0.15102770924568176, lr = 0.0001\n",
            "2025-05-28 23:40:24 INFO     Epoch 5 [1850/1875]: loss = 0.22642947733402252, lr = 0.0001\n",
            "2025-05-28 23:40:28 INFO     Epoch 6 [0/1875]: loss = 0.15336573123931885, lr = 0.0001\n",
            "2025-05-28 23:40:35 INFO     Epoch 6 [50/1875]: loss = 0.15400144457817078, lr = 0.0001\n",
            "2025-05-28 23:40:42 INFO     Epoch 6 [100/1875]: loss = 0.19294743239879608, lr = 0.0001\n",
            "2025-05-28 23:40:49 INFO     Epoch 6 [150/1875]: loss = 0.26477962732315063, lr = 0.0001\n",
            "2025-05-28 23:40:56 INFO     Epoch 6 [200/1875]: loss = 0.18351195752620697, lr = 0.0001\n",
            "2025-05-28 23:41:02 INFO     Epoch 6 [250/1875]: loss = 0.194522425532341, lr = 0.0001\n",
            "2025-05-28 23:41:09 INFO     Epoch 6 [300/1875]: loss = 0.22136642038822174, lr = 0.0001\n",
            "2025-05-28 23:41:16 INFO     Epoch 6 [350/1875]: loss = 0.19901162385940552, lr = 0.0001\n",
            "2025-05-28 23:41:23 INFO     Epoch 6 [400/1875]: loss = 0.2300771325826645, lr = 0.0001\n",
            "2025-05-28 23:41:30 INFO     Epoch 6 [450/1875]: loss = 0.1968139261007309, lr = 0.0001\n",
            "2025-05-28 23:41:37 INFO     Epoch 6 [500/1875]: loss = 0.2176857441663742, lr = 0.0001\n",
            "2025-05-28 23:41:44 INFO     Epoch 6 [550/1875]: loss = 0.20544518530368805, lr = 0.0001\n",
            "2025-05-28 23:41:50 INFO     Epoch 6 [600/1875]: loss = 0.25532111525535583, lr = 0.0001\n",
            "2025-05-28 23:41:57 INFO     Epoch 6 [650/1875]: loss = 0.1790057122707367, lr = 0.0001\n",
            "2025-05-28 23:42:04 INFO     Epoch 6 [700/1875]: loss = 0.19336815178394318, lr = 0.0001\n",
            "2025-05-28 23:42:11 INFO     Epoch 6 [750/1875]: loss = 0.2604447305202484, lr = 0.0001\n",
            "2025-05-28 23:42:18 INFO     Epoch 6 [800/1875]: loss = 0.2385191172361374, lr = 0.0001\n",
            "2025-05-28 23:42:25 INFO     Epoch 6 [850/1875]: loss = 0.20574340224266052, lr = 0.0001\n",
            "2025-05-28 23:42:31 INFO     Epoch 6 [900/1875]: loss = 0.20804773271083832, lr = 0.0001\n",
            "2025-05-28 23:42:38 INFO     Epoch 6 [950/1875]: loss = 0.2424754947423935, lr = 0.0001\n",
            "2025-05-28 23:42:45 INFO     Epoch 6 [1000/1875]: loss = 0.10117904841899872, lr = 0.0001\n",
            "2025-05-28 23:42:52 INFO     Epoch 6 [1050/1875]: loss = 0.23496036231517792, lr = 0.0001\n",
            "2025-05-28 23:42:59 INFO     Epoch 6 [1100/1875]: loss = 0.2560316324234009, lr = 0.0001\n",
            "2025-05-28 23:43:05 INFO     Epoch 6 [1150/1875]: loss = 0.1489402949810028, lr = 0.0001\n",
            "2025-05-28 23:43:12 INFO     Epoch 6 [1200/1875]: loss = 0.20775407552719116, lr = 0.0001\n",
            "2025-05-28 23:43:19 INFO     Epoch 6 [1250/1875]: loss = 0.17516151070594788, lr = 0.0001\n",
            "2025-05-28 23:43:26 INFO     Epoch 6 [1300/1875]: loss = 0.19900135695934296, lr = 0.0001\n",
            "2025-05-28 23:43:33 INFO     Epoch 6 [1350/1875]: loss = 0.21099959313869476, lr = 0.0001\n",
            "2025-05-28 23:43:40 INFO     Epoch 6 [1400/1875]: loss = 0.22793921828269958, lr = 0.0001\n",
            "2025-05-28 23:43:46 INFO     Epoch 6 [1450/1875]: loss = 0.22237245738506317, lr = 0.0001\n",
            "2025-05-28 23:43:53 INFO     Epoch 6 [1500/1875]: loss = 0.20483674108982086, lr = 0.0001\n",
            "2025-05-28 23:44:00 INFO     Epoch 6 [1550/1875]: loss = 0.20709262788295746, lr = 0.0001\n",
            "2025-05-28 23:44:07 INFO     Epoch 6 [1600/1875]: loss = 0.15266568958759308, lr = 0.0001\n",
            "2025-05-28 23:44:14 INFO     Epoch 6 [1650/1875]: loss = 0.20448781549930573, lr = 0.0001\n",
            "2025-05-28 23:44:21 INFO     Epoch 6 [1700/1875]: loss = 0.32127007842063904, lr = 0.0001\n",
            "2025-05-28 23:44:28 INFO     Epoch 6 [1750/1875]: loss = 0.17757752537727356, lr = 0.0001\n",
            "2025-05-28 23:44:35 INFO     Epoch 6 [1800/1875]: loss = 0.18660321831703186, lr = 0.0001\n",
            "2025-05-28 23:44:41 INFO     Epoch 6 [1850/1875]: loss = 0.17220650613307953, lr = 0.0001\n",
            "2025-05-28 23:44:45 INFO     Epoch 7 [0/1875]: loss = 0.19034676253795624, lr = 0.0001\n",
            "2025-05-28 23:44:52 INFO     Epoch 7 [50/1875]: loss = 0.23891589045524597, lr = 0.0001\n",
            "2025-05-28 23:44:59 INFO     Epoch 7 [100/1875]: loss = 0.16548117995262146, lr = 0.0001\n",
            "2025-05-28 23:45:06 INFO     Epoch 7 [150/1875]: loss = 0.22732722759246826, lr = 0.0001\n",
            "2025-05-28 23:45:13 INFO     Epoch 7 [200/1875]: loss = 0.19474206864833832, lr = 0.0001\n",
            "2025-05-28 23:45:19 INFO     Epoch 7 [250/1875]: loss = 0.2466772198677063, lr = 0.0001\n",
            "2025-05-28 23:45:26 INFO     Epoch 7 [300/1875]: loss = 0.20874862372875214, lr = 0.0001\n",
            "2025-05-28 23:45:33 INFO     Epoch 7 [350/1875]: loss = 0.16633901000022888, lr = 0.0001\n",
            "2025-05-28 23:45:40 INFO     Epoch 7 [400/1875]: loss = 0.21586552262306213, lr = 0.0001\n",
            "2025-05-28 23:45:47 INFO     Epoch 7 [450/1875]: loss = 0.19295752048492432, lr = 0.0001\n",
            "2025-05-28 23:45:54 INFO     Epoch 7 [500/1875]: loss = 0.1724836379289627, lr = 0.0001\n",
            "2025-05-28 23:46:00 INFO     Epoch 7 [550/1875]: loss = 0.2151893824338913, lr = 0.0001\n",
            "2025-05-28 23:46:07 INFO     Epoch 7 [600/1875]: loss = 0.20680080354213715, lr = 0.0001\n",
            "2025-05-28 23:46:14 INFO     Epoch 7 [650/1875]: loss = 0.23919272422790527, lr = 0.0001\n",
            "2025-05-28 23:46:21 INFO     Epoch 7 [700/1875]: loss = 0.18131545186042786, lr = 0.0001\n",
            "2025-05-28 23:46:28 INFO     Epoch 7 [750/1875]: loss = 0.2296515256166458, lr = 0.0001\n",
            "2025-05-28 23:46:35 INFO     Epoch 7 [800/1875]: loss = 0.18432708084583282, lr = 0.0001\n",
            "2025-05-28 23:46:41 INFO     Epoch 7 [850/1875]: loss = 0.18601174652576447, lr = 0.0001\n",
            "2025-05-28 23:46:48 INFO     Epoch 7 [900/1875]: loss = 0.21294990181922913, lr = 0.0001\n",
            "2025-05-28 23:46:55 INFO     Epoch 7 [950/1875]: loss = 0.24229805171489716, lr = 0.0001\n",
            "2025-05-28 23:47:02 INFO     Epoch 7 [1000/1875]: loss = 0.21703940629959106, lr = 0.0001\n",
            "2025-05-28 23:47:09 INFO     Epoch 7 [1050/1875]: loss = 0.20760281383991241, lr = 0.0001\n",
            "2025-05-28 23:47:16 INFO     Epoch 7 [1100/1875]: loss = 0.20548151433467865, lr = 0.0001\n",
            "2025-05-28 23:47:23 INFO     Epoch 7 [1150/1875]: loss = 0.17034220695495605, lr = 0.0001\n",
            "2025-05-28 23:47:30 INFO     Epoch 7 [1200/1875]: loss = 0.1594640016555786, lr = 0.0001\n",
            "2025-05-28 23:47:36 INFO     Epoch 7 [1250/1875]: loss = 0.18100818991661072, lr = 0.0001\n",
            "2025-05-28 23:47:43 INFO     Epoch 7 [1300/1875]: loss = 0.2273179441690445, lr = 0.0001\n",
            "2025-05-28 23:47:50 INFO     Epoch 7 [1350/1875]: loss = 0.19481049478054047, lr = 0.0001\n",
            "2025-05-28 23:47:57 INFO     Epoch 7 [1400/1875]: loss = 0.16428792476654053, lr = 0.0001\n",
            "2025-05-28 23:48:04 INFO     Epoch 7 [1450/1875]: loss = 0.25859618186950684, lr = 0.0001\n",
            "2025-05-28 23:48:10 INFO     Epoch 7 [1500/1875]: loss = 0.16863657534122467, lr = 0.0001\n",
            "2025-05-28 23:48:17 INFO     Epoch 7 [1550/1875]: loss = 0.2022305577993393, lr = 0.0001\n",
            "2025-05-28 23:48:24 INFO     Epoch 7 [1600/1875]: loss = 0.213963121175766, lr = 0.0001\n",
            "2025-05-28 23:48:31 INFO     Epoch 7 [1650/1875]: loss = 0.2190280258655548, lr = 0.0001\n",
            "2025-05-28 23:48:38 INFO     Epoch 7 [1700/1875]: loss = 0.21058864891529083, lr = 0.0001\n",
            "2025-05-28 23:48:45 INFO     Epoch 7 [1750/1875]: loss = 0.19080445170402527, lr = 0.0001\n",
            "2025-05-28 23:48:51 INFO     Epoch 7 [1800/1875]: loss = 0.18038928508758545, lr = 0.0001\n",
            "2025-05-28 23:48:58 INFO     Epoch 7 [1850/1875]: loss = 0.18890367448329926, lr = 0.0001\n",
            "2025-05-28 23:49:02 INFO     Epoch 8 [0/1875]: loss = 0.14501890540122986, lr = 0.0001\n",
            "2025-05-28 23:49:09 INFO     Epoch 8 [50/1875]: loss = 0.23901382088661194, lr = 0.0001\n",
            "2025-05-28 23:49:16 INFO     Epoch 8 [100/1875]: loss = 0.21547512710094452, lr = 0.0001\n",
            "2025-05-28 23:49:23 INFO     Epoch 8 [150/1875]: loss = 0.22055760025978088, lr = 0.0001\n",
            "2025-05-28 23:49:29 INFO     Epoch 8 [200/1875]: loss = 0.21495307981967926, lr = 0.0001\n",
            "2025-05-28 23:49:36 INFO     Epoch 8 [250/1875]: loss = 0.22145888209342957, lr = 0.0001\n",
            "2025-05-28 23:49:43 INFO     Epoch 8 [300/1875]: loss = 0.2845897674560547, lr = 0.0001\n",
            "2025-05-28 23:49:50 INFO     Epoch 8 [350/1875]: loss = 0.166701540350914, lr = 0.0001\n",
            "2025-05-28 23:49:57 INFO     Epoch 8 [400/1875]: loss = 0.20175306499004364, lr = 0.0001\n",
            "2025-05-28 23:50:04 INFO     Epoch 8 [450/1875]: loss = 0.2571144998073578, lr = 0.0001\n",
            "2025-05-28 23:50:11 INFO     Epoch 8 [500/1875]: loss = 0.1458432674407959, lr = 0.0001\n",
            "2025-05-28 23:50:17 INFO     Epoch 8 [550/1875]: loss = 0.20300038158893585, lr = 0.0001\n",
            "2025-05-28 23:50:24 INFO     Epoch 8 [600/1875]: loss = 0.20446188747882843, lr = 0.0001\n",
            "2025-05-28 23:50:31 INFO     Epoch 8 [650/1875]: loss = 0.24077551066875458, lr = 0.0001\n",
            "2025-05-28 23:50:38 INFO     Epoch 8 [700/1875]: loss = 0.1805066019296646, lr = 0.0001\n",
            "2025-05-28 23:50:45 INFO     Epoch 8 [750/1875]: loss = 0.2058548480272293, lr = 0.0001\n",
            "2025-05-28 23:50:51 INFO     Epoch 8 [800/1875]: loss = 0.22784769535064697, lr = 0.0001\n",
            "2025-05-28 23:50:58 INFO     Epoch 8 [850/1875]: loss = 0.17208321392536163, lr = 0.0001\n",
            "2025-05-28 23:51:05 INFO     Epoch 8 [900/1875]: loss = 0.2252190262079239, lr = 0.0001\n",
            "2025-05-28 23:51:12 INFO     Epoch 8 [950/1875]: loss = 0.23320293426513672, lr = 0.0001\n",
            "2025-05-28 23:51:19 INFO     Epoch 8 [1000/1875]: loss = 0.23654203116893768, lr = 0.0001\n",
            "2025-05-28 23:51:25 INFO     Epoch 8 [1050/1875]: loss = 0.17033497989177704, lr = 0.0001\n",
            "2025-05-28 23:51:32 INFO     Epoch 8 [1100/1875]: loss = 0.1975538283586502, lr = 0.0001\n",
            "2025-05-28 23:51:39 INFO     Epoch 8 [1150/1875]: loss = 0.19920843839645386, lr = 0.0001\n",
            "2025-05-28 23:51:46 INFO     Epoch 8 [1200/1875]: loss = 0.17982348799705505, lr = 0.0001\n",
            "2025-05-28 23:51:53 INFO     Epoch 8 [1250/1875]: loss = 0.16800521314144135, lr = 0.0001\n",
            "2025-05-28 23:51:59 INFO     Epoch 8 [1300/1875]: loss = 0.20713579654693604, lr = 0.0001\n",
            "2025-05-28 23:52:06 INFO     Epoch 8 [1350/1875]: loss = 0.19315369427204132, lr = 0.0001\n",
            "2025-05-28 23:52:13 INFO     Epoch 8 [1400/1875]: loss = 0.21210898458957672, lr = 0.0001\n",
            "2025-05-28 23:52:20 INFO     Epoch 8 [1450/1875]: loss = 0.20461620390415192, lr = 0.0001\n",
            "2025-05-28 23:52:27 INFO     Epoch 8 [1500/1875]: loss = 0.18148547410964966, lr = 0.0001\n",
            "2025-05-28 23:52:34 INFO     Epoch 8 [1550/1875]: loss = 0.17427898943424225, lr = 0.0001\n",
            "2025-05-28 23:52:40 INFO     Epoch 8 [1600/1875]: loss = 0.22457535564899445, lr = 0.0001\n",
            "2025-05-28 23:52:47 INFO     Epoch 8 [1650/1875]: loss = 0.20367009937763214, lr = 0.0001\n",
            "2025-05-28 23:52:54 INFO     Epoch 8 [1700/1875]: loss = 0.24014383554458618, lr = 0.0001\n",
            "2025-05-28 23:53:01 INFO     Epoch 8 [1750/1875]: loss = 0.2018425166606903, lr = 0.0001\n",
            "2025-05-28 23:53:08 INFO     Epoch 8 [1800/1875]: loss = 0.19744376838207245, lr = 0.0001\n",
            "2025-05-28 23:53:15 INFO     Epoch 8 [1850/1875]: loss = 0.21565401554107666, lr = 0.0001\n",
            "2025-05-28 23:53:18 INFO     Epoch 9 [0/1875]: loss = 0.19971998035907745, lr = 0.0001\n",
            "2025-05-28 23:53:25 INFO     Epoch 9 [50/1875]: loss = 0.2114604264497757, lr = 0.0001\n",
            "2025-05-28 23:53:32 INFO     Epoch 9 [100/1875]: loss = 0.17671138048171997, lr = 0.0001\n",
            "2025-05-28 23:53:39 INFO     Epoch 9 [150/1875]: loss = 0.24337685108184814, lr = 0.0001\n",
            "2025-05-28 23:53:45 INFO     Epoch 9 [200/1875]: loss = 0.18060524761676788, lr = 0.0001\n",
            "2025-05-28 23:53:52 INFO     Epoch 9 [250/1875]: loss = 0.16171982884407043, lr = 0.0001\n",
            "2025-05-28 23:53:59 INFO     Epoch 9 [300/1875]: loss = 0.23179325461387634, lr = 0.0001\n",
            "2025-05-28 23:54:06 INFO     Epoch 9 [350/1875]: loss = 0.157278910279274, lr = 0.0001\n",
            "2025-05-28 23:54:13 INFO     Epoch 9 [400/1875]: loss = 0.18110457062721252, lr = 0.0001\n",
            "2025-05-28 23:54:19 INFO     Epoch 9 [450/1875]: loss = 0.18632782995700836, lr = 0.0001\n",
            "2025-05-28 23:54:26 INFO     Epoch 9 [500/1875]: loss = 0.1690494567155838, lr = 0.0001\n",
            "2025-05-28 23:54:33 INFO     Epoch 9 [550/1875]: loss = 0.18533886969089508, lr = 0.0001\n",
            "2025-05-28 23:54:40 INFO     Epoch 9 [600/1875]: loss = 0.1770503669977188, lr = 0.0001\n",
            "2025-05-28 23:54:47 INFO     Epoch 9 [650/1875]: loss = 0.24064527451992035, lr = 0.0001\n",
            "2025-05-28 23:54:53 INFO     Epoch 9 [700/1875]: loss = 0.19933804869651794, lr = 0.0001\n",
            "2025-05-28 23:55:00 INFO     Epoch 9 [750/1875]: loss = 0.21288296580314636, lr = 0.0001\n",
            "2025-05-28 23:55:07 INFO     Epoch 9 [800/1875]: loss = 0.2175108939409256, lr = 0.0001\n",
            "2025-05-28 23:55:14 INFO     Epoch 9 [850/1875]: loss = 0.16889896988868713, lr = 0.0001\n",
            "2025-05-28 23:55:21 INFO     Epoch 9 [900/1875]: loss = 0.20116741955280304, lr = 0.0001\n",
            "2025-05-28 23:55:28 INFO     Epoch 9 [950/1875]: loss = 0.2645089328289032, lr = 0.0001\n",
            "2025-05-28 23:55:34 INFO     Epoch 9 [1000/1875]: loss = 0.1724185198545456, lr = 0.0001\n",
            "2025-05-28 23:55:41 INFO     Epoch 9 [1050/1875]: loss = 0.22020284831523895, lr = 0.0001\n",
            "2025-05-28 23:55:48 INFO     Epoch 9 [1100/1875]: loss = 0.19072557985782623, lr = 0.0001\n",
            "2025-05-28 23:55:55 INFO     Epoch 9 [1150/1875]: loss = 0.15738780796527863, lr = 0.0001\n",
            "2025-05-28 23:56:02 INFO     Epoch 9 [1200/1875]: loss = 0.20916017889976501, lr = 0.0001\n",
            "2025-05-28 23:56:08 INFO     Epoch 9 [1250/1875]: loss = 0.20680119097232819, lr = 0.0001\n",
            "2025-05-28 23:56:15 INFO     Epoch 9 [1300/1875]: loss = 0.18748921155929565, lr = 0.0001\n",
            "2025-05-28 23:56:22 INFO     Epoch 9 [1350/1875]: loss = 0.22313223779201508, lr = 0.0001\n",
            "2025-05-28 23:56:29 INFO     Epoch 9 [1400/1875]: loss = 0.19035647809505463, lr = 0.0001\n",
            "2025-05-28 23:56:35 INFO     Epoch 9 [1450/1875]: loss = 0.24665331840515137, lr = 0.0001\n",
            "2025-05-28 23:56:42 INFO     Epoch 9 [1500/1875]: loss = 0.1767575889825821, lr = 0.0001\n",
            "2025-05-28 23:56:49 INFO     Epoch 9 [1550/1875]: loss = 0.24644747376441956, lr = 0.0001\n",
            "2025-05-28 23:56:56 INFO     Epoch 9 [1600/1875]: loss = 0.2213483303785324, lr = 0.0001\n",
            "2025-05-28 23:57:02 INFO     Epoch 9 [1650/1875]: loss = 0.17752934992313385, lr = 0.0001\n",
            "2025-05-28 23:57:09 INFO     Epoch 9 [1700/1875]: loss = 0.2413901835680008, lr = 0.0001\n",
            "2025-05-28 23:57:16 INFO     Epoch 9 [1750/1875]: loss = 0.1796802580356598, lr = 0.0001\n",
            "2025-05-28 23:57:23 INFO     Epoch 9 [1800/1875]: loss = 0.1565413624048233, lr = 0.0001\n",
            "2025-05-28 23:57:29 INFO     Epoch 9 [1850/1875]: loss = 0.21563638746738434, lr = 0.0001\n",
            "2025-05-28 23:57:35 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "2025-05-28 23:58:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:58:22 INFO     Evaluating [0/313] samples generated [32/1000] running fid 241.42208862304688\n",
            "2025-05-28 23:59:02 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-28 23:59:41 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:00:21 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:01:00 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:01:40 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:02:20 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:02:59 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:03:39 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:04:19 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:04:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:05:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:06:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:06:57 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:07:37 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:08:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:08:56 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:09:36 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:10:16 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:10:55 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:11:35 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:12:14 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:12:54 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:13:34 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:14:13 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:14:53 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:15:32 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:16:12 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:16:52 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:17:32 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:18:12 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:18:51 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:18:58 INFO     Evaluating [50/313] samples generated [1000/1000] running fid 185.02345275878906\n",
            "2025-05-29 00:19:08 INFO     Evaluating [100/313] samples generated [1000/1000] running fid 184.54310607910156\n",
            "2025-05-29 00:19:19 INFO     Evaluating [150/313] samples generated [1000/1000] running fid 184.1787872314453\n",
            "2025-05-29 00:19:29 INFO     Evaluating [200/313] samples generated [1000/1000] running fid 184.24224853515625\n",
            "2025-05-29 00:19:39 INFO     Evaluating [250/313] samples generated [1000/1000] running fid 184.68490600585938\n",
            "2025-05-29 00:19:49 INFO     Evaluating [300/313] samples generated [1000/1000] running fid 184.75877380371094\n",
            "2025-05-29 00:19:55 INFO     EMA: Switching from eval to train, restoring saved parameters\n",
            "2025-05-29 00:19:55 INFO     Epoch 10 [0/1875]: loss = 0.2012833058834076, lr = 0.0001\n",
            "2025-05-29 00:20:02 INFO     Epoch 10 [50/1875]: loss = 0.15614089369773865, lr = 0.0001\n",
            "2025-05-29 00:20:09 INFO     Epoch 10 [100/1875]: loss = 0.17687952518463135, lr = 0.0001\n",
            "2025-05-29 00:20:16 INFO     Epoch 10 [150/1875]: loss = 0.19590537250041962, lr = 0.0001\n",
            "2025-05-29 00:20:23 INFO     Epoch 10 [200/1875]: loss = 0.17693652212619781, lr = 0.0001\n",
            "2025-05-29 00:20:30 INFO     Epoch 10 [250/1875]: loss = 0.2618074119091034, lr = 0.0001\n",
            "2025-05-29 00:20:36 INFO     Epoch 10 [300/1875]: loss = 0.20122426748275757, lr = 0.0001\n",
            "2025-05-29 00:20:43 INFO     Epoch 10 [350/1875]: loss = 0.16904471814632416, lr = 0.0001\n",
            "2025-05-29 00:20:50 INFO     Epoch 10 [400/1875]: loss = 0.13600113987922668, lr = 0.0001\n",
            "2025-05-29 00:20:57 INFO     Epoch 10 [450/1875]: loss = 0.18859000504016876, lr = 0.0001\n",
            "2025-05-29 00:21:04 INFO     Epoch 10 [500/1875]: loss = 0.2107231169939041, lr = 0.0001\n",
            "2025-05-29 00:21:11 INFO     Epoch 10 [550/1875]: loss = 0.17419610917568207, lr = 0.0001\n",
            "2025-05-29 00:21:18 INFO     Epoch 10 [600/1875]: loss = 0.1323629915714264, lr = 0.0001\n",
            "2025-05-29 00:21:24 INFO     Epoch 10 [650/1875]: loss = 0.2252325415611267, lr = 0.0001\n",
            "2025-05-29 00:21:31 INFO     Epoch 10 [700/1875]: loss = 0.21236111223697662, lr = 0.0001\n",
            "2025-05-29 00:21:38 INFO     Epoch 10 [750/1875]: loss = 0.24539443850517273, lr = 0.0001\n",
            "2025-05-29 00:21:45 INFO     Epoch 10 [800/1875]: loss = 0.17500485479831696, lr = 0.0001\n",
            "2025-05-29 00:21:52 INFO     Epoch 10 [850/1875]: loss = 0.18401263654232025, lr = 0.0001\n",
            "2025-05-29 00:21:58 INFO     Epoch 10 [900/1875]: loss = 0.20162035524845123, lr = 0.0001\n",
            "2025-05-29 00:22:05 INFO     Epoch 10 [950/1875]: loss = 0.14617492258548737, lr = 0.0001\n",
            "2025-05-29 00:22:12 INFO     Epoch 10 [1000/1875]: loss = 0.221321702003479, lr = 0.0001\n",
            "2025-05-29 00:22:19 INFO     Epoch 10 [1050/1875]: loss = 0.2331937998533249, lr = 0.0001\n",
            "2025-05-29 00:22:26 INFO     Epoch 10 [1100/1875]: loss = 0.21627743542194366, lr = 0.0001\n",
            "2025-05-29 00:22:32 INFO     Epoch 10 [1150/1875]: loss = 0.19823119044303894, lr = 0.0001\n",
            "2025-05-29 00:22:39 INFO     Epoch 10 [1200/1875]: loss = 0.19801394641399384, lr = 0.0001\n",
            "2025-05-29 00:22:46 INFO     Epoch 10 [1250/1875]: loss = 0.17437399923801422, lr = 0.0001\n",
            "2025-05-29 00:22:53 INFO     Epoch 10 [1300/1875]: loss = 0.1832556575536728, lr = 0.0001\n",
            "2025-05-29 00:22:59 INFO     Epoch 10 [1350/1875]: loss = 0.1661957949399948, lr = 0.0001\n",
            "2025-05-29 00:23:06 INFO     Epoch 10 [1400/1875]: loss = 0.1543850600719452, lr = 0.0001\n",
            "2025-05-29 00:23:13 INFO     Epoch 10 [1450/1875]: loss = 0.20706774294376373, lr = 0.0001\n",
            "2025-05-29 00:23:20 INFO     Epoch 10 [1500/1875]: loss = 0.1801571398973465, lr = 0.0001\n",
            "2025-05-29 00:23:26 INFO     Epoch 10 [1550/1875]: loss = 0.17672601342201233, lr = 0.0001\n",
            "2025-05-29 00:23:33 INFO     Epoch 10 [1600/1875]: loss = 0.25575709342956543, lr = 0.0001\n",
            "2025-05-29 00:23:40 INFO     Epoch 10 [1650/1875]: loss = 0.22191865742206573, lr = 0.0001\n",
            "2025-05-29 00:23:47 INFO     Epoch 10 [1700/1875]: loss = 0.23125584423542023, lr = 0.0001\n",
            "2025-05-29 00:23:54 INFO     Epoch 10 [1750/1875]: loss = 0.20988872647285461, lr = 0.0001\n",
            "2025-05-29 00:24:01 INFO     Epoch 10 [1800/1875]: loss = 0.14500786364078522, lr = 0.0001\n",
            "2025-05-29 00:24:07 INFO     Epoch 10 [1850/1875]: loss = 0.19298942387104034, lr = 0.0001\n",
            "2025-05-29 00:24:11 INFO     Epoch 11 [0/1875]: loss = 0.13592711091041565, lr = 0.0001\n",
            "2025-05-29 00:24:18 INFO     Epoch 11 [50/1875]: loss = 0.2578516900539398, lr = 0.0001\n",
            "2025-05-29 00:24:25 INFO     Epoch 11 [100/1875]: loss = 0.1513419896364212, lr = 0.0001\n",
            "2025-05-29 00:24:32 INFO     Epoch 11 [150/1875]: loss = 0.22351998090744019, lr = 0.0001\n",
            "2025-05-29 00:24:38 INFO     Epoch 11 [200/1875]: loss = 0.17743659019470215, lr = 0.0001\n",
            "2025-05-29 00:24:45 INFO     Epoch 11 [250/1875]: loss = 0.24468521773815155, lr = 0.0001\n",
            "2025-05-29 00:24:52 INFO     Epoch 11 [300/1875]: loss = 0.15517811477184296, lr = 0.0001\n",
            "2025-05-29 00:24:59 INFO     Epoch 11 [350/1875]: loss = 0.1714903712272644, lr = 0.0001\n",
            "2025-05-29 00:25:05 INFO     Epoch 11 [400/1875]: loss = 0.17835168540477753, lr = 0.0001\n",
            "2025-05-29 00:25:12 INFO     Epoch 11 [450/1875]: loss = 0.21954737603664398, lr = 0.0001\n",
            "2025-05-29 00:25:19 INFO     Epoch 11 [500/1875]: loss = 0.15907011926174164, lr = 0.0001\n",
            "2025-05-29 00:25:26 INFO     Epoch 11 [550/1875]: loss = 0.17118346691131592, lr = 0.0001\n",
            "2025-05-29 00:25:33 INFO     Epoch 11 [600/1875]: loss = 0.14579026401042938, lr = 0.0001\n",
            "2025-05-29 00:25:39 INFO     Epoch 11 [650/1875]: loss = 0.21537922322750092, lr = 0.0001\n",
            "2025-05-29 00:25:46 INFO     Epoch 11 [700/1875]: loss = 0.184968963265419, lr = 0.0001\n",
            "2025-05-29 00:25:53 INFO     Epoch 11 [750/1875]: loss = 0.1764184534549713, lr = 0.0001\n",
            "2025-05-29 00:26:00 INFO     Epoch 11 [800/1875]: loss = 0.25829505920410156, lr = 0.0001\n",
            "2025-05-29 00:26:06 INFO     Epoch 11 [850/1875]: loss = 0.26343047618865967, lr = 0.0001\n",
            "2025-05-29 00:26:13 INFO     Epoch 11 [900/1875]: loss = 0.27314817905426025, lr = 0.0001\n",
            "2025-05-29 00:26:20 INFO     Epoch 11 [950/1875]: loss = 0.2451770305633545, lr = 0.0001\n",
            "2025-05-29 00:26:27 INFO     Epoch 11 [1000/1875]: loss = 0.21900372207164764, lr = 0.0001\n",
            "2025-05-29 00:26:34 INFO     Epoch 11 [1050/1875]: loss = 0.19637572765350342, lr = 0.0001\n",
            "2025-05-29 00:26:40 INFO     Epoch 11 [1100/1875]: loss = 0.23743833601474762, lr = 0.0001\n",
            "2025-05-29 00:26:47 INFO     Epoch 11 [1150/1875]: loss = 0.18769343197345734, lr = 0.0001\n",
            "2025-05-29 00:26:54 INFO     Epoch 11 [1200/1875]: loss = 0.1523445099592209, lr = 0.0001\n",
            "2025-05-29 00:27:01 INFO     Epoch 11 [1250/1875]: loss = 0.16892530024051666, lr = 0.0001\n",
            "2025-05-29 00:27:08 INFO     Epoch 11 [1300/1875]: loss = 0.18289822340011597, lr = 0.0001\n",
            "2025-05-29 00:27:14 INFO     Epoch 11 [1350/1875]: loss = 0.16355007886886597, lr = 0.0001\n",
            "2025-05-29 00:27:21 INFO     Epoch 11 [1400/1875]: loss = 0.18416067957878113, lr = 0.0001\n",
            "2025-05-29 00:27:28 INFO     Epoch 11 [1450/1875]: loss = 0.24850092828273773, lr = 0.0001\n",
            "2025-05-29 00:27:35 INFO     Epoch 11 [1500/1875]: loss = 0.21150673925876617, lr = 0.0001\n",
            "2025-05-29 00:27:41 INFO     Epoch 11 [1550/1875]: loss = 0.19045881927013397, lr = 0.0001\n",
            "2025-05-29 00:27:48 INFO     Epoch 11 [1600/1875]: loss = 0.21536117792129517, lr = 0.0001\n",
            "2025-05-29 00:27:55 INFO     Epoch 11 [1650/1875]: loss = 0.2024475634098053, lr = 0.0001\n",
            "2025-05-29 00:28:02 INFO     Epoch 11 [1700/1875]: loss = 0.22570335865020752, lr = 0.0001\n",
            "2025-05-29 00:28:09 INFO     Epoch 11 [1750/1875]: loss = 0.1626804769039154, lr = 0.0001\n",
            "2025-05-29 00:28:15 INFO     Epoch 11 [1800/1875]: loss = 0.19682499766349792, lr = 0.0001\n",
            "2025-05-29 00:28:22 INFO     Epoch 11 [1850/1875]: loss = 0.23500053584575653, lr = 0.0001\n",
            "2025-05-29 00:28:26 INFO     Epoch 12 [0/1875]: loss = 0.23280023038387299, lr = 0.0001\n",
            "2025-05-29 00:28:33 INFO     Epoch 12 [50/1875]: loss = 0.20719872415065765, lr = 0.0001\n",
            "2025-05-29 00:28:39 INFO     Epoch 12 [100/1875]: loss = 0.1827259510755539, lr = 0.0001\n",
            "2025-05-29 00:28:46 INFO     Epoch 12 [150/1875]: loss = 0.22148573398590088, lr = 0.0001\n",
            "2025-05-29 00:28:53 INFO     Epoch 12 [200/1875]: loss = 0.237028107047081, lr = 0.0001\n",
            "2025-05-29 00:29:00 INFO     Epoch 12 [250/1875]: loss = 0.1608843356370926, lr = 0.0001\n",
            "2025-05-29 00:29:07 INFO     Epoch 12 [300/1875]: loss = 0.2115693986415863, lr = 0.0001\n",
            "2025-05-29 00:29:13 INFO     Epoch 12 [350/1875]: loss = 0.173899844288826, lr = 0.0001\n",
            "2025-05-29 00:29:20 INFO     Epoch 12 [400/1875]: loss = 0.18445834517478943, lr = 0.0001\n",
            "2025-05-29 00:29:27 INFO     Epoch 12 [450/1875]: loss = 0.151956245303154, lr = 0.0001\n",
            "2025-05-29 00:29:34 INFO     Epoch 12 [500/1875]: loss = 0.22351035475730896, lr = 0.0001\n",
            "2025-05-29 00:29:41 INFO     Epoch 12 [550/1875]: loss = 0.14765550196170807, lr = 0.0001\n",
            "2025-05-29 00:29:47 INFO     Epoch 12 [600/1875]: loss = 0.24259702861309052, lr = 0.0001\n",
            "2025-05-29 00:29:54 INFO     Epoch 12 [650/1875]: loss = 0.23098936676979065, lr = 0.0001\n",
            "2025-05-29 00:30:01 INFO     Epoch 12 [700/1875]: loss = 0.2265152484178543, lr = 0.0001\n",
            "2025-05-29 00:30:08 INFO     Epoch 12 [750/1875]: loss = 0.21145059168338776, lr = 0.0001\n",
            "2025-05-29 00:30:15 INFO     Epoch 12 [800/1875]: loss = 0.2342747300863266, lr = 0.0001\n",
            "2025-05-29 00:30:22 INFO     Epoch 12 [850/1875]: loss = 0.23512086272239685, lr = 0.0001\n",
            "2025-05-29 00:30:28 INFO     Epoch 12 [900/1875]: loss = 0.23160254955291748, lr = 0.0001\n",
            "2025-05-29 00:30:35 INFO     Epoch 12 [950/1875]: loss = 0.19217108190059662, lr = 0.0001\n",
            "2025-05-29 00:30:42 INFO     Epoch 12 [1000/1875]: loss = 0.23655039072036743, lr = 0.0001\n",
            "2025-05-29 00:30:49 INFO     Epoch 12 [1050/1875]: loss = 0.21854810416698456, lr = 0.0001\n",
            "2025-05-29 00:30:55 INFO     Epoch 12 [1100/1875]: loss = 0.1979832649230957, lr = 0.0001\n",
            "2025-05-29 00:31:02 INFO     Epoch 12 [1150/1875]: loss = 0.22152888774871826, lr = 0.0001\n",
            "2025-05-29 00:31:09 INFO     Epoch 12 [1200/1875]: loss = 0.1966301053762436, lr = 0.0001\n",
            "2025-05-29 00:31:16 INFO     Epoch 12 [1250/1875]: loss = 0.15380895137786865, lr = 0.0001\n",
            "2025-05-29 00:31:23 INFO     Epoch 12 [1300/1875]: loss = 0.23946598172187805, lr = 0.0001\n",
            "2025-05-29 00:31:29 INFO     Epoch 12 [1350/1875]: loss = 0.18996120989322662, lr = 0.0001\n",
            "2025-05-29 00:31:36 INFO     Epoch 12 [1400/1875]: loss = 0.17809529602527618, lr = 0.0001\n",
            "2025-05-29 00:31:43 INFO     Epoch 12 [1450/1875]: loss = 0.19576989114284515, lr = 0.0001\n",
            "2025-05-29 00:31:50 INFO     Epoch 12 [1500/1875]: loss = 0.16681669652462006, lr = 0.0001\n",
            "2025-05-29 00:31:57 INFO     Epoch 12 [1550/1875]: loss = 0.1712469607591629, lr = 0.0001\n",
            "2025-05-29 00:32:03 INFO     Epoch 12 [1600/1875]: loss = 0.19385024905204773, lr = 0.0001\n",
            "2025-05-29 00:32:10 INFO     Epoch 12 [1650/1875]: loss = 0.23020504415035248, lr = 0.0001\n",
            "2025-05-29 00:32:17 INFO     Epoch 12 [1700/1875]: loss = 0.23942933976650238, lr = 0.0001\n",
            "2025-05-29 00:32:24 INFO     Epoch 12 [1750/1875]: loss = 0.17628242075443268, lr = 0.0001\n",
            "2025-05-29 00:32:30 INFO     Epoch 12 [1800/1875]: loss = 0.2029096782207489, lr = 0.0001\n",
            "2025-05-29 00:32:37 INFO     Epoch 12 [1850/1875]: loss = 0.22934718430042267, lr = 0.0001\n",
            "2025-05-29 00:32:41 INFO     Epoch 13 [0/1875]: loss = 0.18234039843082428, lr = 0.0001\n",
            "2025-05-29 00:32:48 INFO     Epoch 13 [50/1875]: loss = 0.1988803893327713, lr = 0.0001\n",
            "2025-05-29 00:32:55 INFO     Epoch 13 [100/1875]: loss = 0.19166788458824158, lr = 0.0001\n",
            "2025-05-29 00:33:02 INFO     Epoch 13 [150/1875]: loss = 0.22685186564922333, lr = 0.0001\n",
            "2025-05-29 00:33:08 INFO     Epoch 13 [200/1875]: loss = 0.19722092151641846, lr = 0.0001\n",
            "2025-05-29 00:33:15 INFO     Epoch 13 [250/1875]: loss = 0.22247272729873657, lr = 0.0001\n",
            "2025-05-29 00:33:22 INFO     Epoch 13 [300/1875]: loss = 0.16414159536361694, lr = 0.0001\n",
            "2025-05-29 00:33:29 INFO     Epoch 13 [350/1875]: loss = 0.15498089790344238, lr = 0.0001\n",
            "2025-05-29 00:33:35 INFO     Epoch 13 [400/1875]: loss = 0.1373259574174881, lr = 0.0001\n",
            "2025-05-29 00:33:42 INFO     Epoch 13 [450/1875]: loss = 0.1797601878643036, lr = 0.0001\n",
            "2025-05-29 00:33:49 INFO     Epoch 13 [500/1875]: loss = 0.2385658472776413, lr = 0.0001\n",
            "2025-05-29 00:33:56 INFO     Epoch 13 [550/1875]: loss = 0.23205888271331787, lr = 0.0001\n",
            "2025-05-29 00:34:03 INFO     Epoch 13 [600/1875]: loss = 0.13784226775169373, lr = 0.0001\n",
            "2025-05-29 00:34:09 INFO     Epoch 13 [650/1875]: loss = 0.14335153996944427, lr = 0.0001\n",
            "2025-05-29 00:34:16 INFO     Epoch 13 [700/1875]: loss = 0.24554723501205444, lr = 0.0001\n",
            "2025-05-29 00:34:23 INFO     Epoch 13 [750/1875]: loss = 0.23996658623218536, lr = 0.0001\n",
            "2025-05-29 00:34:30 INFO     Epoch 13 [800/1875]: loss = 0.22335615754127502, lr = 0.0001\n",
            "2025-05-29 00:34:36 INFO     Epoch 13 [850/1875]: loss = 0.1813216358423233, lr = 0.0001\n",
            "2025-05-29 00:34:43 INFO     Epoch 13 [900/1875]: loss = 0.16193576157093048, lr = 0.0001\n",
            "2025-05-29 00:34:50 INFO     Epoch 13 [950/1875]: loss = 0.2107241004705429, lr = 0.0001\n",
            "2025-05-29 00:34:57 INFO     Epoch 13 [1000/1875]: loss = 0.16867345571517944, lr = 0.0001\n",
            "2025-05-29 00:35:04 INFO     Epoch 13 [1050/1875]: loss = 0.18270622193813324, lr = 0.0001\n",
            "2025-05-29 00:35:10 INFO     Epoch 13 [1100/1875]: loss = 0.1739233136177063, lr = 0.0001\n",
            "2025-05-29 00:35:17 INFO     Epoch 13 [1150/1875]: loss = 0.19539810717105865, lr = 0.0001\n",
            "2025-05-29 00:35:24 INFO     Epoch 13 [1200/1875]: loss = 0.22404469549655914, lr = 0.0001\n",
            "2025-05-29 00:35:31 INFO     Epoch 13 [1250/1875]: loss = 0.16836833953857422, lr = 0.0001\n",
            "2025-05-29 00:35:38 INFO     Epoch 13 [1300/1875]: loss = 0.21394242346286774, lr = 0.0001\n",
            "2025-05-29 00:35:44 INFO     Epoch 13 [1350/1875]: loss = 0.21702665090560913, lr = 0.0001\n",
            "2025-05-29 00:35:51 INFO     Epoch 13 [1400/1875]: loss = 0.15310388803482056, lr = 0.0001\n",
            "2025-05-29 00:35:58 INFO     Epoch 13 [1450/1875]: loss = 0.2048029750585556, lr = 0.0001\n",
            "2025-05-29 00:36:05 INFO     Epoch 13 [1500/1875]: loss = 0.2018883228302002, lr = 0.0001\n",
            "2025-05-29 00:36:12 INFO     Epoch 13 [1550/1875]: loss = 0.19155091047286987, lr = 0.0001\n",
            "2025-05-29 00:36:18 INFO     Epoch 13 [1600/1875]: loss = 0.20997494459152222, lr = 0.0001\n",
            "2025-05-29 00:36:25 INFO     Epoch 13 [1650/1875]: loss = 0.19844286143779755, lr = 0.0001\n",
            "2025-05-29 00:36:32 INFO     Epoch 13 [1700/1875]: loss = 0.1747780740261078, lr = 0.0001\n",
            "2025-05-29 00:36:39 INFO     Epoch 13 [1750/1875]: loss = 0.2521344721317291, lr = 0.0001\n",
            "2025-05-29 00:36:45 INFO     Epoch 13 [1800/1875]: loss = 0.15176457166671753, lr = 0.0001\n",
            "2025-05-29 00:36:52 INFO     Epoch 13 [1850/1875]: loss = 0.2255486696958542, lr = 0.0001\n",
            "2025-05-29 00:36:56 INFO     Epoch 14 [0/1875]: loss = 0.12869228422641754, lr = 0.0001\n",
            "2025-05-29 00:37:03 INFO     Epoch 14 [50/1875]: loss = 0.185469388961792, lr = 0.0001\n",
            "2025-05-29 00:37:10 INFO     Epoch 14 [100/1875]: loss = 0.21056446433067322, lr = 0.0001\n",
            "2025-05-29 00:37:16 INFO     Epoch 14 [150/1875]: loss = 0.19458705186843872, lr = 0.0001\n",
            "2025-05-29 00:37:23 INFO     Epoch 14 [200/1875]: loss = 0.1739577353000641, lr = 0.0001\n",
            "2025-05-29 00:37:30 INFO     Epoch 14 [250/1875]: loss = 0.2410328984260559, lr = 0.0001\n",
            "2025-05-29 00:37:37 INFO     Epoch 14 [300/1875]: loss = 0.20406904816627502, lr = 0.0001\n",
            "2025-05-29 00:37:43 INFO     Epoch 14 [350/1875]: loss = 0.20251350104808807, lr = 0.0001\n",
            "2025-05-29 00:37:50 INFO     Epoch 14 [400/1875]: loss = 0.16242152452468872, lr = 0.0001\n",
            "2025-05-29 00:37:57 INFO     Epoch 14 [450/1875]: loss = 0.2043239027261734, lr = 0.0001\n",
            "2025-05-29 00:38:04 INFO     Epoch 14 [500/1875]: loss = 0.21227288246154785, lr = 0.0001\n",
            "2025-05-29 00:38:11 INFO     Epoch 14 [550/1875]: loss = 0.17037855088710785, lr = 0.0001\n",
            "2025-05-29 00:38:17 INFO     Epoch 14 [600/1875]: loss = 0.17971856892108917, lr = 0.0001\n",
            "2025-05-29 00:38:24 INFO     Epoch 14 [650/1875]: loss = 0.23838050663471222, lr = 0.0001\n",
            "2025-05-29 00:38:31 INFO     Epoch 14 [700/1875]: loss = 0.17116522789001465, lr = 0.0001\n",
            "2025-05-29 00:38:38 INFO     Epoch 14 [750/1875]: loss = 0.16853749752044678, lr = 0.0001\n",
            "2025-05-29 00:38:45 INFO     Epoch 14 [800/1875]: loss = 0.20243953168392181, lr = 0.0001\n",
            "2025-05-29 00:38:51 INFO     Epoch 14 [850/1875]: loss = 0.2390691041946411, lr = 0.0001\n",
            "2025-05-29 00:38:58 INFO     Epoch 14 [900/1875]: loss = 0.22912642359733582, lr = 0.0001\n",
            "2025-05-29 00:39:05 INFO     Epoch 14 [950/1875]: loss = 0.20760896801948547, lr = 0.0001\n",
            "2025-05-29 00:39:12 INFO     Epoch 14 [1000/1875]: loss = 0.18051739037036896, lr = 0.0001\n",
            "2025-05-29 00:39:19 INFO     Epoch 14 [1050/1875]: loss = 0.13760779798030853, lr = 0.0001\n",
            "2025-05-29 00:39:25 INFO     Epoch 14 [1100/1875]: loss = 0.1511872112751007, lr = 0.0001\n",
            "2025-05-29 00:39:32 INFO     Epoch 14 [1150/1875]: loss = 0.1650419682264328, lr = 0.0001\n",
            "2025-05-29 00:39:39 INFO     Epoch 14 [1200/1875]: loss = 0.1992666870355606, lr = 0.0001\n",
            "2025-05-29 00:39:46 INFO     Epoch 14 [1250/1875]: loss = 0.16961245238780975, lr = 0.0001\n",
            "2025-05-29 00:39:52 INFO     Epoch 14 [1300/1875]: loss = 0.18370553851127625, lr = 0.0001\n",
            "2025-05-29 00:39:59 INFO     Epoch 14 [1350/1875]: loss = 0.17849349975585938, lr = 0.0001\n",
            "2025-05-29 00:40:06 INFO     Epoch 14 [1400/1875]: loss = 0.15741749107837677, lr = 0.0001\n",
            "2025-05-29 00:40:13 INFO     Epoch 14 [1450/1875]: loss = 0.20726346969604492, lr = 0.0001\n",
            "2025-05-29 00:40:20 INFO     Epoch 14 [1500/1875]: loss = 0.19972577691078186, lr = 0.0001\n",
            "2025-05-29 00:40:26 INFO     Epoch 14 [1550/1875]: loss = 0.1608632355928421, lr = 0.0001\n",
            "2025-05-29 00:40:33 INFO     Epoch 14 [1600/1875]: loss = 0.15982693433761597, lr = 0.0001\n",
            "2025-05-29 00:40:40 INFO     Epoch 14 [1650/1875]: loss = 0.21834206581115723, lr = 0.0001\n",
            "2025-05-29 00:40:47 INFO     Epoch 14 [1700/1875]: loss = 0.2569435238838196, lr = 0.0001\n",
            "2025-05-29 00:40:53 INFO     Epoch 14 [1750/1875]: loss = 0.1942824125289917, lr = 0.0001\n",
            "2025-05-29 00:41:00 INFO     Epoch 14 [1800/1875]: loss = 0.1949920803308487, lr = 0.0001\n",
            "2025-05-29 00:41:07 INFO     Epoch 14 [1850/1875]: loss = 0.21701540052890778, lr = 0.0001\n",
            "2025-05-29 00:41:12 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "2025-05-29 00:41:54 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:41:59 INFO     Evaluating [0/313] samples generated [32/1000] running fid 229.87294006347656\n",
            "2025-05-29 00:42:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:43:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:43:57 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:44:37 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:45:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:45:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:46:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:47:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:47:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:48:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:49:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:49:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:50:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:51:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:51:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:52:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:53:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:53:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:54:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:55:18 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:55:58 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:56:38 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:57:17 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:57:57 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:58:37 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:59:16 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 00:59:56 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:00:36 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:01:16 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:01:55 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:02:35 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:02:42 INFO     Evaluating [50/313] samples generated [1000/1000] running fid 185.3086700439453\n",
            "2025-05-29 01:02:52 INFO     Evaluating [100/313] samples generated [1000/1000] running fid 184.77159118652344\n",
            "2025-05-29 01:03:03 INFO     Evaluating [150/313] samples generated [1000/1000] running fid 184.4553985595703\n",
            "2025-05-29 01:03:13 INFO     Evaluating [200/313] samples generated [1000/1000] running fid 184.59144592285156\n",
            "2025-05-29 01:03:24 INFO     Evaluating [250/313] samples generated [1000/1000] running fid 185.01858520507812\n",
            "2025-05-29 01:03:35 INFO     Evaluating [300/313] samples generated [1000/1000] running fid 185.11627197265625\n",
            "2025-05-29 01:03:41 INFO     EMA: Switching from eval to train, restoring saved parameters\n",
            "2025-05-29 01:03:42 INFO     Epoch 15 [0/1875]: loss = 0.17802166938781738, lr = 0.0001\n",
            "2025-05-29 01:03:48 INFO     Epoch 15 [50/1875]: loss = 0.1693105846643448, lr = 0.0001\n",
            "2025-05-29 01:03:55 INFO     Epoch 15 [100/1875]: loss = 0.14961761236190796, lr = 0.0001\n",
            "2025-05-29 01:04:02 INFO     Epoch 15 [150/1875]: loss = 0.19957789778709412, lr = 0.0001\n",
            "2025-05-29 01:04:09 INFO     Epoch 15 [200/1875]: loss = 0.1784772127866745, lr = 0.0001\n",
            "2025-05-29 01:04:16 INFO     Epoch 15 [250/1875]: loss = 0.2054334431886673, lr = 0.0001\n",
            "2025-05-29 01:04:23 INFO     Epoch 15 [300/1875]: loss = 0.1808961182832718, lr = 0.0001\n",
            "2025-05-29 01:04:30 INFO     Epoch 15 [350/1875]: loss = 0.21445652842521667, lr = 0.0001\n",
            "2025-05-29 01:04:36 INFO     Epoch 15 [400/1875]: loss = 0.22404272854328156, lr = 0.0001\n",
            "2025-05-29 01:04:43 INFO     Epoch 15 [450/1875]: loss = 0.2533639073371887, lr = 0.0001\n",
            "2025-05-29 01:04:50 INFO     Epoch 15 [500/1875]: loss = 0.14698101580142975, lr = 0.0001\n",
            "2025-05-29 01:04:57 INFO     Epoch 15 [550/1875]: loss = 0.1812136322259903, lr = 0.0001\n",
            "2025-05-29 01:05:04 INFO     Epoch 15 [600/1875]: loss = 0.1632397174835205, lr = 0.0001\n",
            "2025-05-29 01:05:10 INFO     Epoch 15 [650/1875]: loss = 0.22529739141464233, lr = 0.0001\n",
            "2025-05-29 01:05:17 INFO     Epoch 15 [700/1875]: loss = 0.27593910694122314, lr = 0.0001\n",
            "2025-05-29 01:05:24 INFO     Epoch 15 [750/1875]: loss = 0.18653161823749542, lr = 0.0001\n",
            "2025-05-29 01:05:31 INFO     Epoch 15 [800/1875]: loss = 0.2148457169532776, lr = 0.0001\n",
            "2025-05-29 01:05:38 INFO     Epoch 15 [850/1875]: loss = 0.21587856113910675, lr = 0.0001\n",
            "2025-05-29 01:05:45 INFO     Epoch 15 [900/1875]: loss = 0.22797803580760956, lr = 0.0001\n",
            "2025-05-29 01:05:51 INFO     Epoch 15 [950/1875]: loss = 0.1572703868150711, lr = 0.0001\n",
            "2025-05-29 01:05:58 INFO     Epoch 15 [1000/1875]: loss = 0.19666945934295654, lr = 0.0001\n",
            "2025-05-29 01:06:05 INFO     Epoch 15 [1050/1875]: loss = 0.2092577964067459, lr = 0.0001\n",
            "2025-05-29 01:06:12 INFO     Epoch 15 [1100/1875]: loss = 0.18284949660301208, lr = 0.0001\n",
            "2025-05-29 01:06:19 INFO     Epoch 15 [1150/1875]: loss = 0.2057676464319229, lr = 0.0001\n",
            "2025-05-29 01:06:25 INFO     Epoch 15 [1200/1875]: loss = 0.17999544739723206, lr = 0.0001\n",
            "2025-05-29 01:06:32 INFO     Epoch 15 [1250/1875]: loss = 0.14532570540905, lr = 0.0001\n",
            "2025-05-29 01:06:39 INFO     Epoch 15 [1300/1875]: loss = 0.20715056359767914, lr = 0.0001\n",
            "2025-05-29 01:06:46 INFO     Epoch 15 [1350/1875]: loss = 0.22946703433990479, lr = 0.0001\n",
            "2025-05-29 01:06:53 INFO     Epoch 15 [1400/1875]: loss = 0.17105066776275635, lr = 0.0001\n",
            "2025-05-29 01:07:00 INFO     Epoch 15 [1450/1875]: loss = 0.2687132954597473, lr = 0.0001\n",
            "2025-05-29 01:07:06 INFO     Epoch 15 [1500/1875]: loss = 0.17531122267246246, lr = 0.0001\n",
            "2025-05-29 01:07:13 INFO     Epoch 15 [1550/1875]: loss = 0.12877696752548218, lr = 0.0001\n",
            "2025-05-29 01:07:20 INFO     Epoch 15 [1600/1875]: loss = 0.20604932308197021, lr = 0.0001\n",
            "2025-05-29 01:07:27 INFO     Epoch 15 [1650/1875]: loss = 0.1661096066236496, lr = 0.0001\n",
            "2025-05-29 01:07:34 INFO     Epoch 15 [1700/1875]: loss = 0.2483522593975067, lr = 0.0001\n",
            "2025-05-29 01:07:41 INFO     Epoch 15 [1750/1875]: loss = 0.22097967565059662, lr = 0.0001\n",
            "2025-05-29 01:07:47 INFO     Epoch 15 [1800/1875]: loss = 0.2454703003168106, lr = 0.0001\n",
            "2025-05-29 01:07:54 INFO     Epoch 15 [1850/1875]: loss = 0.2719695568084717, lr = 0.0001\n",
            "2025-05-29 01:07:58 INFO     Epoch 16 [0/1875]: loss = 0.17850540578365326, lr = 0.0001\n",
            "2025-05-29 01:08:05 INFO     Epoch 16 [50/1875]: loss = 0.16831286251544952, lr = 0.0001\n",
            "2025-05-29 01:08:12 INFO     Epoch 16 [100/1875]: loss = 0.20304936170578003, lr = 0.0001\n",
            "2025-05-29 01:08:18 INFO     Epoch 16 [150/1875]: loss = 0.2214217633008957, lr = 0.0001\n",
            "2025-05-29 01:08:25 INFO     Epoch 16 [200/1875]: loss = 0.1850171834230423, lr = 0.0001\n",
            "2025-05-29 01:08:32 INFO     Epoch 16 [250/1875]: loss = 0.16372504830360413, lr = 0.0001\n",
            "2025-05-29 01:08:39 INFO     Epoch 16 [300/1875]: loss = 0.2070331573486328, lr = 0.0001\n",
            "2025-05-29 01:08:45 INFO     Epoch 16 [350/1875]: loss = 0.16092976927757263, lr = 0.0001\n",
            "2025-05-29 01:08:52 INFO     Epoch 16 [400/1875]: loss = 0.1944081336259842, lr = 0.0001\n",
            "2025-05-29 01:08:59 INFO     Epoch 16 [450/1875]: loss = 0.2123311161994934, lr = 0.0001\n",
            "2025-05-29 01:09:06 INFO     Epoch 16 [500/1875]: loss = 0.19817201793193817, lr = 0.0001\n",
            "2025-05-29 01:09:13 INFO     Epoch 16 [550/1875]: loss = 0.12497513741254807, lr = 0.0001\n",
            "2025-05-29 01:09:19 INFO     Epoch 16 [600/1875]: loss = 0.1743336021900177, lr = 0.0001\n",
            "2025-05-29 01:09:26 INFO     Epoch 16 [650/1875]: loss = 0.19777745008468628, lr = 0.0001\n",
            "2025-05-29 01:09:33 INFO     Epoch 16 [700/1875]: loss = 0.19356095790863037, lr = 0.0001\n",
            "2025-05-29 01:09:40 INFO     Epoch 16 [750/1875]: loss = 0.2151457965373993, lr = 0.0001\n",
            "2025-05-29 01:09:47 INFO     Epoch 16 [800/1875]: loss = 0.17321893572807312, lr = 0.0001\n",
            "2025-05-29 01:09:54 INFO     Epoch 16 [850/1875]: loss = 0.17414340376853943, lr = 0.0001\n",
            "2025-05-29 01:10:01 INFO     Epoch 16 [900/1875]: loss = 0.20010848343372345, lr = 0.0001\n",
            "2025-05-29 01:10:07 INFO     Epoch 16 [950/1875]: loss = 0.18507571518421173, lr = 0.0001\n",
            "2025-05-29 01:10:14 INFO     Epoch 16 [1000/1875]: loss = 0.17569635808467865, lr = 0.0001\n",
            "2025-05-29 01:10:21 INFO     Epoch 16 [1050/1875]: loss = 0.17757385969161987, lr = 0.0001\n",
            "2025-05-29 01:10:28 INFO     Epoch 16 [1100/1875]: loss = 0.17345941066741943, lr = 0.0001\n",
            "2025-05-29 01:10:35 INFO     Epoch 16 [1150/1875]: loss = 0.2427036315202713, lr = 0.0001\n",
            "2025-05-29 01:10:41 INFO     Epoch 16 [1200/1875]: loss = 0.15152797102928162, lr = 0.0001\n",
            "2025-05-29 01:10:48 INFO     Epoch 16 [1250/1875]: loss = 0.17640098929405212, lr = 0.0001\n",
            "2025-05-29 01:10:55 INFO     Epoch 16 [1300/1875]: loss = 0.18369059264659882, lr = 0.0001\n",
            "2025-05-29 01:11:02 INFO     Epoch 16 [1350/1875]: loss = 0.16304652392864227, lr = 0.0001\n",
            "2025-05-29 01:11:09 INFO     Epoch 16 [1400/1875]: loss = 0.19615428149700165, lr = 0.0001\n",
            "2025-05-29 01:11:15 INFO     Epoch 16 [1450/1875]: loss = 0.16387276351451874, lr = 0.0001\n",
            "2025-05-29 01:11:22 INFO     Epoch 16 [1500/1875]: loss = 0.14067479968070984, lr = 0.0001\n",
            "2025-05-29 01:11:29 INFO     Epoch 16 [1550/1875]: loss = 0.24549022316932678, lr = 0.0001\n",
            "2025-05-29 01:11:36 INFO     Epoch 16 [1600/1875]: loss = 0.2269105464220047, lr = 0.0001\n",
            "2025-05-29 01:11:43 INFO     Epoch 16 [1650/1875]: loss = 0.16876475512981415, lr = 0.0001\n",
            "2025-05-29 01:11:49 INFO     Epoch 16 [1700/1875]: loss = 0.23781718313694, lr = 0.0001\n",
            "2025-05-29 01:11:56 INFO     Epoch 16 [1750/1875]: loss = 0.25807854533195496, lr = 0.0001\n",
            "2025-05-29 01:12:03 INFO     Epoch 16 [1800/1875]: loss = 0.10735383629798889, lr = 0.0001\n",
            "2025-05-29 01:12:10 INFO     Epoch 16 [1850/1875]: loss = 0.220053568482399, lr = 0.0001\n",
            "2025-05-29 01:12:14 INFO     Epoch 17 [0/1875]: loss = 0.19156545400619507, lr = 0.0001\n",
            "2025-05-29 01:12:20 INFO     Epoch 17 [50/1875]: loss = 0.20449525117874146, lr = 0.0001\n",
            "2025-05-29 01:12:27 INFO     Epoch 17 [100/1875]: loss = 0.1241200864315033, lr = 0.0001\n",
            "2025-05-29 01:12:34 INFO     Epoch 17 [150/1875]: loss = 0.2562493681907654, lr = 0.0001\n",
            "2025-05-29 01:12:41 INFO     Epoch 17 [200/1875]: loss = 0.10970330983400345, lr = 0.0001\n",
            "2025-05-29 01:12:48 INFO     Epoch 17 [250/1875]: loss = 0.2521021366119385, lr = 0.0001\n",
            "2025-05-29 01:12:54 INFO     Epoch 17 [300/1875]: loss = 0.2279958873987198, lr = 0.0001\n",
            "2025-05-29 01:13:01 INFO     Epoch 17 [350/1875]: loss = 0.17429646849632263, lr = 0.0001\n",
            "2025-05-29 01:13:08 INFO     Epoch 17 [400/1875]: loss = 0.2073855698108673, lr = 0.0001\n",
            "2025-05-29 01:13:15 INFO     Epoch 17 [450/1875]: loss = 0.2610258460044861, lr = 0.0001\n",
            "2025-05-29 01:13:22 INFO     Epoch 17 [500/1875]: loss = 0.18678133189678192, lr = 0.0001\n",
            "2025-05-29 01:13:29 INFO     Epoch 17 [550/1875]: loss = 0.1583060473203659, lr = 0.0001\n",
            "2025-05-29 01:13:36 INFO     Epoch 17 [600/1875]: loss = 0.17564833164215088, lr = 0.0001\n",
            "2025-05-29 01:13:42 INFO     Epoch 17 [650/1875]: loss = 0.25011900067329407, lr = 0.0001\n",
            "2025-05-29 01:13:49 INFO     Epoch 17 [700/1875]: loss = 0.16323497891426086, lr = 0.0001\n",
            "2025-05-29 01:13:56 INFO     Epoch 17 [750/1875]: loss = 0.18485954403877258, lr = 0.0001\n",
            "2025-05-29 01:14:03 INFO     Epoch 17 [800/1875]: loss = 0.23608216643333435, lr = 0.0001\n",
            "2025-05-29 01:14:10 INFO     Epoch 17 [850/1875]: loss = 0.16608725488185883, lr = 0.0001\n",
            "2025-05-29 01:14:16 INFO     Epoch 17 [900/1875]: loss = 0.19032925367355347, lr = 0.0001\n",
            "2025-05-29 01:14:23 INFO     Epoch 17 [950/1875]: loss = 0.22364716231822968, lr = 0.0001\n",
            "2025-05-29 01:14:30 INFO     Epoch 17 [1000/1875]: loss = 0.23359911143779755, lr = 0.0001\n",
            "2025-05-29 01:14:37 INFO     Epoch 17 [1050/1875]: loss = 0.1956726312637329, lr = 0.0001\n",
            "2025-05-29 01:14:44 INFO     Epoch 17 [1100/1875]: loss = 0.146046981215477, lr = 0.0001\n",
            "2025-05-29 01:14:50 INFO     Epoch 17 [1150/1875]: loss = 0.1653570681810379, lr = 0.0001\n",
            "2025-05-29 01:14:57 INFO     Epoch 17 [1200/1875]: loss = 0.1807355284690857, lr = 0.0001\n",
            "2025-05-29 01:15:04 INFO     Epoch 17 [1250/1875]: loss = 0.1945839822292328, lr = 0.0001\n",
            "2025-05-29 01:15:11 INFO     Epoch 17 [1300/1875]: loss = 0.18897001445293427, lr = 0.0001\n",
            "2025-05-29 01:15:18 INFO     Epoch 17 [1350/1875]: loss = 0.18851181864738464, lr = 0.0001\n",
            "2025-05-29 01:15:24 INFO     Epoch 17 [1400/1875]: loss = 0.14642338454723358, lr = 0.0001\n",
            "2025-05-29 01:15:31 INFO     Epoch 17 [1450/1875]: loss = 0.20283910632133484, lr = 0.0001\n",
            "2025-05-29 01:15:38 INFO     Epoch 17 [1500/1875]: loss = 0.1676541119813919, lr = 0.0001\n",
            "2025-05-29 01:15:45 INFO     Epoch 17 [1550/1875]: loss = 0.1413743495941162, lr = 0.0001\n",
            "2025-05-29 01:15:52 INFO     Epoch 17 [1600/1875]: loss = 0.20833098888397217, lr = 0.0001\n",
            "2025-05-29 01:15:58 INFO     Epoch 17 [1650/1875]: loss = 0.21627379953861237, lr = 0.0001\n",
            "2025-05-29 01:16:05 INFO     Epoch 17 [1700/1875]: loss = 0.22860385477542877, lr = 0.0001\n",
            "2025-05-29 01:16:12 INFO     Epoch 17 [1750/1875]: loss = 0.14657357335090637, lr = 0.0001\n",
            "2025-05-29 01:16:19 INFO     Epoch 17 [1800/1875]: loss = 0.15973693132400513, lr = 0.0001\n",
            "2025-05-29 01:16:26 INFO     Epoch 17 [1850/1875]: loss = 0.2651328444480896, lr = 0.0001\n",
            "2025-05-29 01:16:30 INFO     Epoch 18 [0/1875]: loss = 0.18904881179332733, lr = 0.0001\n",
            "2025-05-29 01:16:36 INFO     Epoch 18 [50/1875]: loss = 0.20288148522377014, lr = 0.0001\n",
            "2025-05-29 01:16:43 INFO     Epoch 18 [100/1875]: loss = 0.2027292400598526, lr = 0.0001\n",
            "2025-05-29 01:16:50 INFO     Epoch 18 [150/1875]: loss = 0.16725976765155792, lr = 0.0001\n",
            "2025-05-29 01:16:57 INFO     Epoch 18 [200/1875]: loss = 0.18673962354660034, lr = 0.0001\n",
            "2025-05-29 01:17:04 INFO     Epoch 18 [250/1875]: loss = 0.2092694342136383, lr = 0.0001\n",
            "2025-05-29 01:17:10 INFO     Epoch 18 [300/1875]: loss = 0.274647057056427, lr = 0.0001\n",
            "2025-05-29 01:17:17 INFO     Epoch 18 [350/1875]: loss = 0.1641090214252472, lr = 0.0001\n",
            "2025-05-29 01:17:24 INFO     Epoch 18 [400/1875]: loss = 0.18923872709274292, lr = 0.0001\n",
            "2025-05-29 01:17:31 INFO     Epoch 18 [450/1875]: loss = 0.1915324479341507, lr = 0.0001\n",
            "2025-05-29 01:17:38 INFO     Epoch 18 [500/1875]: loss = 0.23836153745651245, lr = 0.0001\n",
            "2025-05-29 01:17:45 INFO     Epoch 18 [550/1875]: loss = 0.21045036613941193, lr = 0.0001\n",
            "2025-05-29 01:17:52 INFO     Epoch 18 [600/1875]: loss = 0.166454017162323, lr = 0.0001\n",
            "2025-05-29 01:17:58 INFO     Epoch 18 [650/1875]: loss = 0.20256023108959198, lr = 0.0001\n",
            "2025-05-29 01:18:05 INFO     Epoch 18 [700/1875]: loss = 0.15622521936893463, lr = 0.0001\n",
            "2025-05-29 01:18:12 INFO     Epoch 18 [750/1875]: loss = 0.21333833038806915, lr = 0.0001\n",
            "2025-05-29 01:18:19 INFO     Epoch 18 [800/1875]: loss = 0.21090197563171387, lr = 0.0001\n",
            "2025-05-29 01:18:26 INFO     Epoch 18 [850/1875]: loss = 0.1846800595521927, lr = 0.0001\n",
            "2025-05-29 01:18:32 INFO     Epoch 18 [900/1875]: loss = 0.22776521742343903, lr = 0.0001\n",
            "2025-05-29 01:18:39 INFO     Epoch 18 [950/1875]: loss = 0.2493472695350647, lr = 0.0001\n",
            "2025-05-29 01:18:46 INFO     Epoch 18 [1000/1875]: loss = 0.2074449360370636, lr = 0.0001\n",
            "2025-05-29 01:18:53 INFO     Epoch 18 [1050/1875]: loss = 0.18215319514274597, lr = 0.0001\n",
            "2025-05-29 01:19:00 INFO     Epoch 18 [1100/1875]: loss = 0.1985860913991928, lr = 0.0001\n",
            "2025-05-29 01:19:07 INFO     Epoch 18 [1150/1875]: loss = 0.17451275885105133, lr = 0.0001\n",
            "2025-05-29 01:19:13 INFO     Epoch 18 [1200/1875]: loss = 0.19188933074474335, lr = 0.0001\n",
            "2025-05-29 01:19:20 INFO     Epoch 18 [1250/1875]: loss = 0.20194005966186523, lr = 0.0001\n",
            "2025-05-29 01:19:27 INFO     Epoch 18 [1300/1875]: loss = 0.17643019556999207, lr = 0.0001\n",
            "2025-05-29 01:19:34 INFO     Epoch 18 [1350/1875]: loss = 0.10814963281154633, lr = 0.0001\n",
            "2025-05-29 01:19:41 INFO     Epoch 18 [1400/1875]: loss = 0.209246426820755, lr = 0.0001\n",
            "2025-05-29 01:19:48 INFO     Epoch 18 [1450/1875]: loss = 0.21967561542987823, lr = 0.0001\n",
            "2025-05-29 01:19:55 INFO     Epoch 18 [1500/1875]: loss = 0.1737968921661377, lr = 0.0001\n",
            "2025-05-29 01:20:02 INFO     Epoch 18 [1550/1875]: loss = 0.13362857699394226, lr = 0.0001\n",
            "2025-05-29 01:20:08 INFO     Epoch 18 [1600/1875]: loss = 0.20987163484096527, lr = 0.0001\n",
            "2025-05-29 01:20:15 INFO     Epoch 18 [1650/1875]: loss = 0.23468388617038727, lr = 0.0001\n",
            "2025-05-29 01:20:22 INFO     Epoch 18 [1700/1875]: loss = 0.20023566484451294, lr = 0.0001\n",
            "2025-05-29 01:20:29 INFO     Epoch 18 [1750/1875]: loss = 0.19431403279304504, lr = 0.0001\n",
            "2025-05-29 01:20:36 INFO     Epoch 18 [1800/1875]: loss = 0.20453588664531708, lr = 0.0001\n",
            "2025-05-29 01:20:42 INFO     Epoch 18 [1850/1875]: loss = 0.18249264359474182, lr = 0.0001\n",
            "2025-05-29 01:20:46 INFO     Epoch 19 [0/1875]: loss = 0.20309101045131683, lr = 0.0001\n",
            "2025-05-29 01:20:53 INFO     Epoch 19 [50/1875]: loss = 0.23300370573997498, lr = 0.0001\n",
            "2025-05-29 01:21:00 INFO     Epoch 19 [100/1875]: loss = 0.2312999665737152, lr = 0.0001\n",
            "2025-05-29 01:21:07 INFO     Epoch 19 [150/1875]: loss = 0.1636960655450821, lr = 0.0001\n",
            "2025-05-29 01:21:13 INFO     Epoch 19 [200/1875]: loss = 0.1473740190267563, lr = 0.0001\n",
            "2025-05-29 01:21:20 INFO     Epoch 19 [250/1875]: loss = 0.1734292209148407, lr = 0.0001\n",
            "2025-05-29 01:21:27 INFO     Epoch 19 [300/1875]: loss = 0.21310850977897644, lr = 0.0001\n",
            "2025-05-29 01:21:34 INFO     Epoch 19 [350/1875]: loss = 0.17649364471435547, lr = 0.0001\n",
            "2025-05-29 01:21:41 INFO     Epoch 19 [400/1875]: loss = 0.15983396768569946, lr = 0.0001\n",
            "2025-05-29 01:21:47 INFO     Epoch 19 [450/1875]: loss = 0.18429505825042725, lr = 0.0001\n",
            "2025-05-29 01:21:54 INFO     Epoch 19 [500/1875]: loss = 0.20369023084640503, lr = 0.0001\n",
            "2025-05-29 01:22:01 INFO     Epoch 19 [550/1875]: loss = 0.22217804193496704, lr = 0.0001\n",
            "2025-05-29 01:22:08 INFO     Epoch 19 [600/1875]: loss = 0.21920545399188995, lr = 0.0001\n",
            "2025-05-29 01:22:15 INFO     Epoch 19 [650/1875]: loss = 0.23024775087833405, lr = 0.0001\n",
            "2025-05-29 01:22:21 INFO     Epoch 19 [700/1875]: loss = 0.20944614708423615, lr = 0.0001\n",
            "2025-05-29 01:22:28 INFO     Epoch 19 [750/1875]: loss = 0.24268417060375214, lr = 0.0001\n",
            "2025-05-29 01:22:35 INFO     Epoch 19 [800/1875]: loss = 0.1952369213104248, lr = 0.0001\n",
            "2025-05-29 01:22:42 INFO     Epoch 19 [850/1875]: loss = 0.11916076391935349, lr = 0.0001\n",
            "2025-05-29 01:22:49 INFO     Epoch 19 [900/1875]: loss = 0.17827318608760834, lr = 0.0001\n",
            "2025-05-29 01:22:56 INFO     Epoch 19 [950/1875]: loss = 0.1946074366569519, lr = 0.0001\n",
            "2025-05-29 01:23:02 INFO     Epoch 19 [1000/1875]: loss = 0.22633084654808044, lr = 0.0001\n",
            "2025-05-29 01:23:09 INFO     Epoch 19 [1050/1875]: loss = 0.19544266164302826, lr = 0.0001\n",
            "2025-05-29 01:23:16 INFO     Epoch 19 [1100/1875]: loss = 0.1891765147447586, lr = 0.0001\n",
            "2025-05-29 01:23:23 INFO     Epoch 19 [1150/1875]: loss = 0.1725110113620758, lr = 0.0001\n",
            "2025-05-29 01:23:30 INFO     Epoch 19 [1200/1875]: loss = 0.21491768956184387, lr = 0.0001\n",
            "2025-05-29 01:23:36 INFO     Epoch 19 [1250/1875]: loss = 0.1108384057879448, lr = 0.0001\n",
            "2025-05-29 01:23:43 INFO     Epoch 19 [1300/1875]: loss = 0.23156395554542542, lr = 0.0001\n",
            "2025-05-29 01:23:50 INFO     Epoch 19 [1350/1875]: loss = 0.19407905638217926, lr = 0.0001\n",
            "2025-05-29 01:23:57 INFO     Epoch 19 [1400/1875]: loss = 0.1709010899066925, lr = 0.0001\n",
            "2025-05-29 01:24:04 INFO     Epoch 19 [1450/1875]: loss = 0.2093919962644577, lr = 0.0001\n",
            "2025-05-29 01:24:10 INFO     Epoch 19 [1500/1875]: loss = 0.15256203711032867, lr = 0.0001\n",
            "2025-05-29 01:24:17 INFO     Epoch 19 [1550/1875]: loss = 0.14527389407157898, lr = 0.0001\n",
            "2025-05-29 01:24:24 INFO     Epoch 19 [1600/1875]: loss = 0.16637417674064636, lr = 0.0001\n",
            "2025-05-29 01:24:31 INFO     Epoch 19 [1650/1875]: loss = 0.2230743169784546, lr = 0.0001\n",
            "2025-05-29 01:24:38 INFO     Epoch 19 [1700/1875]: loss = 0.2358899563550949, lr = 0.0001\n",
            "2025-05-29 01:24:44 INFO     Epoch 19 [1750/1875]: loss = 0.13585776090621948, lr = 0.0001\n",
            "2025-05-29 01:24:51 INFO     Epoch 19 [1800/1875]: loss = 0.16328071057796478, lr = 0.0001\n",
            "2025-05-29 01:24:58 INFO     Epoch 19 [1850/1875]: loss = 0.19278806447982788, lr = 0.0001\n",
            "2025-05-29 01:25:03 INFO     EMA: Switching from train to eval, backing up parameters and copying EMA params\n",
            "2025-05-29 01:25:46 INFO     32 samples generated in 1024 evaluations.\n",
            "2025-05-29 01:25:50 INFO     Evaluating [0/313] samples generated [32/1000] running fid 246.11891174316406\n"
          ]
        }
      ]
    }
  ]
}